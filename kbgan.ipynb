{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T08:45:11.445722Z",
     "iopub.status.busy": "2025-10-09T08:45:11.445531Z",
     "iopub.status.idle": "2025-10-09T08:45:11.487971Z",
     "shell.execute_reply": "2025-10-09T08:45:11.487297Z",
     "shell.execute_reply.started": "2025-10-09T08:45:11.445706Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "import logging\n",
    "\n",
    "_config = None\n",
    "class ConfigDict(dict):\n",
    "    __getattr__ = dict.__getitem__\n",
    "\n",
    "def config(config_path='./config/config_wn18rr.yaml'):\n",
    "    \"\"\"\n",
    "    default: config(\"config_wn18rr.yaml\")\n",
    "    \"\"\"\n",
    "    def _make_config_dict(obj):\n",
    "        if isinstance(obj, dict):\n",
    "            return ConfigDict({k: _make_config_dict(v) for k, v in obj.items()})\n",
    "        elif isinstance(obj, list):\n",
    "            return [_make_config_dict(x) for x in obj]\n",
    "        else:\n",
    "            return obj\n",
    "    \n",
    "    global _config\n",
    "    if _config is None:\n",
    "        with open(config_path) as f:\n",
    "            _config = _make_config_dict(yaml.load(f, Loader=yaml.FullLoader))\n",
    "    return _config\n",
    "\n",
    "def overwrite_config_with_args(args=[], sep='.'):\n",
    "    \"\"\"\n",
    "    Manually pass parameters. E.g. overwrite_config_with_args([\"--pretrain_config=TransD\"])\n",
    "    \"\"\"\n",
    "    def path_set(path, val, sep='.', auto_convert=False):\n",
    "        steps = path.split(sep)\n",
    "        obj = _config\n",
    "        for step in steps[:-1]:\n",
    "            obj = obj[step]\n",
    "        old_val = obj[steps[-1]]\n",
    "        \n",
    "        if not auto_convert:\n",
    "            obj[steps[-1]] = val\n",
    "        elif isinstance(old_val, bool):\n",
    "            obj[steps[-1]] = val.lower() == 'true'\n",
    "        elif isinstance(old_val, float):\n",
    "            obj[steps[-1]] = float(val)\n",
    "        elif isinstance(old_val, int):\n",
    "            try:\n",
    "                obj[steps[-1]] = int(val)\n",
    "            except ValueError:\n",
    "                obj[steps[-1]] = float(val)\n",
    "        else:\n",
    "            obj[steps[-1]] = val\n",
    "    \n",
    "    for arg in args:\n",
    "        if arg.startswith('--') and '=' in arg:\n",
    "            path, val = arg[2:].split('=')\n",
    "            if path != 'config':\n",
    "                path_set(path, val, sep, auto_convert=True)\n",
    "\n",
    "def dump_config():\n",
    "    def _dump_config(obj, prefix):\n",
    "        if isinstance(obj, dict):\n",
    "            for k, v in obj.items():\n",
    "                _dump_config(v, prefix + (k,))\n",
    "        elif isinstance(obj, list):\n",
    "            for i, v in enumerate(obj):\n",
    "                _dump_config(v, prefix + (str(i),))\n",
    "        else:\n",
    "            logging.debug('%s=%s', '.'.join(prefix), repr(obj))\n",
    "    return _dump_config(_config, tuple())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import logging\n",
    "import torch\n",
    "\n",
    "def select_gpu():\n",
    "    if not torch.cuda.is_available():\n",
    "        logging.warning(\"No GPU available. Running on CPU.\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        nvidia_info = subprocess.run(\n",
    "            ['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=True\n",
    "        )\n",
    "    except FileNotFoundError:\n",
    "        logging.warning(\"nvidia-smi not found. Running on CPU.\")\n",
    "        return None\n",
    "\n",
    "    gpu_info = False\n",
    "    gpu_info_line = 0\n",
    "    proc_info = False\n",
    "    gpu_mem = []\n",
    "    gpu_occupied = set()\n",
    "\n",
    "    for line in nvidia_info.stdout.split(b'\\n'):\n",
    "        line = line.decode().strip()\n",
    "        if gpu_info:\n",
    "            gpu_info_line += 1\n",
    "            if line == '':\n",
    "                gpu_info = False\n",
    "                continue\n",
    "            if gpu_info_line % 3 == 2:\n",
    "                mem_info = line.split('|')[2]\n",
    "                used_mem_mb = int(mem_info.strip().split()[0][:-3])\n",
    "                gpu_mem.append(used_mem_mb)\n",
    "        if proc_info:\n",
    "            if line == '|  No running processes found                                                 |':\n",
    "                continue\n",
    "            if line == '+-----------------------------------------------------------------------------+':\n",
    "                proc_info = False\n",
    "                continue\n",
    "            proc_gpu = int(line.split()[1])\n",
    "            gpu_occupied.add(proc_gpu)\n",
    "        if line == '|===============================+======================+======================|':\n",
    "            gpu_info = True\n",
    "        if line == '|=============================================================================|':\n",
    "            proc_info = True\n",
    "\n",
    "    if not gpu_mem:\n",
    "        logging.warning(\"Could not parse nvidia-smi output. Defaulting to GPU 0.\")\n",
    "        return 0\n",
    "\n",
    "    for i in range(len(gpu_mem)):\n",
    "        if i not in gpu_occupied:\n",
    "            logging.info('Automatically selected GPU %d because it is vacant.', i)\n",
    "            return i\n",
    "    for i in range(len(gpu_mem)):\n",
    "        if gpu_mem[i] == min(gpu_mem):\n",
    "            logging.info('All GPUs are occupied. Automatically selected GPU %d because it has the most free memory.', i)\n",
    "            return i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logger Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "def logger_init():\n",
    "    root_logger = logging.getLogger()\n",
    "    root_logger.handlers.clear()    # Xoá các handler mặc định của Jupyter\n",
    "    root_logger.setLevel(logging.DEBUG) # Hiện cả DEBUG, INFO...\n",
    "\n",
    "    # Hiện log ra output của cell\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setFormatter(logging.Formatter('%(module)15s %(asctime)s %(message)s', datefmt='%H:%M:%S'))\n",
    "    root_logger.addHandler(console_handler)\n",
    "\n",
    "    if (config().log.to_file):\n",
    "        log_dir = './output/' + config().task.dir + '/logs'\n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "        log_filename = os.path.join(\n",
    "            log_dir,\n",
    "            _config.log.prefix + datetime.datetime.now().strftime(\"%m%d%H%M%S\") + \".log\"\n",
    "        )\n",
    "        file_handler = logging.FileHandler(log_filename)\n",
    "        file_handler.setFormatter(logging.Formatter('%(module)15s %(asctime)s %(message)s', datefmt='%H:%M:%S'))\n",
    "        root_logger.addHandler(file_handler)\n",
    "\n",
    "    if config().log.dump_config:\n",
    "        dump_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T08:45:11.488928Z",
     "iopub.status.busy": "2025-10-09T08:45:11.488689Z",
     "iopub.status.idle": "2025-10-09T08:45:15.120647Z",
     "shell.execute_reply": "2025-10-09T08:45:15.119837Z",
     "shell.execute_reply.started": "2025-10-09T08:45:11.488903Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "\n",
    "def sparse_heads_tails(n_entity, train_data, valid_data=None, test_data=None):\n",
    "    if train_data:\n",
    "        train_head, train_relation, train_tail = train_data\n",
    "    else:\n",
    "        train_head = train_relation = train_tail = []\n",
    "    if valid_data:\n",
    "        valid_head, valid_relation, valid_tail = valid_data\n",
    "    else:\n",
    "        valid_head = valid_relation = valid_tail = []\n",
    "        \n",
    "    if test_data:\n",
    "        test_head, test_relation, test_tail = test_data\n",
    "    else:\n",
    "        test_head = test_relation = test_tail = []\n",
    "        \n",
    "    all_head = train_head + valid_head + test_head\n",
    "    all_relation = train_relation + valid_relation + test_relation\n",
    "    all_tail = train_tail + valid_tail + test_tail\n",
    "    \n",
    "    heads = defaultdict(lambda: set())\n",
    "    tails = defaultdict(lambda: set())\n",
    "    for h, r, t in zip(all_head, all_relation, all_tail):\n",
    "        heads[(t, r)].add(h)\n",
    "        tails[(h, r)].add(t)\n",
    "    \n",
    "    heads_sparse = tails_sparse = {}\n",
    "    for k in heads.keys():\n",
    "        heads_sparse[k] = torch.sparse.FloatTensor(torch.LongTensor([list(heads[k])]), torch.ones(len(heads[k])), torch.Size([n_entity]))\n",
    "    for k in tails.keys():\n",
    "        tails_sparse[k] = torch.sparse.FloatTensor(torch.LongTensor([list(tails[k])]), torch.ones(len(tails[k])), torch.Size([n_entity]))\n",
    "    return heads_sparse, tails_sparse\n",
    "\n",
    "def inplace_shuffle(*lists):\n",
    "    idx = []\n",
    "    for i in range(len(lists[0])):\n",
    "        idx.append(randint(0, i+1))\n",
    "    for ls in lists:\n",
    "        for i, item in enumerate(ls):\n",
    "            j = idx[i]\n",
    "            ls[i], ls[j] = ls[j], ls[i]\n",
    "\n",
    "def batch_by_num(n_batch, *lists, n_sample=None):\n",
    "    if n_sample is None:\n",
    "        n_sample = len(lists[0])\n",
    "    for i in range(n_batch):\n",
    "        head = int(n_sample * i / n_batch)\n",
    "        tail = int(n_sample * (i + 1) / n_batch)\n",
    "        ret = [ls[head:tail] for ls in lists]\n",
    "        if len(ret) > 1:\n",
    "            yield ret\n",
    "        else:\n",
    "            yield ret[0]\n",
    "\n",
    "def batch_by_size(batch_size, *lists, n_sample=None):\n",
    "    if n_sample is None:\n",
    "        n_sample = len(lists[0])\n",
    "    head = 0\n",
    "    while head < n_sample:\n",
    "        tail = min(n_sample, head + batch_size)\n",
    "        ret = [ls[head:tail] for ls in lists]\n",
    "        head += batch_size\n",
    "        if len(ret) > 1:\n",
    "            yield ret\n",
    "        else:\n",
    "            yield ret[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corrupter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from numpy.random import choice\n",
    "\n",
    "def get_bern_prob(data, n_relation):\n",
    "    head, relation, tail = data\n",
    "    edges = defaultdict(lambda: defaultdict(lambda: set()))\n",
    "    rev_edges = defaultdict(lambda: defaultdict(lambda: set()))\n",
    "    for s, r, t in zip(head, relation, tail):\n",
    "        edges[r][s].add(t)\n",
    "        rev_edges[r][t].add(s)\n",
    "    bern_prob = torch.zeros(n_relation)\n",
    "    for r in edges.keys():\n",
    "        tph = sum(len(tails) for tails in edges[r].values()) / len(edges[r])\n",
    "        htp = sum(len(heads) for heads in rev_edges[r].values()) / len(rev_edges[r])\n",
    "        bern_prob[r] = tph / (tph + htp)\n",
    "    return bern_prob\n",
    "\n",
    "class BernCorrupter(object):\n",
    "    def __init__(self, data, n_entity, n_relation):\n",
    "        self.bern_prob = get_bern_prob(data, n_relation)\n",
    "        self.n_entity = n_entity\n",
    "\n",
    "    def corrupt(self, head, relation, tail):\n",
    "        prob = self.bern_prob[relation]\n",
    "        selection = torch.bernoulli(prob).numpy().astype('int64')\n",
    "        entity_random = choice(self.n_entity, len(head))\n",
    "        head_out = (1 - selection) * head.numpy() + selection * entity_random\n",
    "        tail_out = selection * tail.numpy() + (1 - selection) * entity_random\n",
    "        return torch.from_numpy(head_out), torch.from_numpy(tail_out)\n",
    "\n",
    "class BernCorrupterMulti(object):\n",
    "    def __init__(self, data, n_entity, n_relation, n_sample):\n",
    "        self.bern_prob = get_bern_prob(data, n_relation)\n",
    "        self.n_entity = n_entity\n",
    "        self.n_sample = n_sample\n",
    "\n",
    "    def corrupt(self, head, relation, tail, keep_truth=True):\n",
    "        n = len(head)\n",
    "        prob = self.bern_prob[relation]\n",
    "        selection = torch.bernoulli(prob).numpy().astype('bool')\n",
    "        head_out = np.tile(head.numpy(), (self.n_sample, 1)).transpose()\n",
    "        tail_out = np.tile(tail.numpy(), (self.n_sample, 1)).transpose()\n",
    "        relation_out = relation.unsqueeze(1).expand(n, self.n_sample)\n",
    "        if keep_truth:\n",
    "            entity_random = choice(self.n_entity, (n, self.n_sample - 1))\n",
    "            head_out[selection, 1:] = entity_random[selection]\n",
    "            tail_out[~selection, 1:] = entity_random[~selection]\n",
    "        else:\n",
    "            entity_random = choice(self.n_entity, (n, self.n_sample))\n",
    "            head_out[selection, :] = entity_random[selection]\n",
    "            tail_out[~selection, :] = entity_random[~selection]\n",
    "        return torch.from_numpy(head_out), relation_out, torch.from_numpy(tail_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T08:45:15.254250Z",
     "iopub.status.busy": "2025-10-09T08:45:15.253815Z",
     "iopub.status.idle": "2025-10-09T08:45:15.273462Z",
     "shell.execute_reply": "2025-10-09T08:45:15.272769Z",
     "shell.execute_reply.started": "2025-10-09T08:45:15.254233Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "from collections import namedtuple\n",
    "import logging\n",
    "\n",
    "KBIndex = namedtuple('KBIndex', ['entity_list', 'relation_list', 'entity_id', 'relation_id'])\n",
    "\n",
    "def index_entity_relation(*filenames):\n",
    "    entity_set = set()\n",
    "    relation_set = set()\n",
    "    for filename in filenames:\n",
    "        with open(filename) as f:\n",
    "            for ln in f:\n",
    "                s, r, t = ln.strip().split('\\t')[:3]\n",
    "                entity_set.add(s)\n",
    "                entity_set.add(t)\n",
    "                relation_set.add(r)\n",
    "    entity_list = sorted(list(entity_set))\n",
    "    relation_list = sorted(list(relation_set))\n",
    "    entity_id = dict(zip(entity_list, count()))\n",
    "    relation_id = dict(zip(relation_list, count()))\n",
    "    return KBIndex(entity_list, relation_list, entity_id, relation_id)\n",
    "\n",
    "def graph_size(kb_index):\n",
    "    return len(kb_index.entity_id), len(kb_index.relation_id)\n",
    "\n",
    "def read_data(filename, kb_index, with_label=False):\n",
    "    heads, relations, tails = [], [], []\n",
    "    labels = []\n",
    "    skipped_count = 0\n",
    "    \n",
    "    with open(filename) as f:\n",
    "        for ln in f:\n",
    "            parts = ln.strip().split('\\t')\n",
    "            h, r, t = parts[:3]\n",
    "            \n",
    "            # Check if entity and relation exist in kb_index\n",
    "            if h not in kb_index.entity_id:\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "            if r not in kb_index.relation_id:\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "            if t not in kb_index.entity_id:\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "            \n",
    "            # All entities and relations are valid, add to lists\n",
    "            heads.append(kb_index.entity_id[h])\n",
    "            relations.append(kb_index.relation_id[r])\n",
    "            tails.append(kb_index.entity_id[t])\n",
    "\n",
    "            if with_label and len(parts) > 3:\n",
    "                labels.append(int(parts[3]))\n",
    "    \n",
    "    if skipped_count > 0:\n",
    "        logging.warning(f\"Skipped {skipped_count} triples with entities/relations not in kb_index from {filename}\")\n",
    "    \n",
    "    if with_label:\n",
    "        return heads, relations, tails, labels\n",
    "    else:\n",
    "        return heads, relations, tails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics for Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mr_mrr_hitsk(scores, target, k_list=[1, 3, 10]):\n",
    "    _, sorted_idx = torch.sort(scores)\n",
    "    find_target = sorted_idx == target\n",
    "    target_rank = torch.nonzero(find_target)[0, 0] + 1\n",
    "    target_score = scores[target].item()  # Get the score of the target entity\n",
    "    return target_rank, 1 / target_rank, [int(target_rank <= k) for k in k_list], target_score\n",
    "\n",
    "def acc_pre_rec_f1(predictions, true_labels):\n",
    "    try:\n",
    "        y_pred = np.asarray(predictions)\n",
    "        y_true = np.asarray(true_labels)\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting inputs to numpy arrays: {e}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    if y_pred.shape != y_true.shape:\n",
    "        raise ValueError(\"Predictions and true labels must have the same shape.\")\n",
    "\n",
    "    # TP: y_pred == 1 AND y_true == 1\n",
    "    TP = np.sum((y_pred == 1) & (y_true == 1))\n",
    "    # TN: y_pred == 0 AND y_true == 0\n",
    "    TN = np.sum((y_pred == 0) & (y_true == 0))\n",
    "    # FP: y_pred == 1 AND y_true == 0\n",
    "    FP = np.sum((y_pred == 1) & (y_true == 0))\n",
    "    # FN: y_pred == 0 AND y_true == 1\n",
    "    FN = np.sum((y_pred == 0) & (y_true == 1))\n",
    "\n",
    "    # (TP + TN) / (TP + TN + FP + FN)\n",
    "    total_samples = TP + TN + FP + FN\n",
    "    accuracy = (TP + TN) / total_samples if total_samples > 0 else 0.0\n",
    "\n",
    "    # TP / (TP + FP)\n",
    "    precision_denominator = TP + FP\n",
    "    precision = TP / precision_denominator if precision_denominator > 0 else 0.0\n",
    "\n",
    "    # TP / (TP + FN)\n",
    "    recall_denominator = TP + FN\n",
    "    recall = TP / recall_denominator if recall_denominator > 0 else 0.0\n",
    "\n",
    "    # 2 * (Precision * Recall) / (Precision + Recall)\n",
    "    f1_denominator = precision + recall\n",
    "    f1_score = 2 * (precision * recall) / f1_denominator if f1_denominator > 0 else 0.0\n",
    "    return accuracy, precision, recall, f1_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as nnf\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "from config import config\n",
    "from datasets import batch_by_size\n",
    "\n",
    "\n",
    "class BaseModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def init_weight(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, head, relation, tail):\n",
    "        pass\n",
    "\n",
    "    def dist(self, head, relation, tail):\n",
    "        pass\n",
    "\n",
    "    def score(self, head, relation, tail):\n",
    "        pass\n",
    "\n",
    "    def prob_logit(self, head, relation, tail):\n",
    "        pass\n",
    "\n",
    "    def constraint(self):\n",
    "        pass\n",
    "\n",
    "    def prob(self, head, relation, tail):\n",
    "        return nnf.softmax(self.prob_logit(head, relation, tail), dim=-1)\n",
    "\n",
    "    def pair_loss(self, head, relation, tail, head_bad, tail_bad):\n",
    "        d_good = self.dist(head, relation, tail)\n",
    "        d_bad = self.dist(head_bad, relation, tail_bad)\n",
    "        return nnf.relu(self.margin + d_good - d_bad)\n",
    "\n",
    "    def softmax_loss(self, head, relation, tail, truth):\n",
    "        probs = self.prob(head, relation, tail)\n",
    "        n = probs.size(0)\n",
    "        truth_probs = torch.log(probs[torch.arange(0, n).type(torch.LongTensor).cuda(), truth] + 1e-30)\n",
    "        return -truth_probs\n",
    "\n",
    "class BaseModel(object):\n",
    "    def __init__(self):\n",
    "        self.mdl = None # type: BaseModule\n",
    "        self.weight_decay = 0\n",
    "\n",
    "    def train(self, train_data, corrupter, tester):\n",
    "        pass\n",
    "\n",
    "    def save(self, filename):\n",
    "        torch.save(self.mdl.state_dict(), filename)\n",
    "\n",
    "    def load(self, filename):\n",
    "        state_dict = torch.load(filename, map_location=lambda storage, location: storage.cuda(), weights_only=True)\n",
    "        self.mdl.load_state_dict(state_dict)\n",
    "\n",
    "    def _ensure_optimizer(self):\n",
    "        if not hasattr(self, 'opt'):\n",
    "            self.opt = Adam(self.mdl.parameters(), weight_decay=self.weight_decay)\n",
    "\n",
    "    def evaluate(self, test_data, n_entity, heads, tails, filt=True):\n",
    "        \"\"\"\n",
    "        Evaluate the model on Link Prediction task.\n",
    "        \"\"\"\n",
    "        mr_total = mrr_total = 0\n",
    "        k_list = [1, 3, 10]\n",
    "        hits_total = [0] * len(k_list)\n",
    "\n",
    "        count = 0\n",
    "        with torch.no_grad():  # Thay volatile=True\n",
    "            for batch_head, batch_relation, batch_tail in batch_by_size(config().test_batch_size, *test_data):\n",
    "                batch_size = batch_head.size(0)\n",
    "\n",
    "                all_var = torch.arange(0, n_entity).unsqueeze(0).expand(batch_size, n_entity).long().cuda()\n",
    "                head_var = batch_head.unsqueeze(1).expand(batch_size, n_entity).cuda()\n",
    "                relation_var = batch_relation.unsqueeze(1).expand(batch_size, n_entity).cuda()\n",
    "                tail_var = batch_tail.unsqueeze(1).expand(batch_size, n_entity).cuda()\n",
    "\n",
    "                batch_head_scores = self.mdl.score(all_var, relation_var, tail_var)\n",
    "                batch_tail_scores = self.mdl.score(head_var, relation_var, all_var)\n",
    "            \n",
    "                # Convert to numpy if needed\n",
    "                batch_head_scores = batch_head_scores.detach()\n",
    "                batch_tail_scores = batch_tail_scores.detach()\n",
    "\n",
    "                for head, relation, tail, head_scores, tail_scores in zip(batch_head, batch_relation, batch_tail, batch_head_scores, batch_tail_scores):\n",
    "                    head_id, relation_id, tail_id = head.item(), relation.item(), tail.item()\n",
    "                    if filt:\n",
    "                        key_head = (tail_id, relation_id)\n",
    "                        if key_head in heads and heads[key_head]._nnz() > 1:\n",
    "                            tmp = head_scores[head_id].item()\n",
    "                            head_scores += heads[key_head].cuda() * 1e30\n",
    "                            head_scores[head_id] = tmp\n",
    "                            \n",
    "                        key_tail = (head_id, relation_id)\n",
    "                        if key_tail in tails and tails[key_tail]._nnz() > 1:\n",
    "                            tmp = tail_scores[tail_id].item()\n",
    "                            tail_scores += tails[key_tail].cuda() * 1e30\n",
    "                            tail_scores[tail_id] = tmp\n",
    "\n",
    "                    head_mr, head_mrr, head_hits, head_target_score = mr_mrr_hitsk(scores=head_scores, target=head_id, k_list=k_list)\n",
    "                    tail_mr, tail_mrr, tail_hits, tail_target_score = mr_mrr_hitsk(scores=tail_scores, target=tail_id, k_list=k_list)                    \n",
    "                    \n",
    "                    mr_total += (head_mr + tail_mr)\n",
    "                    mrr_total += (head_mrr + tail_mrr)\n",
    "                    hits_total = [(hits_total[i] + head_hits[i] + tail_hits[i]) for i in range(len(k_list))]\n",
    "                    count += 2\n",
    "                    \n",
    "        mr_rate = mr_total / count\n",
    "        mrr_rate = mrr_total / count\n",
    "        hits_rate = [hit_total / count for hit_total in hits_total]\n",
    "        \n",
    "        metrics_str = f\"MR = {mr_rate}\\nMRR = {mrr_rate}\\n\"\n",
    "        for i in range(len(k_list)):\n",
    "            metrics_str += f\"Hit@{k_list[i]} = {hits_rate[i]}\\n\"\n",
    "\n",
    "        logging.info(metrics_str)\n",
    "        print(metrics_str)\n",
    "        return mrr_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TransE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T08:45:15.236823Z",
     "iopub.status.busy": "2025-10-09T08:45:15.236627Z",
     "iopub.status.idle": "2025-10-09T08:45:15.253064Z",
     "shell.execute_reply": "2025-10-09T08:45:15.252396Z",
     "shell.execute_reply.started": "2025-10-09T08:45:15.236809Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "import logging\n",
    "import os\n",
    "\n",
    "class TransEModule(BaseModule):\n",
    "    def __init__(self, n_entity, n_relation, config):\n",
    "        super().__init__()\n",
    "        self.p = config.p\n",
    "        self.margin = config.margin\n",
    "        self.temp = config.get('temp', 1)\n",
    "        self.relation_embed = nn.Embedding(n_relation, config.dim)\n",
    "        self.entity_embed = nn.Embedding(n_entity, config.dim)\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        for param in self.parameters():\n",
    "            param.data.normal_(1 / param.size(1) ** 0.5)\n",
    "            param.data.renorm_(2, 0, 1)\n",
    "\n",
    "    def forward(self, head, relation, tail):\n",
    "        return torch.norm(self.entity_embed(tail) - self.entity_embed(head) - self.relation_embed(relation) + 1e-30, p=self.p, dim=-1)\n",
    "\n",
    "    def dist(self, head, relation, tail):\n",
    "        return self.forward(head, relation, tail)\n",
    "\n",
    "    def score(self, head, relation, tail):\n",
    "        return self.forward(head, relation, tail)\n",
    "\n",
    "    def prob_logit(self, head, relation, tail):\n",
    "        return -self.forward(head, relation ,tail) / self.temp\n",
    "\n",
    "    def constraint(self):\n",
    "        self.entity_embed.weight.data.renorm_(2, 0, 1)\n",
    "        self.relation_embed.weight.data.renorm_(2, 0, 1)\n",
    "\n",
    "class TransE(BaseModel):\n",
    "    def __init__(self, n_entity, n_relation, config):\n",
    "        super().__init__()\n",
    "        self.mdl = TransEModule(n_entity, n_relation, config)\n",
    "        self.mdl.cuda()\n",
    "        self.config = config\n",
    "\n",
    "    def train(self, train_data, corrupter, tester):\n",
    "        head, relation, tail = train_data\n",
    "        n_train = len(head)\n",
    "        n_epoch = self.config.n_epoch\n",
    "        n_batch = self.config.n_batch\n",
    "        optimizer = Adam(self.mdl.parameters())\n",
    "\n",
    "        best_perf = 0\n",
    "        for epoch in range(n_epoch):\n",
    "            rand_idx = torch.randperm(n_train)\n",
    "            head = head[rand_idx]\n",
    "            relation = relation[rand_idx]\n",
    "            tail = tail[rand_idx]\n",
    "\n",
    "            head_corrupted, tail_corrupted = corrupter.corrupt(head, relation, tail)\n",
    "            head_cuda = head.cuda()\n",
    "            relation_cuda = relation.cuda()\n",
    "            tail_cuda = tail.cuda()\n",
    "            head_corrupted = head_corrupted.cuda()\n",
    "            tail_corrupted = tail_corrupted.cuda()\n",
    "\n",
    "            epoch_loss = 0\n",
    "            for h0, r, t0, h1, t1 in batch_by_num(n_batch, head_cuda, relation_cuda, tail_cuda, head_corrupted, tail_corrupted, n_sample=n_train):\n",
    "                self.mdl.zero_grad()\n",
    "\n",
    "                loss = torch.sum(self.mdl.pair_loss(Variable(h0), Variable(r), Variable(t0), Variable(h1), Variable(t1)))\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "                self.mdl.constraint()\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            logging.info('Epoch %d/%d, Loss=%f', epoch + 1, n_epoch, epoch_loss / n_train)\n",
    "            if ((epoch + 1) % self.config.epoch_per_test == 0):\n",
    "                test_perf = tester()\n",
    "                if (test_perf > best_perf):\n",
    "                    task_dir = './output/' + config().task.dir + '/models'\n",
    "                    os.makedirs(task_dir, exist_ok=True)\n",
    "                    self.save(os.path.join(task_dir, self.config.model_file))\n",
    "                    \n",
    "                    best_perf = test_perf\n",
    "        return best_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TransD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T08:45:15.221101Z",
     "iopub.status.busy": "2025-10-09T08:45:15.220904Z",
     "iopub.status.idle": "2025-10-09T08:45:15.235924Z",
     "shell.execute_reply": "2025-10-09T08:45:15.235225Z",
     "shell.execute_reply.started": "2025-10-09T08:45:15.221086Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "import logging\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "class TransDModule(BaseModule):\n",
    "    def __init__(self, n_entity, n_relation, config):\n",
    "        super().__init__()\n",
    "        self.margin = config.margin\n",
    "        self.p = config.p\n",
    "        self.temp = config.get('temp', 1)\n",
    "        self.relation_embed = nn.Embedding(n_relation, config.dim)\n",
    "        self.entity_embed = nn.Embedding(n_entity, config.dim)\n",
    "        self.proj_relation_embed = nn.Embedding(n_relation, config.dim)\n",
    "        self.proj_entity_embed = nn.Embedding(n_entity, config.dim)\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        for param in self.parameters():\n",
    "            param.data.normal_(1 / param.size(1) ** 0.5)\n",
    "            param.data.renorm_(2, 0, 1)\n",
    "\n",
    "    def forward(self, head, relation, tail):\n",
    "        head_proj = self.entity_embed(head) +\\\n",
    "                   torch.sum(self.proj_entity_embed(head) * self.entity_embed(head), dim=-1, keepdim=True) * self.proj_relation_embed(relation)\n",
    "        tail_proj = self.entity_embed(tail) +\\\n",
    "                   torch.sum(self.proj_entity_embed(tail) * self.entity_embed(tail), dim=-1, keepdim=True) * self.proj_relation_embed(relation)\n",
    "        return torch.norm(tail_proj - self.relation_embed(relation) - head_proj + 1e-30, p=self.p, dim=-1)\n",
    "\n",
    "    def dist(self, head, relation, tail):\n",
    "        return self.forward(head, relation, tail)\n",
    "\n",
    "    def score(self, head, relation, tail):\n",
    "        return self.forward(head, relation, tail)\n",
    "\n",
    "    def prob_logit(self, head, relation, tail):\n",
    "        return -self.forward(head, relation ,tail) / self.temp\n",
    "\n",
    "    def constraint(self):\n",
    "        for param in self.parameters():\n",
    "            param.data.renorm_(2, 0, 1)\n",
    "\n",
    "class TransD(BaseModel):\n",
    "    def __init__(self, n_entity, n_relation, config):\n",
    "        super().__init__()\n",
    "        self.mdl = TransDModule(n_entity, n_relation, config)\n",
    "        self.mdl.cuda()\n",
    "        self.config = config\n",
    "\n",
    "    def load_vec(self, path):\n",
    "        entity_mat = np.loadtxt(os.path.join(path, 'entity2vec.vec'))\n",
    "        self.mdl.entity_embed.weight.data.copy_(torch.from_numpy(entity_mat))\n",
    "\n",
    "        relation_mat = np.loadtxt(os.path.join(path, 'relation2vec.vec'))\n",
    "        n_relation = relation_mat.shape[0]\n",
    "        self.mdl.relation_embed.weight.data.copy_(torch.from_numpy(relation_mat))\n",
    "\n",
    "        a_mat = np.loadtxt(os.path.join(path, 'A.vec'))\n",
    "        self.mdl.proj_relation_embed.weight.data.copy_(torch.from_numpy(a_mat[:n_relation, :]))\n",
    "        self.mdl.proj_entity_embed.weight.data.copy_(torch.from_numpy(a_mat[n_relation:, :]))\n",
    "        self.mdl.cuda()\n",
    "\n",
    "    def train(self, train_data, corrupter, tester):\n",
    "        head, relation, tail = train_data\n",
    "        n_train = len(head)\n",
    "        n_epoch = self.config.n_epoch\n",
    "        n_batch = self.config.n_batch\n",
    "        optimizer = Adam(self.mdl.parameters())\n",
    "\n",
    "        best_perf = 0\n",
    "        for epoch in range(n_epoch):\n",
    "            rand_idx = torch.randperm(n_train)\n",
    "            head = head[rand_idx]\n",
    "            relation = relation[rand_idx]\n",
    "            tail = tail[rand_idx]\n",
    "\n",
    "            head_corrupted, tail_corrupted = corrupter.corrupt(head, relation, tail)\n",
    "            head_cuda = head.cuda()\n",
    "            relation_cuda = relation.cuda()\n",
    "            tail_cuda = tail.cuda()\n",
    "            head_corrupted = head_corrupted.cuda()\n",
    "            tail_corrupted = tail_corrupted.cuda()\n",
    "\n",
    "            epoch_loss = 0\n",
    "            for h0, r, t0, h1, t1 in batch_by_num(n_batch, head_cuda, relation_cuda, tail_cuda, head_corrupted, tail_corrupted, n_sample=n_train):\n",
    "                self.mdl.zero_grad()\n",
    "\n",
    "                loss = torch.sum(self.mdl.pair_loss(Variable(h0), Variable(r), Variable(t0), Variable(h1), Variable(t1)))\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "                self.mdl.constraint()\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            logging.info('Epoch %d/%d, Loss=%f', epoch + 1, n_epoch, epoch_loss / n_train)\n",
    "            if ((epoch + 1) % self.config.epoch_per_test == 0):\n",
    "                test_perf = tester()\n",
    "                if (test_perf > best_perf):\n",
    "                    task_dir = './output/' + config().task.dir + '/models'\n",
    "                    os.makedirs(task_dir, exist_ok=True)\n",
    "                    self.save(os.path.join(task_dir, self.config.model_file))\n",
    "                    \n",
    "                    best_perf = test_perf\n",
    "        return best_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DistMult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T08:45:15.200214Z",
     "iopub.status.busy": "2025-10-09T08:45:15.199923Z",
     "iopub.status.idle": "2025-10-09T08:45:15.220221Z",
     "shell.execute_reply": "2025-10-09T08:45:15.219536Z",
     "shell.execute_reply.started": "2025-10-09T08:45:15.200196Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "import logging\n",
    "import os\n",
    "\n",
    "class DistMultModule(BaseModule):\n",
    "    def __init__(self, n_entity, n_relation, config):\n",
    "        super().__init__()\n",
    "        sigma = 0.2\n",
    "        self.relation_embed = nn.Embedding(n_relation, config.dim)\n",
    "        self.relation_embed.weight.data.div_((config.dim / sigma ** 2) ** (1 / 6))\n",
    "        self.entity_embed = nn.Embedding(n_entity, config.dim)\n",
    "        self.entity_embed.weight.data.div_((config.dim / sigma ** 2) ** (1 / 6))\n",
    "\n",
    "    def forward(self, head, relation, tail):\n",
    "        return torch.sum(self.entity_embed(tail) * self.entity_embed(head) * self.relation_embed(relation), dim=-1)\n",
    "\n",
    "    def dist(self, head, relation, tail):\n",
    "        return -self.forward(head, relation, tail)\n",
    "    \n",
    "    def score(self, head, relation, tail):\n",
    "        return -self.forward(head, relation, tail)\n",
    "\n",
    "    def prob_logit(self, head, relation, tail):\n",
    "        return self.forward(head, relation, tail)\n",
    "\n",
    "class DistMult(BaseModel):\n",
    "    def __init__(self, n_entity, n_relation, config):\n",
    "        super().__init__()\n",
    "        self.mdl = DistMultModule(n_entity, n_relation, config)\n",
    "        self.mdl.cuda()\n",
    "        self.config = config\n",
    "        self.weight_decay = config.lam / config.n_batch\n",
    "\n",
    "    def train(self, train_data, corrupter, tester):\n",
    "        head, relation, tail = train_data\n",
    "        n_train = len(head)\n",
    "        n_epoch = self.config.n_epoch\n",
    "        n_batch = self.config.n_batch\n",
    "        optimizer = Adam(self.mdl.parameters(), weight_decay=self.weight_decay)\n",
    "    \n",
    "        best_perf = 0\n",
    "        for epoch in range(n_epoch):\n",
    "            epoch_loss = 0\n",
    "            if (epoch % self.config.sample_freq == 0):\n",
    "                rand_idx = torch.randperm(n_train)\n",
    "                head = head[rand_idx]\n",
    "                relation = relation[rand_idx]\n",
    "                tail = tail[rand_idx]\n",
    "\n",
    "                head_corrupted, relation_corrupted, tail_corrupted = corrupter.corrupt(head, relation, tail)\n",
    "                head_corrupted = head_corrupted.cuda()\n",
    "                relation_corrupted = relation_corrupted.cuda()\n",
    "                tail_corrupted = tail_corrupted.cuda()\n",
    "\n",
    "            for hs, rs, ts in batch_by_num(n_batch, head_corrupted, relation_corrupted, tail_corrupted, n_sample=n_train):\n",
    "                self.mdl.zero_grad()\n",
    "                label = torch.zeros(len(hs)).type(torch.LongTensor).cuda()\n",
    "\n",
    "                loss = torch.sum(self.mdl.softmax_loss(Variable(hs), Variable(rs), Variable(ts), label))\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            logging.info('Epoch %d/%d, Loss=%f', epoch + 1, n_epoch, epoch_loss / n_train)\n",
    "            if ((epoch + 1) % self.config.epoch_per_test == 0):\n",
    "                test_perf = tester()\n",
    "                if (test_perf > best_perf):\n",
    "                    task_dir = './output/' + config().task.dir + '/models'\n",
    "                    os.makedirs(task_dir, exist_ok=True)\n",
    "                    self.save(os.path.join(task_dir, self.config.model_file))\n",
    "\n",
    "                    best_perf = test_perf\n",
    "        return best_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ComplEx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T08:45:15.178328Z",
     "iopub.status.busy": "2025-10-09T08:45:15.178106Z",
     "iopub.status.idle": "2025-10-09T08:45:15.199127Z",
     "shell.execute_reply": "2025-10-09T08:45:15.198330Z",
     "shell.execute_reply.started": "2025-10-09T08:45:15.178303Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "import logging\n",
    "import os\n",
    "\n",
    "class ComplExModule(BaseModule):\n",
    "    def __init__(self, n_entity, n_relation, config):\n",
    "        super().__init__()\n",
    "        self.sigma = 0.2\n",
    "        self.relation_re_embed = nn.Embedding(n_relation, config.dim)\n",
    "        self.relation_im_embed = nn.Embedding(n_relation, config.dim)\n",
    "        self.entity_re_embed = nn.Embedding(n_entity, config.dim)\n",
    "        self.entity_im_embed = nn.Embedding(n_entity, config.dim)\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        for param in self.parameters():\n",
    "            param.data.div_((config.dim / self.sigma ** 2) ** (1 / 6))\n",
    "\n",
    "    def forward(self, head, relation, tail):\n",
    "        return torch.sum(self.relation_re_embed(relation) * self.entity_re_embed(head) * self.entity_re_embed(tail), dim=-1) \\\n",
    "            + torch.sum(self.relation_re_embed(relation) * self.entity_im_embed(head) * self.entity_im_embed(tail), dim=-1) \\\n",
    "            + torch.sum(self.relation_im_embed(relation) * self.entity_re_embed(head) * self.entity_im_embed(tail), dim=-1) \\\n",
    "            - torch.sum(self.relation_im_embed(relation) * self.entity_im_embed(head) * self.entity_re_embed(tail), dim=-1)\n",
    "\n",
    "    def dist(self, head, relation, tail):\n",
    "        return -self.forward(head, relation, tail)\n",
    "    \n",
    "    def score(self, head, relation, tail):\n",
    "        return -self.forward(head, relation, tail)\n",
    "\n",
    "    def prob_logit(self, head, relation, tail):\n",
    "        return self.forward(head, relation, tail)\n",
    "\n",
    "class ComplEx(BaseModel):\n",
    "    def __init__(self, n_entity, n_relation, config):\n",
    "        super().__init__()\n",
    "        self.mdl = ComplExModule(n_entity, n_relation, config)\n",
    "        self.mdl.cuda()\n",
    "        self.config = config\n",
    "        self.weight_decay = config.lam / config.n_batch\n",
    "\n",
    "    def train(self, train_data, corrupter, tester):\n",
    "        head, relation, tail = train_data\n",
    "        n_train = len(head)\n",
    "        n_epoch = self.config.n_epoch\n",
    "        n_batch = self.config.n_batch\n",
    "        optimizer = Adam(self.mdl.parameters(), weight_decay=self.weight_decay)\n",
    "\n",
    "        best_perf = 0\n",
    "        for epoch in range(n_epoch):\n",
    "            epoch_loss = 0\n",
    "            if (epoch % self.config.sample_freq == 0):\n",
    "                rand_idx = torch.randperm(n_train)\n",
    "                head = head[rand_idx]\n",
    "                relation = relation[rand_idx]\n",
    "                tail = tail[rand_idx]\n",
    "\n",
    "                head_corrupted, relation_corrupted, tail_corrupted = corrupter.corrupt(head, relation, tail)\n",
    "                head_corrupted = head_corrupted.cuda()\n",
    "                relation_corrupted = relation_corrupted.cuda()\n",
    "                tail_corrupted = tail_corrupted.cuda()\n",
    "\n",
    "            for hs, rs, ts in batch_by_num(n_batch, head_corrupted, relation_corrupted, tail_corrupted, n_sample=n_train):\n",
    "                self.mdl.zero_grad()\n",
    "                label = torch.zeros(len(hs)).type(torch.LongTensor).cuda()\n",
    "\n",
    "                loss = torch.sum(self.mdl.softmax_loss(Variable(hs), Variable(rs), Variable(ts), label))\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            logging.info('Epoch %d/%d, Loss=%f', epoch + 1, n_epoch, epoch_loss / n_train)\n",
    "            if ((epoch + 1) % self.config.epoch_per_test == 0):\n",
    "                test_perf = tester()\n",
    "                if (test_perf > best_perf):\n",
    "                    task_dir = './output/' + config().task.dir + '/models'\n",
    "                    os.makedirs(task_dir, exist_ok=True)\n",
    "                    self.save(os.path.join(task_dir, self.config.model_file))\n",
    "\n",
    "                    best_perf = test_perf\n",
    "        return best_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KBGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "\n",
    "class Component():\n",
    "    def __init__(self, model_type, role='discriminator'):\n",
    "        \"\"\"\n",
    "        model_type = [\"TransE\", \"TransD\", \"DistMult\", \"ComplEx\"]\n",
    "        role = [\"discriminator\", \"generator\"]\n",
    "        \"\"\"\n",
    "        self.model_type = model_type\n",
    "        self.model = None \n",
    "        self.model_config = None\n",
    "        self.role = role\n",
    "\n",
    "    def load_model(self, model_type, role, n_entity, n_relation, model_path=None):\n",
    "        print(f'Loading pretrained {self.model_type} model...')\n",
    "        self.model_type = model_type\n",
    "        self.role = role\n",
    "        self.model_config = config()[self.model_type]\n",
    "        output_dir = './output/' + config().task.dir + '/models'\n",
    "        if self.model_type == 'TransE':\n",
    "            self.model = TransE(n_entity, n_relation, self.model_config)\n",
    "        elif self.model_type == 'TransD':\n",
    "            self.model = TransD(n_entity, n_relation, self.model_config)\n",
    "        elif self.model_type == 'DistMult':\n",
    "            self.model = DistMult(n_entity, n_relation, self.model_config)\n",
    "        elif self.model_type == 'ComplEx':\n",
    "            self.model = ComplEx(n_entity, n_relation, self.model_config)\n",
    "        self.model.load(model_path if model_path is not None else os.path.join(output_dir, self.model_config.model_file))\n",
    "\n",
    "    def pretrain(self, n_entity, n_relation, heads, tails, train_data, valid_data):    \n",
    "        overwrite_config_with_args([\"--pretrain_config=\" + self.model_type])\n",
    "        overwrite_config_with_args([\"--log.prefix=\" + self.model_type + '_'])\n",
    "        logger_init()\n",
    "        \n",
    "        print(f'Pretraining {self.model_type} model...')\n",
    "        self.model_config = config()[self.model_type]\n",
    "        \n",
    "        if self.model_type == 'TransE':\n",
    "            corrupter = BernCorrupter(train_data, n_entity, n_relation)\n",
    "            self.model = TransE(n_entity, n_relation, self.model_config)\n",
    "        elif self.model_type == 'TransD':\n",
    "            corrupter = BernCorrupter(train_data, n_entity, n_relation)\n",
    "            self.model = TransD(n_entity, n_relation, self.model_config)\n",
    "        elif self.model_type == 'DistMult':\n",
    "            corrupter = BernCorrupterMulti(train_data, n_entity, n_relation, self.model_config.n_sample)\n",
    "            self.model = DistMult(n_entity, n_relation, self.model_config)\n",
    "        elif self.model_type == 'ComplEx':\n",
    "            corrupter = BernCorrupterMulti(train_data, n_entity, n_relation, self.model_config.n_sample)\n",
    "            self.model = ComplEx(n_entity, n_relation, self.model_config)\n",
    "            \n",
    "        tester = lambda: self.model.evaluate(valid_data, n_entity, heads, tails)\n",
    "        self.model.train(train_data, corrupter, tester)\n",
    "\n",
    "    def step(self, head, relation, tail, **kwargs):\n",
    "        \"\"\"\n",
    "        Unified step function that handles both generator and discriminator logic.\n",
    "        \n",
    "        For generator:\n",
    "            kwargs: n_sample=1, temperature=1.0, train=True\n",
    "            Returns generator coroutine (yields samples, receives rewards)\n",
    "            \n",
    "        For discriminator:\n",
    "            kwargs: head_fake, tail_fake, train=True\n",
    "            Returns (losses, rewards)\n",
    "        \"\"\"\n",
    "        if self.role == 'generator':\n",
    "            return self._generator_step(head, relation, tail, **kwargs)\n",
    "        elif self.role == 'discriminator':\n",
    "            return self._discriminator_step(head, relation, tail, **kwargs)\n",
    "        else:\n",
    "            raise ValueError(\"Role must be either 'generator' or 'discriminator'\")\n",
    "    \n",
    "    def _generator_step(self, head, relation, tail, n_sample=1, temperature=1.0, train=True):\n",
    "        \"\"\"Generator step: sample fake triples and update with REINFORCE\"\"\"\n",
    "        # Forward pass: generate samples\n",
    "        n, m = tail.size()\n",
    "        relation_var = Variable(relation.cuda())\n",
    "        head_var = Variable(head.cuda())\n",
    "        tail_var = Variable(tail.cuda())\n",
    "\n",
    "        logits = self.model.prob_logit(head_var, relation_var, tail_var) / temperature\n",
    "        probs = nnf.softmax(logits, dim=-1)\n",
    "        row_idx = torch.arange(0, n).type(torch.LongTensor).unsqueeze(1).expand(n, n_sample)\n",
    "        sample_idx = torch.multinomial(probs, n_sample, replacement=True)\n",
    "        sample_heads = head[row_idx, sample_idx.data.cpu()]\n",
    "        sample_tails = tail[row_idx, sample_idx.data.cpu()]\n",
    "        \n",
    "        # Yield samples to get rewards from discriminator\n",
    "        rewards = yield sample_heads, sample_tails\n",
    "        \n",
    "        # Backward pass: update generator with REINFORCE\n",
    "        if train:            \n",
    "            self.model._ensure_optimizer()\n",
    "            self.model.mdl.zero_grad()\n",
    "            log_probs = nnf.log_softmax(logits, dim=-1)\n",
    "            reinforce_loss = -torch.sum(Variable(rewards) * log_probs[row_idx.cuda(), sample_idx.data])\n",
    "            reinforce_loss.backward()\n",
    "            self.model.opt.step()\n",
    "            self.model.mdl.constraint()\n",
    "\n",
    "        yield None\n",
    "\n",
    "    def _discriminator_step(self, head, relation, tail, head_fake=None, tail_fake=None, train=True):\n",
    "        \"\"\"Discriminator step: distinguish real from fake triples\"\"\"\n",
    "        if head_fake is None or tail_fake is None:\n",
    "            raise ValueError(\"head_fake and tail_fake must be provided for discriminator step\")\n",
    "        \n",
    "        # Forward pass: compute losses and scores\n",
    "        head_var = Variable(head.cuda())\n",
    "        relation_var = Variable(relation.cuda())\n",
    "        tail_var = Variable(tail.cuda())\n",
    "        head_fake_var = Variable(head_fake.cuda())\n",
    "        tail_fake_var = Variable(tail_fake.cuda())\n",
    "        \n",
    "        losses = self.model.mdl.pair_loss(head_var, relation_var, tail_var, head_fake_var, tail_fake_var)\n",
    "        fake_scores = self.model.mdl.score(head_fake_var, relation_var, tail_fake_var)\n",
    "                \n",
    "        # Backward pass: update discriminator\n",
    "        if train:\n",
    "            self.model._ensure_optimizer()\n",
    "            self.model.mdl.zero_grad()\n",
    "            torch.sum(losses).backward()\n",
    "            self.model.opt.step()\n",
    "            self.model.mdl.constraint()\n",
    "        \n",
    "        return losses.data, -fake_scores.data\n",
    "        \n",
    "    def evaluate(self, test_data, n_entity, heads, tails):\n",
    "        \"\"\"\n",
    "        Evaluate the model on Link Prediction task.\n",
    "        \"\"\"\n",
    "        output_dir = './output/' + config().task.dir + '/models'\n",
    "        self.model.load(os.path.join(output_dir, self.model_config.model_file))\n",
    "        print(f'Testing {self.model_type} model...')\n",
    "        self.model.evaluate(test_data, n_entity, heads, tails)\n",
    "\n",
    "class KBGAN():\n",
    "    def __init__(self, discriminator_type=\"TransE\", generator_type=\"DistMult\"):\n",
    "        \"\"\"\n",
    "        discriminator_type = [\"TransE\", \"TransD\"]\n",
    "        generator_type = [\"DistMult\", \"ComplEx\"]\n",
    "        \"\"\"    \n",
    "        self.discriminator_type = discriminator_type\n",
    "        self.discriminator = Component(model_type=discriminator_type, role='discriminator')\n",
    "        self.generator_type = generator_type\n",
    "        self.generator = Component(model_type=generator_type, role='generator')\n",
    "\n",
    "    def load_discriminator(self, n_entity, n_relation, test_data, heads, tails, disc_model_path=None):\n",
    "        self.discriminator.load_model(self.discriminator_type, 'discriminator', n_entity, n_relation, disc_model_path)\n",
    "        self.discriminator.evaluate(test_data, n_entity, heads, tails)\n",
    "\n",
    "    def load_generator(self, n_entity, n_relation, test_data, heads, tails, gen_model_path=None):\n",
    "        self.generator.load_model(self.discriminator_type, 'generator', n_entity, n_relation, gen_model_path)\n",
    "        self.generator.evaluate(test_data, n_entity, heads, tails)\n",
    "        \n",
    "    def pretrain(self, n_entity, n_relation, heads, tails, train_data, valid_data, test_data):\n",
    "        if not isinstance(train_data[0], torch.Tensor):\n",
    "            train_data = [torch.LongTensor(vec) for vec in train_data]\n",
    "        if not isinstance(valid_data[0], torch.Tensor):\n",
    "            valid_data = [torch.LongTensor(vec) for vec in valid_data]\n",
    "        if not isinstance(test_data[0], torch.Tensor):\n",
    "            test_data = [torch.LongTensor(vec) for vec in test_data]\n",
    "\n",
    "        # Pretrain discriminator\n",
    "        self.discriminator.pretrain(n_entity, n_relation, heads, tails, train_data, valid_data)\n",
    "        self.discriminator.evaluate(test_data, n_entity, heads, tails)\n",
    "\n",
    "        # Pretrain generator\n",
    "        self.generator.pretrain(n_entity, n_relation, heads, tails, train_data, valid_data)\n",
    "        self.generator.evaluate(test_data, n_entity, heads, tails)\n",
    "\n",
    "    def train_n_test(self, n_entity, n_relation, heads, tails, train_data, valid_data, test_data):\n",
    "        if not isinstance(train_data[0], torch.Tensor):\n",
    "            train_data = [torch.LongTensor(vec) for vec in train_data]\n",
    "        if not isinstance(valid_data[0], torch.Tensor):\n",
    "            valid_data = [torch.LongTensor(vec) for vec in valid_data]\n",
    "        if not isinstance(test_data[0], torch.Tensor):\n",
    "            test_data = [torch.LongTensor(vec) for vec in test_data]\n",
    "        \n",
    "        overwrite_config_with_args([\"--log.prefix=\" + self.discriminator_type + '-' + self.generator_type + \"_\"])\n",
    "        logger_init()\n",
    "\n",
    "        generator_config = config()[config().g_config]\n",
    "        discriminator_config = config()[config().d_config]\n",
    "\n",
    "        models = {'TransE': TransE, 'TransD': TransD, 'DistMult': DistMult, 'ComplEx': ComplEx}\n",
    "\n",
    "        # Load pretrained models into the Component instances\n",
    "        model_dir = './output/' + config().task.dir + '/models'\n",
    "        \n",
    "        # Initialize generator model if not already done\n",
    "        if self.generator.model is None:\n",
    "            self.generator.model_config = generator_config\n",
    "            self.generator.model = models[config().g_config](n_entity, n_relation, generator_config)\n",
    "        self.generator.model.load(os.path.join(model_dir, generator_config.model_file))\n",
    "        \n",
    "        # Initialize discriminator model if not already done\n",
    "        if self.discriminator.model is None:\n",
    "            self.discriminator.model_config = discriminator_config\n",
    "            self.discriminator.model = models[config().d_config](n_entity, n_relation, discriminator_config)\n",
    "        self.discriminator.model.load(os.path.join(model_dir, discriminator_config.model_file))\n",
    "\n",
    "        corrupter = BernCorrupterMulti(train_data, n_entity, n_relation, config().KBGAN.n_sample)\n",
    "        head, relation, tail = train_data\n",
    "        n_train = len(head)\n",
    "        n_epoch = config().KBGAN.n_epoch\n",
    "        n_batch = config().KBGAN.n_batch\n",
    "\n",
    "        model_name = 'gan_' + self.discriminator_type + '-dis_' + self.generator_type + '-gen_' + datetime.datetime.now().strftime(\"%m%d%H%M%S\") + '.mdl'\n",
    "        best_perf = 0\n",
    "        avg_reward = 0\n",
    "\n",
    "        print(f'Training KBGAN with {self.generator_type} as generator and {self.discriminator_type} as discriminator...')\n",
    "        for epoch in range(n_epoch):\n",
    "            epoch_d_loss = 0\n",
    "            epoch_reward = 0\n",
    "\n",
    "            head_cand, relation_cand, tail_cand = corrupter.corrupt(head, relation, tail, keep_truth=False)\n",
    "            for h, r, t, hs, rs, ts in batch_by_num(n_batch, head, relation, tail, head_cand, relation_cand, tail_cand, n_sample=n_train):\n",
    "                gen_step = self.generator.step(hs, rs, ts, temperature=config().KBGAN.temperature)\n",
    "                head_smpl, tail_smpl = next(gen_step)\n",
    "                \n",
    "                losses, rewards = self.discriminator.step(h, r, t, head_fake=head_smpl.squeeze(), tail_fake=tail_smpl.squeeze())\n",
    "                epoch_reward += torch.sum(rewards)\n",
    "\n",
    "                rewards = rewards - avg_reward\n",
    "                \n",
    "                # Update generator with rewards\n",
    "                try:\n",
    "                    gen_step.send(rewards.unsqueeze(1))\n",
    "                except StopIteration:\n",
    "                    pass\n",
    "                \n",
    "                epoch_d_loss += torch.sum(losses)\n",
    "                \n",
    "            avg_loss = epoch_d_loss / n_train\n",
    "            avg_reward = epoch_reward / n_train\n",
    "            logging.info('Epoch %d/%d, D_loss=%f, reward=%f', epoch + 1, n_epoch, avg_loss, avg_reward)\n",
    "            \n",
    "            if (epoch + 1) % config().KBGAN.epoch_per_test == 0:\n",
    "                perf = self.discriminator.model.evaluate(valid_data, n_entity, heads, tails)\n",
    "                if perf > best_perf:\n",
    "                    best_perf = perf\n",
    "                    save_dir = './output/' + config().task.dir + '/kbgan/'\n",
    "                    os.makedirs(save_dir, exist_ok=True)\n",
    "                    self.discriminator.model.save(os.path.join(save_dir, model_name))\n",
    "\n",
    "        save_dir = './output/' + config().task.dir + '/kbgan/'\n",
    "        self.discriminator.model.load(os.path.join(save_dir, model_name))\n",
    "        print('Testing KBGAN discriminator model...')\n",
    "        self.discriminator.model.evaluate(test_data, n_entity, heads, tails)\n",
    "\n",
    "    def evaluate(self, model_path, model_type, threshold = None, auto_threshold = True, eval_subdir='evaluation on TP'):\n",
    "        \"\"\"\n",
    "        Evaluate a pretrained model on Triple Classification task.\n",
    "        \n",
    "        Args:\n",
    "            model_type: Type of model to evaluate ['DistMult', 'ComplEx', 'TransE', 'TransD']\n",
    "            eval_subdir: Subdirectory name for evaluation data (default: 'evaluation on TP')\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary containing evaluation results\n",
    "        \"\"\"\n",
    "        # Configuration\n",
    "        base_task_name = config().task.dir  # e.g., 'wn18rr' or 'FB15k-237'\n",
    "        train_dir = f'./data/{base_task_name}'\n",
    "        eval_dir = f'./data/{eval_subdir}/{base_task_name}'\n",
    "        \n",
    "        print(f\"Loading data from:\")\n",
    "        print(f\"  Training dir: {train_dir}\")\n",
    "        print(f\"  Evaluation dir: {eval_dir}\")\n",
    "        \n",
    "        # Create kb_index from training directory\n",
    "        kb_index = index_entity_relation(\n",
    "            os.path.join(train_dir, 'train.txt'),\n",
    "            os.path.join(train_dir, 'valid.txt'),\n",
    "            os.path.join(train_dir, 'test.txt')\n",
    "        )\n",
    "        n_entity, n_relation = graph_size(kb_index)\n",
    "        print(f\"  Entities: {n_entity}, Relations: {n_relation}\")\n",
    "        \n",
    "        # Load evaluation data with labels\n",
    "        test_data = read_data(os.path.join(eval_dir, 'test.txt'), kb_index, with_label=True)\n",
    "        print(f\"  Test data: {len(test_data[0])} triples\")\n",
    "        test_labels = test_data[3]\n",
    "        test_data = test_data[:3]\n",
    "        \n",
    "        valid_data = read_data(os.path.join(eval_dir, 'valid.txt'), kb_index, with_label=True)\n",
    "        print(f\"  Valid data: {len(valid_data[0])} triples\")\n",
    "        valid_labels = valid_data[3]\n",
    "        valid_data = valid_data[:3]\n",
    "        \n",
    "        # Create filtering dictionaries\n",
    "        heads, tails = sparse_heads_tails(n_entity, train_data=None, valid_data=valid_data, test_data=test_data)\n",
    "        \n",
    "        # Load pretrained model\n",
    "        model_dir = f'./output/{base_task_name}/models'\n",
    "        model_config = config()[self.discriminator_type]\n",
    "        \n",
    "        print(f\"\\nLoading {self.discriminator_type} model from {model_dir}\")\n",
    "        \n",
    "        if self.discriminator_type == 'DistMult':\n",
    "            model = DistMult(n_entity, n_relation, model_config)\n",
    "        elif self.discriminator_type == 'ComplEx':\n",
    "            model = ComplEx(n_entity, n_relation, model_config)\n",
    "        elif self.discriminator_type == 'TransE':\n",
    "            model = TransE(n_entity, n_relation, model_config)\n",
    "        elif self.discriminator_type == 'TransD':\n",
    "            model = TransD(n_entity, n_relation, model_config)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model type: {self.discriminator_type}\")\n",
    "        \n",
    "        model.load(model_path if model_path is not None else os.path.join(model_dir, model_config.model_file))\n",
    "        print(f\"Model loaded successfully!\\n\")\n",
    "        \n",
    "        # Evaluate on triple classification\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Evaluating {model_path} on Triple Classification task\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        heads_test, relations_test, tails_test = test_data\n",
    "        predictions = [] \n",
    "        scores_list = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(len(heads_test)):\n",
    "                head = torch.LongTensor([heads_test[i]]).cuda()\n",
    "                relation = torch.LongTensor([relations_test[i]]).cuda()\n",
    "                tail = torch.LongTensor([tails_test[i]]).cuda()\n",
    "                \n",
    "                score = model.mdl.score(head, relation, tail).item()\n",
    "                scores_list.append(score)\n",
    "        \n",
    "        # Auto-compute threshold if needed\n",
    "        if threshold is None and auto_threshold:\n",
    "            # Use validation data to find optimal threshold\n",
    "            threshold = find_optimal_threshold(model, model_type, valid_data, valid_labels)\n",
    "            logging.info(f\"Auto-computed threshold: {threshold:.4f}\")\n",
    "        elif threshold is None:\n",
    "            threshold = 0.0\n",
    "            logging.info(f\"Using default threshold: {threshold:.4f}\")\n",
    "        \n",
    "        is_distance_based = model_type in ['TransE', 'TransD']\n",
    "        \n",
    "        \n",
    "        for score in scores_list:\n",
    "            if is_distance_based:\n",
    "                predictions.append(1 if score < threshold else 0)\n",
    "            else:       \n",
    "                predictions.append(0 if score < threshold else 1)\n",
    "       \n",
    "        \n",
    "        # Compute metrics\n",
    "        accuracy, precision, recall, f1 = acc_pre_rec_f1(predictions, test_labels)\n",
    "        \n",
    "        metrics_str = f\"\\nTriple Classification Results:\\n\"\n",
    "        metrics_str += f\"Threshold = {threshold:.4f}\\n\"\n",
    "        metrics_str += f\"Accuracy = {accuracy:.4f}\\n\"\n",
    "        metrics_str += f\"Precision = {precision:.4f}\\n\"\n",
    "        metrics_str += f\"Recall = {recall:.4f}\\n\"\n",
    "        metrics_str += f\"F1 Score = {f1:.4f}\\n\"\n",
    "        logging.info(metrics_str)\n",
    "\n",
    "        # # ======================================================\n",
    "        # # ✅ EXPORT TO EXCEL\n",
    "        # # ======================================================\n",
    "        # output_df = pd.DataFrame({\n",
    "        #     \"score\": scores_list,\n",
    "        #     \"prediction\": predictions,\n",
    "        #     \"label\": test_labels\n",
    "        # })\n",
    "\n",
    "        # output_df[\"threshold_used\"] = threshold\n",
    "\n",
    "        # csv_path = f\"./output/{base_task_name}/{os.path.basename(model_path)}.csv\"\n",
    "        # output_df.to_csv(csv_path, index=False)\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'threshold': threshold,\n",
    "            'predictions': predictions,\n",
    "            'scores': scores_list\n",
    "        }\n",
    "\n",
    "def find_optimal_threshold(model, model_type, validation_data, labels, n_thresholds=100):\n",
    "    \"\"\"\n",
    "    Find the optimal threshold for triple classification using validation data.\n",
    "\n",
    "    Args:\n",
    "        model: The model to evaluate\n",
    "        validation_data: Tuple of (heads, relations, tails)\n",
    "        labels: Ground truth labels for validation data\n",
    "        n_thresholds: Number of threshold values to try\n",
    "\n",
    "    Returns:\n",
    "        Optimal threshold value that maximizes F1 score\n",
    "    \"\"\"\n",
    "    heads, relations, tails = validation_data\n",
    "\n",
    "    # Compute scores for all validation samples\n",
    "    scores_list = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(heads)):\n",
    "            head = torch.LongTensor([heads[i]]).cuda()\n",
    "            relation = torch.LongTensor([relations[i]]).cuda()\n",
    "            tail = torch.LongTensor([tails[i]]).cuda()\n",
    "            score = model.mdl.score(head, relation, tail).item()\n",
    "            scores_list.append(score)\n",
    "\n",
    "    # Try different threshold values\n",
    "    min_score = min(scores_list)\n",
    "    max_score = max(scores_list)\n",
    "    threshold_values = np.linspace(min_score, max_score, n_thresholds)\n",
    "\n",
    "    best_f1 = 0\n",
    "    best_threshold = 0\n",
    "\n",
    "    # Determine if model is distance-based or similarity-based\n",
    "    is_distance_based = model_type in ['TransE', 'TransD']\n",
    " \n",
    "    for threshold in threshold_values:\n",
    "        predictions = []\n",
    "        for score in scores_list:\n",
    "            if is_distance_based:\n",
    "                predictions.append(1 if score < threshold else 0)\n",
    "            else:       \n",
    "                predictions.append(0 if score < threshold else 1)\n",
    "        \n",
    "        _, _, _, f1 = acc_pre_rec_f1(predictions, labels)\n",
    "        \n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "\n",
    "    logging.info(f\"Optimal threshold: {best_threshold:.4f} (F1: {best_f1:.4f})\")\n",
    "    return best_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PU cell for fixing bugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from:\n",
      "  Training dir: ./data/wn18rr\n",
      "  Evaluation dir: ./data/evaluation on TP/wn18rr\n",
      "  Entities: 40943, Relations: 11\n",
      "  Test data: 6268 triples\n",
      "  Valid data: 6068 triples\n",
      "\n",
      "Loading TransE model from ./output/wn18rr/models\n",
      "Model loaded successfully!\n",
      "\n",
      "============================================================\n",
      "Evaluating output/wn18rr/kbgan/gan_TransE-dis_DistMult-gen_1117155145.mdl on Triple Classification task\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     2331752683 17:56:40 Optimal threshold: 7.1507 (F1: 0.7924)\n",
      "     2331752683 17:56:40 Auto-computed threshold: 7.1507\n",
      "     2331752683 17:56:40 \n",
      "Triple Classification Results:\n",
      "Threshold = 7.1507\n",
      "Accuracy = 0.8028\n",
      "Precision = 0.8625\n",
      "Recall = 0.7205\n",
      "F1 Score = 0.7851\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main = KBGAN()\n",
    "result = main.evaluate('output/wn18rr/kbgan/gan_TransE-dis_DistMult-gen_1117155145.mdl', 'TransE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Could not parse nvidia-smi output. Defaulting to GPU 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3218/2408958816.py:32: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /opt/conda/conda-bld/pytorch_1729647429097/work/torch/csrc/utils/tensor_new.cpp:651.)\n",
      "  heads_sparse[k] = torch.sparse.FloatTensor(torch.LongTensor([list(heads[k])]), torch.ones(len(heads[k])), torch.Size([n_entity]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained TransE model...\n",
      "Loading pretrained DistMult model...\n",
      "Testing TransE model...\n",
      "hhehe\n",
      "MR = 5660.6748046875\n",
      "MRR = 0.1760859191417694\n",
      "Hit@1 = 0.006541161455009573\n",
      "Hit@3 = 0.33806636885768987\n",
      "Hit@10 = 0.40682833439693683\n",
      "\n",
      "Testing DistMult model...\n",
      "hhehe\n",
      "MR = 5310.08203125\n",
      "MRR = 0.3606793284416199\n",
      "Hit@1 = 0.30871091257179323\n",
      "Hit@3 = 0.3948627951499681\n",
      "Hit@10 = 0.44416081684747927\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.set_device(select_gpu())\n",
    "\n",
    "_config = config()\n",
    "_config.log.to_file = True\n",
    "# overwrite_config_with_args([\"--TransE.n_epoch=10\"])\n",
    "# overwrite_config_with_args([\"--TransE.epoch_per_test=10\"])\n",
    "# overwrite_config_with_args([\"--DistMult.n_epoch=10\"])\n",
    "# overwrite_config_with_args([\"--DistMult.epoch_per_test=10\"])\n",
    "# overwrite_config_with_args([\"--KBGAN.n_epoch=10\"])\n",
    "# overwrite_config_with_args([\"--KBGAN.epoch_per_test=10\"])\n",
    "\n",
    "# Load data\n",
    "task_dir = _config.task.dir\n",
    "task_dir = './data/' + task_dir\n",
    "kb_index = index_entity_relation(os.path.join(task_dir, 'train.txt'),\n",
    "                                    os.path.join(task_dir, 'valid.txt'),\n",
    "                                    os.path.join(task_dir, 'test.txt'))\n",
    "n_entity, n_relation = graph_size(kb_index)\n",
    "\n",
    "train_data = read_data(os.path.join(task_dir, 'train.txt'), kb_index)\n",
    "inplace_shuffle(*train_data)\n",
    "\n",
    "valid_data = read_data(os.path.join(task_dir, 'valid.txt'), kb_index)\n",
    "test_data = read_data(os.path.join(task_dir, 'test.txt'), kb_index)\n",
    "heads, tails = sparse_heads_tails(n_entity, train_data, valid_data, test_data)\n",
    "\n",
    "valid_data = [torch.LongTensor(vec) for vec in valid_data]\n",
    "test_data = [torch.LongTensor(vec) for vec in test_data]\n",
    "train_data = [torch.LongTensor(vec) for vec in train_data]\n",
    "\n",
    "main_model = KBGAN(discriminator_type=\"TransE\", generator_type=\"DistMult\")\n",
    "main_model.load_models(n_entity, n_relation, test_data, heads, tails, disc_model_path='output/wn18rr/models/transe.mdl', gen_model_path='output/wn18rr/models/distmult.mdl')\n",
    "# main_model.train_n_test(n_entity, n_relation, heads, tails, train_data, valid_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training KBGAN with DistMult as generator and TransE as discriminator...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      433983126 15:51:46 Epoch 1/5000, D_loss=0.062418, reward=-8.051137\n",
      "      433983126 15:51:46 Epoch 2/5000, D_loss=0.062277, reward=-8.047962\n",
      "      433983126 15:51:47 Epoch 3/5000, D_loss=0.063198, reward=-8.047464\n",
      "      433983126 15:51:47 Epoch 4/5000, D_loss=0.060982, reward=-8.056492\n",
      "      433983126 15:51:47 Epoch 5/5000, D_loss=0.063656, reward=-8.048387\n",
      "      433983126 15:51:48 Epoch 6/5000, D_loss=0.062375, reward=-8.055072\n",
      "      433983126 15:51:48 Epoch 7/5000, D_loss=0.061417, reward=-8.051257\n",
      "      433983126 15:51:49 Epoch 8/5000, D_loss=0.061189, reward=-8.055042\n",
      "      433983126 15:51:49 Epoch 9/5000, D_loss=0.061309, reward=-8.050263\n",
      "      433983126 15:51:49 Epoch 10/5000, D_loss=0.062492, reward=-8.051524\n",
      "      433983126 15:51:50 Epoch 11/5000, D_loss=0.060551, reward=-8.056904\n",
      "      433983126 15:51:50 Epoch 12/5000, D_loss=0.061429, reward=-8.046429\n",
      "      433983126 15:51:50 Epoch 13/5000, D_loss=0.060959, reward=-8.050442\n",
      "      433983126 15:51:51 Epoch 14/5000, D_loss=0.061519, reward=-8.047571\n",
      "      433983126 15:51:51 Epoch 15/5000, D_loss=0.061863, reward=-8.044993\n",
      "      433983126 15:51:52 Epoch 16/5000, D_loss=0.061556, reward=-8.041520\n",
      "      433983126 15:51:52 Epoch 17/5000, D_loss=0.060753, reward=-8.039950\n",
      "      433983126 15:51:52 Epoch 18/5000, D_loss=0.058292, reward=-8.048819\n",
      "      433983126 15:51:53 Epoch 19/5000, D_loss=0.059725, reward=-8.043442\n",
      "      433983126 15:51:53 Epoch 20/5000, D_loss=0.061108, reward=-8.041332\n",
      "      433983126 15:51:53 Epoch 21/5000, D_loss=0.058695, reward=-8.044953\n",
      "      433983126 15:51:54 Epoch 22/5000, D_loss=0.059610, reward=-8.046957\n",
      "      433983126 15:51:54 Epoch 23/5000, D_loss=0.059613, reward=-8.037459\n",
      "      433983126 15:51:55 Epoch 24/5000, D_loss=0.059256, reward=-8.037236\n",
      "      433983126 15:51:55 Epoch 25/5000, D_loss=0.059756, reward=-8.041917\n",
      "      433983126 15:51:55 Epoch 26/5000, D_loss=0.059222, reward=-8.036357\n",
      "      433983126 15:51:56 Epoch 27/5000, D_loss=0.057410, reward=-8.040835\n",
      "      433983126 15:51:56 Epoch 28/5000, D_loss=0.060626, reward=-8.031910\n",
      "      433983126 15:51:56 Epoch 29/5000, D_loss=0.056859, reward=-8.038645\n",
      "      433983126 15:51:57 Epoch 30/5000, D_loss=0.058379, reward=-8.034659\n",
      "      433983126 15:51:57 Epoch 31/5000, D_loss=0.057615, reward=-8.040295\n",
      "      433983126 15:51:58 Epoch 32/5000, D_loss=0.057508, reward=-8.034527\n",
      "      433983126 15:51:58 Epoch 33/5000, D_loss=0.055528, reward=-8.034062\n",
      "      433983126 15:51:58 Epoch 34/5000, D_loss=0.055180, reward=-8.036946\n",
      "      433983126 15:51:59 Epoch 35/5000, D_loss=0.057769, reward=-8.030030\n",
      "      433983126 15:51:59 Epoch 36/5000, D_loss=0.055681, reward=-8.035499\n",
      "      433983126 15:52:00 Epoch 37/5000, D_loss=0.056897, reward=-8.029051\n",
      "      433983126 15:52:00 Epoch 38/5000, D_loss=0.056123, reward=-8.025182\n",
      "      433983126 15:52:00 Epoch 39/5000, D_loss=0.056256, reward=-8.027905\n",
      "      433983126 15:52:01 Epoch 40/5000, D_loss=0.055816, reward=-8.022863\n",
      "      433983126 15:52:01 Epoch 41/5000, D_loss=0.056968, reward=-8.024985\n",
      "      433983126 15:52:01 Epoch 42/5000, D_loss=0.055738, reward=-8.023112\n",
      "      433983126 15:52:02 Epoch 43/5000, D_loss=0.053765, reward=-8.023817\n",
      "      433983126 15:52:02 Epoch 44/5000, D_loss=0.057134, reward=-8.018721\n",
      "      433983126 15:52:03 Epoch 45/5000, D_loss=0.056934, reward=-8.018047\n",
      "      433983126 15:52:03 Epoch 46/5000, D_loss=0.054504, reward=-8.024867\n",
      "      433983126 15:52:03 Epoch 47/5000, D_loss=0.054511, reward=-8.019551\n",
      "      433983126 15:52:04 Epoch 48/5000, D_loss=0.056300, reward=-8.014665\n",
      "      433983126 15:52:04 Epoch 49/5000, D_loss=0.054658, reward=-8.016537\n",
      "      433983126 15:52:04 Epoch 50/5000, D_loss=0.052942, reward=-8.014351\n",
      "      433983126 15:52:05 Epoch 51/5000, D_loss=0.056457, reward=-8.014172\n",
      "      433983126 15:52:05 Epoch 52/5000, D_loss=0.054078, reward=-8.014634\n",
      "      433983126 15:52:06 Epoch 53/5000, D_loss=0.053247, reward=-8.015921\n",
      "      433983126 15:52:06 Epoch 54/5000, D_loss=0.053768, reward=-8.014896\n",
      "      433983126 15:52:06 Epoch 55/5000, D_loss=0.053899, reward=-8.012708\n",
      "      433983126 15:52:07 Epoch 56/5000, D_loss=0.052444, reward=-8.013899\n",
      "      433983126 15:52:07 Epoch 57/5000, D_loss=0.054146, reward=-8.007643\n",
      "      433983126 15:52:07 Epoch 58/5000, D_loss=0.055492, reward=-8.004392\n",
      "      433983126 15:52:08 Epoch 59/5000, D_loss=0.054928, reward=-8.007792\n",
      "      433983126 15:52:08 Epoch 60/5000, D_loss=0.053487, reward=-8.006554\n",
      "      433983126 15:52:09 Epoch 61/5000, D_loss=0.052965, reward=-8.004596\n",
      "      433983126 15:52:09 Epoch 62/5000, D_loss=0.053361, reward=-8.003690\n",
      "      433983126 15:52:09 Epoch 63/5000, D_loss=0.056177, reward=-8.001413\n",
      "      433983126 15:52:10 Epoch 64/5000, D_loss=0.053444, reward=-8.007631\n",
      "      433983126 15:52:10 Epoch 65/5000, D_loss=0.053493, reward=-8.005857\n",
      "      433983126 15:52:10 Epoch 66/5000, D_loss=0.051615, reward=-8.003553\n",
      "      433983126 15:52:11 Epoch 67/5000, D_loss=0.052993, reward=-8.002552\n",
      "      433983126 15:52:11 Epoch 68/5000, D_loss=0.053833, reward=-7.996842\n",
      "      433983126 15:52:12 Epoch 69/5000, D_loss=0.054032, reward=-7.993627\n",
      "      433983126 15:52:12 Epoch 70/5000, D_loss=0.054868, reward=-7.997098\n",
      "      433983126 15:52:12 Epoch 71/5000, D_loss=0.053358, reward=-8.001343\n",
      "      433983126 15:52:13 Epoch 72/5000, D_loss=0.054048, reward=-7.997436\n",
      "      433983126 15:52:13 Epoch 73/5000, D_loss=0.052631, reward=-7.996183\n",
      "      433983126 15:52:13 Epoch 74/5000, D_loss=0.053522, reward=-7.992532\n",
      "      433983126 15:52:14 Epoch 75/5000, D_loss=0.053329, reward=-7.998445\n",
      "      433983126 15:52:14 Epoch 76/5000, D_loss=0.052067, reward=-7.994480\n",
      "      433983126 15:52:15 Epoch 77/5000, D_loss=0.054692, reward=-7.992383\n",
      "      433983126 15:52:15 Epoch 78/5000, D_loss=0.051706, reward=-7.997687\n",
      "      433983126 15:52:15 Epoch 79/5000, D_loss=0.052507, reward=-7.993070\n",
      "      433983126 15:52:16 Epoch 80/5000, D_loss=0.053002, reward=-7.998502\n",
      "      433983126 15:52:16 Epoch 81/5000, D_loss=0.051259, reward=-7.995157\n",
      "      433983126 15:52:16 Epoch 82/5000, D_loss=0.051904, reward=-7.989195\n",
      "      433983126 15:52:17 Epoch 83/5000, D_loss=0.050579, reward=-7.991770\n",
      "      433983126 15:52:17 Epoch 84/5000, D_loss=0.051906, reward=-7.990828\n",
      "      433983126 15:52:18 Epoch 85/5000, D_loss=0.049529, reward=-7.992447\n",
      "      433983126 15:52:18 Epoch 86/5000, D_loss=0.052888, reward=-7.987823\n",
      "      433983126 15:52:18 Epoch 87/5000, D_loss=0.051023, reward=-7.990241\n",
      "      433983126 15:52:19 Epoch 88/5000, D_loss=0.050178, reward=-7.988684\n",
      "      433983126 15:52:19 Epoch 89/5000, D_loss=0.052138, reward=-7.986304\n",
      "      433983126 15:52:19 Epoch 90/5000, D_loss=0.051537, reward=-7.984946\n",
      "      433983126 15:52:20 Epoch 91/5000, D_loss=0.051368, reward=-7.979367\n",
      "      433983126 15:52:20 Epoch 92/5000, D_loss=0.048798, reward=-7.987858\n",
      "      433983126 15:52:20 Epoch 93/5000, D_loss=0.052144, reward=-7.983981\n",
      "      433983126 15:52:21 Epoch 94/5000, D_loss=0.050088, reward=-7.985199\n",
      "      433983126 15:52:21 Epoch 95/5000, D_loss=0.051747, reward=-7.982759\n",
      "      433983126 15:52:22 Epoch 96/5000, D_loss=0.050002, reward=-7.982458\n",
      "      433983126 15:52:22 Epoch 97/5000, D_loss=0.050231, reward=-7.981802\n",
      "      433983126 15:52:22 Epoch 98/5000, D_loss=0.051208, reward=-7.978766\n",
      "      433983126 15:52:23 Epoch 99/5000, D_loss=0.051216, reward=-7.978279\n",
      "      433983126 15:52:23 Epoch 100/5000, D_loss=0.051034, reward=-7.974426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hhehe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      248942384 15:52:27 MR = 5369.865234375\n",
      "MRR = 0.17133787274360657\n",
      "Hit@1 = 0.004449571522742254\n",
      "Hit@3 = 0.32333553065260384\n",
      "Hit@10 = 0.41529334212261043\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MR = 5369.865234375\n",
      "MRR = 0.17133787274360657\n",
      "Hit@1 = 0.004449571522742254\n",
      "Hit@3 = 0.32333553065260384\n",
      "Hit@10 = 0.41529334212261043\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      433983126 15:52:27 Epoch 101/5000, D_loss=0.049927, reward=-7.979209\n",
      "      433983126 15:52:28 Epoch 102/5000, D_loss=0.052282, reward=-7.972497\n",
      "      433983126 15:52:28 Epoch 103/5000, D_loss=0.049850, reward=-7.979791\n",
      "      433983126 15:52:28 Epoch 104/5000, D_loss=0.050921, reward=-7.976317\n",
      "      433983126 15:52:29 Epoch 105/5000, D_loss=0.050676, reward=-7.978796\n",
      "      433983126 15:52:29 Epoch 106/5000, D_loss=0.050434, reward=-7.972086\n",
      "      433983126 15:52:30 Epoch 107/5000, D_loss=0.049596, reward=-7.976666\n",
      "      433983126 15:52:30 Epoch 108/5000, D_loss=0.051329, reward=-7.973950\n",
      "      433983126 15:52:30 Epoch 109/5000, D_loss=0.049334, reward=-7.977036\n",
      "      433983126 15:52:31 Epoch 110/5000, D_loss=0.050504, reward=-7.970263\n",
      "      433983126 15:52:31 Epoch 111/5000, D_loss=0.053034, reward=-7.966420\n",
      "      433983126 15:52:31 Epoch 112/5000, D_loss=0.048658, reward=-7.972291\n",
      "      433983126 15:52:32 Epoch 113/5000, D_loss=0.050715, reward=-7.971625\n",
      "      433983126 15:52:32 Epoch 114/5000, D_loss=0.050368, reward=-7.969857\n",
      "      433983126 15:52:33 Epoch 115/5000, D_loss=0.048907, reward=-7.969525\n",
      "      433983126 15:52:33 Epoch 116/5000, D_loss=0.049132, reward=-7.970648\n",
      "      433983126 15:52:33 Epoch 117/5000, D_loss=0.051267, reward=-7.965416\n",
      "      433983126 15:52:34 Epoch 118/5000, D_loss=0.051056, reward=-7.966485\n",
      "      433983126 15:52:34 Epoch 119/5000, D_loss=0.050386, reward=-7.965847\n",
      "      433983126 15:52:34 Epoch 120/5000, D_loss=0.048539, reward=-7.967829\n",
      "      433983126 15:52:35 Epoch 121/5000, D_loss=0.050231, reward=-7.966298\n",
      "      433983126 15:52:35 Epoch 122/5000, D_loss=0.048634, reward=-7.973920\n",
      "      433983126 15:52:36 Epoch 123/5000, D_loss=0.049658, reward=-7.963413\n",
      "      433983126 15:52:36 Epoch 124/5000, D_loss=0.050049, reward=-7.964957\n",
      "      433983126 15:52:36 Epoch 125/5000, D_loss=0.050144, reward=-7.968115\n",
      "      433983126 15:52:37 Epoch 126/5000, D_loss=0.049647, reward=-7.963871\n",
      "      433983126 15:52:37 Epoch 127/5000, D_loss=0.049469, reward=-7.963145\n",
      "      433983126 15:52:37 Epoch 128/5000, D_loss=0.050395, reward=-7.962774\n",
      "      433983126 15:52:38 Epoch 129/5000, D_loss=0.049225, reward=-7.967638\n",
      "      433983126 15:52:38 Epoch 130/5000, D_loss=0.048685, reward=-7.964226\n",
      "      433983126 15:52:39 Epoch 131/5000, D_loss=0.048732, reward=-7.960080\n",
      "      433983126 15:52:39 Epoch 132/5000, D_loss=0.049118, reward=-7.966161\n",
      "      433983126 15:52:39 Epoch 133/5000, D_loss=0.049707, reward=-7.962178\n",
      "      433983126 15:52:40 Epoch 134/5000, D_loss=0.049771, reward=-7.959943\n",
      "      433983126 15:52:40 Epoch 135/5000, D_loss=0.047523, reward=-7.962708\n",
      "      433983126 15:52:40 Epoch 136/5000, D_loss=0.050761, reward=-7.953951\n",
      "      433983126 15:52:41 Epoch 137/5000, D_loss=0.049089, reward=-7.955570\n",
      "      433983126 15:52:41 Epoch 138/5000, D_loss=0.050188, reward=-7.956452\n",
      "      433983126 15:52:42 Epoch 139/5000, D_loss=0.049405, reward=-7.956304\n",
      "      433983126 15:52:42 Epoch 140/5000, D_loss=0.048275, reward=-7.958276\n",
      "      433983126 15:52:42 Epoch 141/5000, D_loss=0.050924, reward=-7.954967\n",
      "      433983126 15:52:43 Epoch 142/5000, D_loss=0.049481, reward=-7.953471\n",
      "      433983126 15:52:43 Epoch 143/5000, D_loss=0.048626, reward=-7.955763\n",
      "      433983126 15:52:43 Epoch 144/5000, D_loss=0.048394, reward=-7.952760\n",
      "      433983126 15:52:44 Epoch 145/5000, D_loss=0.049972, reward=-7.946089\n",
      "      433983126 15:52:44 Epoch 146/5000, D_loss=0.049594, reward=-7.955833\n",
      "      433983126 15:52:44 Epoch 147/5000, D_loss=0.048708, reward=-7.957116\n",
      "      433983126 15:52:45 Epoch 148/5000, D_loss=0.049572, reward=-7.948427\n",
      "      433983126 15:52:45 Epoch 149/5000, D_loss=0.049799, reward=-7.944803\n",
      "      433983126 15:52:46 Epoch 150/5000, D_loss=0.046662, reward=-7.955737\n",
      "      433983126 15:52:46 Epoch 151/5000, D_loss=0.049003, reward=-7.949749\n",
      "      433983126 15:52:46 Epoch 152/5000, D_loss=0.047993, reward=-7.954002\n",
      "      433983126 15:52:47 Epoch 153/5000, D_loss=0.050155, reward=-7.943819\n",
      "      433983126 15:52:47 Epoch 154/5000, D_loss=0.049051, reward=-7.943412\n",
      "      433983126 15:52:47 Epoch 155/5000, D_loss=0.048909, reward=-7.945700\n",
      "      433983126 15:52:48 Epoch 156/5000, D_loss=0.048797, reward=-7.947476\n",
      "      433983126 15:52:48 Epoch 157/5000, D_loss=0.049811, reward=-7.943748\n",
      "      433983126 15:52:49 Epoch 158/5000, D_loss=0.048070, reward=-7.951044\n",
      "      433983126 15:52:49 Epoch 159/5000, D_loss=0.049503, reward=-7.949165\n",
      "      433983126 15:52:49 Epoch 160/5000, D_loss=0.047358, reward=-7.945313\n",
      "      433983126 15:52:50 Epoch 161/5000, D_loss=0.048378, reward=-7.945372\n",
      "      433983126 15:52:50 Epoch 162/5000, D_loss=0.048984, reward=-7.947831\n",
      "      433983126 15:52:50 Epoch 163/5000, D_loss=0.046757, reward=-7.954389\n",
      "      433983126 15:52:51 Epoch 164/5000, D_loss=0.051178, reward=-7.942352\n",
      "      433983126 15:52:51 Epoch 165/5000, D_loss=0.048429, reward=-7.947001\n",
      "      433983126 15:52:52 Epoch 166/5000, D_loss=0.048734, reward=-7.948190\n",
      "      433983126 15:52:52 Epoch 167/5000, D_loss=0.047526, reward=-7.945663\n",
      "      433983126 15:52:52 Epoch 168/5000, D_loss=0.047988, reward=-7.944526\n",
      "      433983126 15:52:53 Epoch 169/5000, D_loss=0.048462, reward=-7.944641\n",
      "      433983126 15:52:53 Epoch 170/5000, D_loss=0.047893, reward=-7.944024\n",
      "      433983126 15:52:53 Epoch 171/5000, D_loss=0.049030, reward=-7.939425\n",
      "      433983126 15:52:54 Epoch 172/5000, D_loss=0.048762, reward=-7.944055\n",
      "      433983126 15:52:54 Epoch 173/5000, D_loss=0.048432, reward=-7.947596\n",
      "      433983126 15:52:55 Epoch 174/5000, D_loss=0.049283, reward=-7.937459\n",
      "      433983126 15:52:55 Epoch 175/5000, D_loss=0.049058, reward=-7.936985\n",
      "      433983126 15:52:55 Epoch 176/5000, D_loss=0.048589, reward=-7.937978\n",
      "      433983126 15:52:56 Epoch 177/5000, D_loss=0.048199, reward=-7.941396\n",
      "      433983126 15:52:56 Epoch 178/5000, D_loss=0.048201, reward=-7.938008\n",
      "      433983126 15:52:56 Epoch 179/5000, D_loss=0.047809, reward=-7.938262\n",
      "      433983126 15:52:57 Epoch 180/5000, D_loss=0.046886, reward=-7.938340\n",
      "      433983126 15:52:57 Epoch 181/5000, D_loss=0.048398, reward=-7.939461\n",
      "      433983126 15:52:58 Epoch 182/5000, D_loss=0.048135, reward=-7.941985\n",
      "      433983126 15:52:58 Epoch 183/5000, D_loss=0.047499, reward=-7.939266\n",
      "      433983126 15:52:58 Epoch 184/5000, D_loss=0.047952, reward=-7.939540\n",
      "      433983126 15:52:59 Epoch 185/5000, D_loss=0.048339, reward=-7.938804\n",
      "      433983126 15:52:59 Epoch 186/5000, D_loss=0.047836, reward=-7.940353\n",
      "      433983126 15:52:59 Epoch 187/5000, D_loss=0.048283, reward=-7.939281\n",
      "      433983126 15:53:00 Epoch 188/5000, D_loss=0.050580, reward=-7.930731\n",
      "      433983126 15:53:00 Epoch 189/5000, D_loss=0.048042, reward=-7.938646\n",
      "      433983126 15:53:01 Epoch 190/5000, D_loss=0.048308, reward=-7.938427\n",
      "      433983126 15:53:01 Epoch 191/5000, D_loss=0.046955, reward=-7.938134\n",
      "      433983126 15:53:01 Epoch 192/5000, D_loss=0.048601, reward=-7.939097\n",
      "      433983126 15:53:02 Epoch 193/5000, D_loss=0.047020, reward=-7.930881\n",
      "      433983126 15:53:02 Epoch 194/5000, D_loss=0.048331, reward=-7.930077\n",
      "      433983126 15:53:02 Epoch 195/5000, D_loss=0.046820, reward=-7.933039\n",
      "      433983126 15:53:03 Epoch 196/5000, D_loss=0.046749, reward=-7.935272\n",
      "      433983126 15:53:03 Epoch 197/5000, D_loss=0.047413, reward=-7.933711\n",
      "      433983126 15:53:04 Epoch 198/5000, D_loss=0.048699, reward=-7.932545\n",
      "      433983126 15:53:04 Epoch 199/5000, D_loss=0.048254, reward=-7.928293\n",
      "      433983126 15:53:04 Epoch 200/5000, D_loss=0.048615, reward=-7.932162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hhehe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      248942384 15:53:08 MR = 5111.28125\n",
      "MRR = 0.1725415587425232\n",
      "Hit@1 = 0.005273566249176005\n",
      "Hit@3 = 0.3246539222148978\n",
      "Hit@10 = 0.41727092946605143\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MR = 5111.28125\n",
      "MRR = 0.1725415587425232\n",
      "Hit@1 = 0.005273566249176005\n",
      "Hit@3 = 0.3246539222148978\n",
      "Hit@10 = 0.41727092946605143\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      433983126 15:53:09 Epoch 201/5000, D_loss=0.047369, reward=-7.932260\n",
      "      433983126 15:53:09 Epoch 202/5000, D_loss=0.046649, reward=-7.942058\n",
      "      433983126 15:53:09 Epoch 203/5000, D_loss=0.045997, reward=-7.936792\n",
      "      433983126 15:53:10 Epoch 204/5000, D_loss=0.048778, reward=-7.936220\n",
      "      433983126 15:53:10 Epoch 205/5000, D_loss=0.049351, reward=-7.930022\n",
      "      433983126 15:53:10 Epoch 206/5000, D_loss=0.048522, reward=-7.927615\n",
      "      433983126 15:53:11 Epoch 207/5000, D_loss=0.048237, reward=-7.930089\n",
      "      433983126 15:53:11 Epoch 208/5000, D_loss=0.047345, reward=-7.925621\n",
      "      433983126 15:53:12 Epoch 209/5000, D_loss=0.046669, reward=-7.930151\n",
      "      433983126 15:53:12 Epoch 210/5000, D_loss=0.047763, reward=-7.928990\n",
      "      433983126 15:53:12 Epoch 211/5000, D_loss=0.046871, reward=-7.928370\n",
      "      433983126 15:53:13 Epoch 212/5000, D_loss=0.048148, reward=-7.928568\n",
      "      433983126 15:53:13 Epoch 213/5000, D_loss=0.047528, reward=-7.932072\n",
      "      433983126 15:53:13 Epoch 214/5000, D_loss=0.046245, reward=-7.930985\n",
      "      433983126 15:53:14 Epoch 215/5000, D_loss=0.047793, reward=-7.926826\n",
      "      433983126 15:53:14 Epoch 216/5000, D_loss=0.048218, reward=-7.925699\n",
      "      433983126 15:53:15 Epoch 217/5000, D_loss=0.046602, reward=-7.930759\n",
      "      433983126 15:53:15 Epoch 218/5000, D_loss=0.046449, reward=-7.926383\n",
      "      433983126 15:53:15 Epoch 219/5000, D_loss=0.046842, reward=-7.926924\n",
      "      433983126 15:53:16 Epoch 220/5000, D_loss=0.048291, reward=-7.928232\n",
      "      433983126 15:53:16 Epoch 221/5000, D_loss=0.047835, reward=-7.927416\n",
      "      433983126 15:53:17 Epoch 222/5000, D_loss=0.045518, reward=-7.928193\n",
      "      433983126 15:53:17 Epoch 223/5000, D_loss=0.047059, reward=-7.926816\n",
      "      433983126 15:53:17 Epoch 224/5000, D_loss=0.046567, reward=-7.929221\n",
      "      433983126 15:53:18 Epoch 225/5000, D_loss=0.048077, reward=-7.923794\n",
      "      433983126 15:53:18 Epoch 226/5000, D_loss=0.046941, reward=-7.920002\n",
      "      433983126 15:53:18 Epoch 227/5000, D_loss=0.047160, reward=-7.923773\n",
      "      433983126 15:53:19 Epoch 228/5000, D_loss=0.046233, reward=-7.921558\n",
      "      433983126 15:53:19 Epoch 229/5000, D_loss=0.047266, reward=-7.918956\n",
      "      433983126 15:53:20 Epoch 230/5000, D_loss=0.047250, reward=-7.926434\n",
      "      433983126 15:53:20 Epoch 231/5000, D_loss=0.048017, reward=-7.926961\n",
      "      433983126 15:53:20 Epoch 232/5000, D_loss=0.045352, reward=-7.926581\n",
      "      433983126 15:53:21 Epoch 233/5000, D_loss=0.046919, reward=-7.921834\n",
      "      433983126 15:53:21 Epoch 234/5000, D_loss=0.047300, reward=-7.920523\n",
      "      433983126 15:53:21 Epoch 235/5000, D_loss=0.045959, reward=-7.926586\n",
      "      433983126 15:53:22 Epoch 236/5000, D_loss=0.046548, reward=-7.924364\n",
      "      433983126 15:53:22 Epoch 237/5000, D_loss=0.047288, reward=-7.917799\n",
      "      433983126 15:53:23 Epoch 238/5000, D_loss=0.047053, reward=-7.925217\n",
      "      433983126 15:53:23 Epoch 239/5000, D_loss=0.048707, reward=-7.918734\n",
      "      433983126 15:53:23 Epoch 240/5000, D_loss=0.047783, reward=-7.918022\n",
      "      433983126 15:53:24 Epoch 241/5000, D_loss=0.047593, reward=-7.917944\n",
      "      433983126 15:53:24 Epoch 242/5000, D_loss=0.047036, reward=-7.922335\n",
      "      433983126 15:53:25 Epoch 243/5000, D_loss=0.045518, reward=-7.921999\n",
      "      433983126 15:53:25 Epoch 244/5000, D_loss=0.046727, reward=-7.915802\n",
      "      433983126 15:53:25 Epoch 245/5000, D_loss=0.045503, reward=-7.922930\n",
      "      433983126 15:53:26 Epoch 246/5000, D_loss=0.047802, reward=-7.919199\n",
      "      433983126 15:53:26 Epoch 247/5000, D_loss=0.046325, reward=-7.917733\n",
      "      433983126 15:53:26 Epoch 248/5000, D_loss=0.047634, reward=-7.920649\n",
      "      433983126 15:53:27 Epoch 249/5000, D_loss=0.045947, reward=-7.918699\n",
      "      433983126 15:53:27 Epoch 250/5000, D_loss=0.046133, reward=-7.916515\n",
      "      433983126 15:53:28 Epoch 251/5000, D_loss=0.050148, reward=-7.914913\n",
      "      433983126 15:53:28 Epoch 252/5000, D_loss=0.047976, reward=-7.916879\n",
      "      433983126 15:53:28 Epoch 253/5000, D_loss=0.045929, reward=-7.916941\n",
      "      433983126 15:53:29 Epoch 254/5000, D_loss=0.046168, reward=-7.915168\n",
      "      433983126 15:53:29 Epoch 255/5000, D_loss=0.045781, reward=-7.925373\n",
      "      433983126 15:53:29 Epoch 256/5000, D_loss=0.047788, reward=-7.914890\n",
      "      433983126 15:53:30 Epoch 257/5000, D_loss=0.048769, reward=-7.911982\n",
      "      433983126 15:53:30 Epoch 258/5000, D_loss=0.045410, reward=-7.918989\n",
      "      433983126 15:53:31 Epoch 259/5000, D_loss=0.046719, reward=-7.920327\n",
      "      433983126 15:53:31 Epoch 260/5000, D_loss=0.047932, reward=-7.915365\n",
      "      433983126 15:53:31 Epoch 261/5000, D_loss=0.046937, reward=-7.914720\n",
      "      433983126 15:53:32 Epoch 262/5000, D_loss=0.045220, reward=-7.918399\n",
      "      433983126 15:53:32 Epoch 263/5000, D_loss=0.045789, reward=-7.919196\n",
      "      433983126 15:53:32 Epoch 264/5000, D_loss=0.046861, reward=-7.914071\n",
      "      433983126 15:53:33 Epoch 265/5000, D_loss=0.046901, reward=-7.914653\n",
      "      433983126 15:53:33 Epoch 266/5000, D_loss=0.044868, reward=-7.920117\n",
      "      433983126 15:53:34 Epoch 267/5000, D_loss=0.047569, reward=-7.917617\n",
      "      433983126 15:53:34 Epoch 268/5000, D_loss=0.046249, reward=-7.914346\n",
      "      433983126 15:53:34 Epoch 269/5000, D_loss=0.046739, reward=-7.909009\n",
      "      433983126 15:53:35 Epoch 270/5000, D_loss=0.047396, reward=-7.908933\n",
      "      433983126 15:53:35 Epoch 271/5000, D_loss=0.047405, reward=-7.909568\n",
      "      433983126 15:53:36 Epoch 272/5000, D_loss=0.046932, reward=-7.905133\n",
      "      433983126 15:53:36 Epoch 273/5000, D_loss=0.047293, reward=-7.911424\n",
      "      433983126 15:53:36 Epoch 274/5000, D_loss=0.045358, reward=-7.916916\n",
      "      433983126 15:53:37 Epoch 275/5000, D_loss=0.046579, reward=-7.912257\n",
      "      433983126 15:53:37 Epoch 276/5000, D_loss=0.047156, reward=-7.915978\n",
      "      433983126 15:53:38 Epoch 277/5000, D_loss=0.046089, reward=-7.917993\n",
      "      433983126 15:53:38 Epoch 278/5000, D_loss=0.046285, reward=-7.911645\n",
      "      433983126 15:53:38 Epoch 279/5000, D_loss=0.047968, reward=-7.912168\n",
      "      433983126 15:53:39 Epoch 280/5000, D_loss=0.046153, reward=-7.913441\n",
      "      433983126 15:53:39 Epoch 281/5000, D_loss=0.045974, reward=-7.908092\n",
      "      433983126 15:53:39 Epoch 282/5000, D_loss=0.046638, reward=-7.915557\n",
      "      433983126 15:53:40 Epoch 283/5000, D_loss=0.046725, reward=-7.914486\n",
      "      433983126 15:53:40 Epoch 284/5000, D_loss=0.045892, reward=-7.911477\n",
      "      433983126 15:53:41 Epoch 285/5000, D_loss=0.046210, reward=-7.914570\n",
      "      433983126 15:53:41 Epoch 286/5000, D_loss=0.046036, reward=-7.906861\n",
      "      433983126 15:53:41 Epoch 287/5000, D_loss=0.046264, reward=-7.912497\n",
      "      433983126 15:53:42 Epoch 288/5000, D_loss=0.045873, reward=-7.908493\n",
      "      433983126 15:53:42 Epoch 289/5000, D_loss=0.046683, reward=-7.912848\n",
      "      433983126 15:53:42 Epoch 290/5000, D_loss=0.047239, reward=-7.910242\n",
      "      433983126 15:53:43 Epoch 291/5000, D_loss=0.044798, reward=-7.912975\n",
      "      433983126 15:53:43 Epoch 292/5000, D_loss=0.046243, reward=-7.909295\n",
      "      433983126 15:53:44 Epoch 293/5000, D_loss=0.046109, reward=-7.908477\n",
      "      433983126 15:53:44 Epoch 294/5000, D_loss=0.045309, reward=-7.910118\n",
      "      433983126 15:53:44 Epoch 295/5000, D_loss=0.043856, reward=-7.914130\n",
      "      433983126 15:53:45 Epoch 296/5000, D_loss=0.045499, reward=-7.908708\n",
      "      433983126 15:53:45 Epoch 297/5000, D_loss=0.045492, reward=-7.903734\n",
      "      433983126 15:53:45 Epoch 298/5000, D_loss=0.045814, reward=-7.909047\n",
      "      433983126 15:53:46 Epoch 299/5000, D_loss=0.045795, reward=-7.908830\n",
      "      433983126 15:53:46 Epoch 300/5000, D_loss=0.046728, reward=-7.909410\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hhehe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      248942384 15:53:50 MR = 5053.005859375\n",
      "MRR = 0.17394746840000153\n",
      "Hit@1 = 0.005767963085036256\n",
      "Hit@3 = 0.3264667106130521\n",
      "Hit@10 = 0.4225444957152274\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MR = 5053.005859375\n",
      "MRR = 0.17394746840000153\n",
      "Hit@1 = 0.005767963085036256\n",
      "Hit@3 = 0.3264667106130521\n",
      "Hit@10 = 0.4225444957152274\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      433983126 15:53:51 Epoch 301/5000, D_loss=0.045747, reward=-7.911760\n",
      "      433983126 15:53:51 Epoch 302/5000, D_loss=0.047738, reward=-7.907630\n",
      "      433983126 15:53:51 Epoch 303/5000, D_loss=0.046424, reward=-7.912888\n",
      "      433983126 15:53:52 Epoch 304/5000, D_loss=0.047167, reward=-7.902901\n",
      "      433983126 15:53:52 Epoch 305/5000, D_loss=0.047320, reward=-7.910397\n",
      "      433983126 15:53:52 Epoch 306/5000, D_loss=0.045935, reward=-7.914910\n",
      "      433983126 15:53:53 Epoch 307/5000, D_loss=0.044857, reward=-7.907165\n",
      "      433983126 15:53:53 Epoch 308/5000, D_loss=0.045954, reward=-7.905906\n",
      "      433983126 15:53:54 Epoch 309/5000, D_loss=0.045179, reward=-7.907722\n",
      "      433983126 15:53:54 Epoch 310/5000, D_loss=0.045070, reward=-7.910141\n",
      "      433983126 15:53:54 Epoch 311/5000, D_loss=0.047052, reward=-7.906309\n",
      "      433983126 15:53:55 Epoch 312/5000, D_loss=0.047657, reward=-7.903251\n",
      "      433983126 15:53:55 Epoch 313/5000, D_loss=0.045973, reward=-7.908085\n",
      "      433983126 15:53:55 Epoch 314/5000, D_loss=0.046912, reward=-7.904448\n",
      "      433983126 15:53:56 Epoch 315/5000, D_loss=0.047167, reward=-7.898820\n",
      "      433983126 15:53:56 Epoch 316/5000, D_loss=0.045829, reward=-7.907649\n",
      "      433983126 15:53:57 Epoch 317/5000, D_loss=0.046635, reward=-7.900753\n",
      "      433983126 15:53:57 Epoch 318/5000, D_loss=0.048305, reward=-7.903749\n",
      "      433983126 15:53:57 Epoch 319/5000, D_loss=0.046649, reward=-7.904246\n",
      "      433983126 15:53:58 Epoch 320/5000, D_loss=0.046443, reward=-7.906728\n",
      "      433983126 15:53:58 Epoch 321/5000, D_loss=0.046627, reward=-7.904141\n",
      "      433983126 15:53:59 Epoch 322/5000, D_loss=0.044882, reward=-7.907477\n",
      "      433983126 15:53:59 Epoch 323/5000, D_loss=0.045330, reward=-7.901832\n",
      "      433983126 15:53:59 Epoch 324/5000, D_loss=0.046994, reward=-7.900908\n",
      "      433983126 15:54:00 Epoch 325/5000, D_loss=0.044465, reward=-7.901984\n",
      "      433983126 15:54:00 Epoch 326/5000, D_loss=0.046360, reward=-7.901867\n",
      "      433983126 15:54:00 Epoch 327/5000, D_loss=0.044929, reward=-7.898675\n",
      "      433983126 15:54:01 Epoch 328/5000, D_loss=0.047065, reward=-7.899746\n",
      "      433983126 15:54:01 Epoch 329/5000, D_loss=0.045088, reward=-7.905915\n",
      "      433983126 15:54:02 Epoch 330/5000, D_loss=0.044955, reward=-7.905497\n",
      "      433983126 15:54:02 Epoch 331/5000, D_loss=0.046794, reward=-7.897454\n",
      "      433983126 15:54:02 Epoch 332/5000, D_loss=0.045597, reward=-7.905140\n",
      "      433983126 15:54:03 Epoch 333/5000, D_loss=0.044651, reward=-7.898538\n",
      "      433983126 15:54:03 Epoch 334/5000, D_loss=0.045261, reward=-7.901621\n",
      "      433983126 15:54:03 Epoch 335/5000, D_loss=0.045612, reward=-7.903126\n",
      "      433983126 15:54:04 Epoch 336/5000, D_loss=0.046329, reward=-7.898908\n",
      "      433983126 15:54:04 Epoch 337/5000, D_loss=0.046090, reward=-7.900524\n",
      "      433983126 15:54:05 Epoch 338/5000, D_loss=0.045835, reward=-7.904035\n",
      "      433983126 15:54:05 Epoch 339/5000, D_loss=0.043906, reward=-7.905757\n",
      "      433983126 15:54:05 Epoch 340/5000, D_loss=0.044655, reward=-7.900321\n",
      "      433983126 15:54:06 Epoch 341/5000, D_loss=0.045336, reward=-7.897689\n",
      "      433983126 15:54:06 Epoch 342/5000, D_loss=0.045851, reward=-7.902412\n",
      "      433983126 15:54:07 Epoch 343/5000, D_loss=0.045863, reward=-7.897049\n",
      "      433983126 15:54:07 Epoch 344/5000, D_loss=0.046443, reward=-7.895599\n",
      "      433983126 15:54:07 Epoch 345/5000, D_loss=0.045284, reward=-7.899811\n",
      "      433983126 15:54:08 Epoch 346/5000, D_loss=0.046962, reward=-7.894770\n",
      "      433983126 15:54:08 Epoch 347/5000, D_loss=0.045746, reward=-7.898232\n",
      "      433983126 15:54:08 Epoch 348/5000, D_loss=0.045645, reward=-7.901429\n",
      "      433983126 15:54:09 Epoch 349/5000, D_loss=0.046217, reward=-7.890769\n",
      "      433983126 15:54:09 Epoch 350/5000, D_loss=0.045577, reward=-7.901573\n",
      "      433983126 15:54:10 Epoch 351/5000, D_loss=0.045778, reward=-7.899968\n",
      "      433983126 15:54:10 Epoch 352/5000, D_loss=0.046588, reward=-7.893761\n",
      "      433983126 15:54:10 Epoch 353/5000, D_loss=0.044682, reward=-7.903764\n",
      "      433983126 15:54:11 Epoch 354/5000, D_loss=0.045858, reward=-7.901897\n",
      "      433983126 15:54:11 Epoch 355/5000, D_loss=0.045425, reward=-7.894202\n",
      "      433983126 15:54:12 Epoch 356/5000, D_loss=0.046229, reward=-7.898331\n",
      "      433983126 15:54:12 Epoch 357/5000, D_loss=0.046249, reward=-7.899164\n",
      "      433983126 15:54:12 Epoch 358/5000, D_loss=0.045958, reward=-7.889940\n",
      "      433983126 15:54:13 Epoch 359/5000, D_loss=0.046606, reward=-7.895854\n",
      "      433983126 15:54:13 Epoch 360/5000, D_loss=0.044413, reward=-7.902739\n",
      "      433983126 15:54:13 Epoch 361/5000, D_loss=0.044893, reward=-7.898504\n",
      "      433983126 15:54:14 Epoch 362/5000, D_loss=0.045450, reward=-7.896729\n",
      "      433983126 15:54:14 Epoch 363/5000, D_loss=0.045525, reward=-7.896344\n",
      "      433983126 15:54:15 Epoch 364/5000, D_loss=0.045854, reward=-7.896101\n",
      "      433983126 15:54:15 Epoch 365/5000, D_loss=0.044538, reward=-7.896886\n",
      "      433983126 15:54:15 Epoch 366/5000, D_loss=0.044740, reward=-7.902322\n",
      "      433983126 15:54:16 Epoch 367/5000, D_loss=0.045893, reward=-7.897898\n",
      "      433983126 15:54:16 Epoch 368/5000, D_loss=0.045808, reward=-7.892769\n",
      "      433983126 15:54:16 Epoch 369/5000, D_loss=0.046975, reward=-7.896259\n",
      "      433983126 15:54:17 Epoch 370/5000, D_loss=0.045390, reward=-7.895308\n",
      "      433983126 15:54:17 Epoch 371/5000, D_loss=0.047012, reward=-7.885036\n",
      "      433983126 15:54:18 Epoch 372/5000, D_loss=0.045648, reward=-7.888291\n",
      "      433983126 15:54:18 Epoch 373/5000, D_loss=0.045395, reward=-7.895425\n",
      "      433983126 15:54:18 Epoch 374/5000, D_loss=0.045684, reward=-7.896400\n",
      "      433983126 15:54:19 Epoch 375/5000, D_loss=0.045285, reward=-7.891068\n",
      "      433983126 15:54:19 Epoch 376/5000, D_loss=0.044136, reward=-7.895944\n",
      "      433983126 15:54:20 Epoch 377/5000, D_loss=0.044985, reward=-7.892275\n",
      "      433983126 15:54:20 Epoch 378/5000, D_loss=0.045187, reward=-7.894520\n",
      "      433983126 15:54:20 Epoch 379/5000, D_loss=0.043735, reward=-7.892454\n",
      "      433983126 15:54:21 Epoch 380/5000, D_loss=0.044554, reward=-7.896652\n",
      "      433983126 15:54:21 Epoch 381/5000, D_loss=0.043428, reward=-7.895550\n",
      "      433983126 15:54:21 Epoch 382/5000, D_loss=0.044372, reward=-7.895003\n",
      "      433983126 15:54:22 Epoch 383/5000, D_loss=0.043851, reward=-7.890841\n",
      "      433983126 15:54:22 Epoch 384/5000, D_loss=0.044907, reward=-7.890704\n",
      "      433983126 15:54:23 Epoch 385/5000, D_loss=0.045635, reward=-7.886164\n",
      "      433983126 15:54:23 Epoch 386/5000, D_loss=0.045218, reward=-7.892231\n",
      "      433983126 15:54:23 Epoch 387/5000, D_loss=0.044197, reward=-7.892722\n",
      "      433983126 15:54:24 Epoch 388/5000, D_loss=0.046505, reward=-7.893576\n",
      "      433983126 15:54:24 Epoch 389/5000, D_loss=0.045804, reward=-7.891998\n",
      "      433983126 15:54:24 Epoch 390/5000, D_loss=0.044690, reward=-7.893515\n",
      "      433983126 15:54:25 Epoch 391/5000, D_loss=0.045549, reward=-7.889651\n",
      "      433983126 15:54:25 Epoch 392/5000, D_loss=0.045763, reward=-7.890798\n",
      "      433983126 15:54:26 Epoch 393/5000, D_loss=0.046121, reward=-7.888964\n",
      "      433983126 15:54:26 Epoch 394/5000, D_loss=0.046403, reward=-7.889976\n",
      "      433983126 15:54:26 Epoch 395/5000, D_loss=0.045165, reward=-7.888628\n",
      "      433983126 15:54:27 Epoch 396/5000, D_loss=0.044055, reward=-7.888819\n",
      "      433983126 15:54:27 Epoch 397/5000, D_loss=0.044940, reward=-7.885357\n",
      "      433983126 15:54:27 Epoch 398/5000, D_loss=0.046458, reward=-7.886592\n",
      "      433983126 15:54:28 Epoch 399/5000, D_loss=0.044644, reward=-7.882435\n",
      "      433983126 15:54:28 Epoch 400/5000, D_loss=0.045737, reward=-7.889404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hhehe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      248942384 15:54:32 MR = 4901.384765625\n",
      "MRR = 0.17428149282932281\n",
      "Hit@1 = 0.0054383651944627555\n",
      "Hit@3 = 0.3276203032300593\n",
      "Hit@10 = 0.4215557020435069\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MR = 4901.384765625\n",
      "MRR = 0.17428149282932281\n",
      "Hit@1 = 0.0054383651944627555\n",
      "Hit@3 = 0.3276203032300593\n",
      "Hit@10 = 0.4215557020435069\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      433983126 15:54:33 Epoch 401/5000, D_loss=0.044692, reward=-7.890929\n",
      "      433983126 15:54:33 Epoch 402/5000, D_loss=0.045659, reward=-7.890118\n",
      "      433983126 15:54:33 Epoch 403/5000, D_loss=0.046801, reward=-7.882823\n",
      "      433983126 15:54:34 Epoch 404/5000, D_loss=0.045887, reward=-7.889050\n",
      "      433983126 15:54:34 Epoch 405/5000, D_loss=0.044632, reward=-7.893925\n",
      "      433983126 15:54:34 Epoch 406/5000, D_loss=0.046047, reward=-7.890211\n",
      "      433983126 15:54:35 Epoch 407/5000, D_loss=0.045610, reward=-7.888832\n",
      "      433983126 15:54:35 Epoch 408/5000, D_loss=0.045283, reward=-7.888434\n",
      "      433983126 15:54:36 Epoch 409/5000, D_loss=0.045644, reward=-7.889419\n",
      "      433983126 15:54:36 Epoch 410/5000, D_loss=0.043950, reward=-7.894065\n",
      "      433983126 15:54:36 Epoch 411/5000, D_loss=0.046460, reward=-7.889514\n",
      "      433983126 15:54:37 Epoch 412/5000, D_loss=0.046064, reward=-7.879041\n",
      "      433983126 15:54:37 Epoch 413/5000, D_loss=0.046212, reward=-7.884784\n",
      "      433983126 15:54:38 Epoch 414/5000, D_loss=0.046026, reward=-7.882310\n",
      "      433983126 15:54:38 Epoch 415/5000, D_loss=0.044790, reward=-7.890357\n",
      "      433983126 15:54:38 Epoch 416/5000, D_loss=0.045948, reward=-7.892642\n",
      "      433983126 15:54:39 Epoch 417/5000, D_loss=0.045083, reward=-7.885547\n",
      "      433983126 15:54:39 Epoch 418/5000, D_loss=0.046201, reward=-7.886508\n",
      "      433983126 15:54:39 Epoch 419/5000, D_loss=0.045822, reward=-7.885280\n",
      "      433983126 15:54:40 Epoch 420/5000, D_loss=0.044969, reward=-7.889302\n",
      "      433983126 15:54:40 Epoch 421/5000, D_loss=0.045568, reward=-7.883359\n",
      "      433983126 15:54:41 Epoch 422/5000, D_loss=0.045269, reward=-7.889674\n",
      "      433983126 15:54:41 Epoch 423/5000, D_loss=0.045257, reward=-7.883070\n",
      "      433983126 15:54:41 Epoch 424/5000, D_loss=0.045296, reward=-7.882651\n",
      "      433983126 15:54:42 Epoch 425/5000, D_loss=0.045082, reward=-7.888939\n",
      "      433983126 15:54:42 Epoch 426/5000, D_loss=0.043827, reward=-7.890486\n",
      "      433983126 15:54:43 Epoch 427/5000, D_loss=0.045774, reward=-7.880816\n",
      "      433983126 15:54:43 Epoch 428/5000, D_loss=0.045140, reward=-7.884228\n",
      "      433983126 15:54:43 Epoch 429/5000, D_loss=0.044928, reward=-7.879873\n",
      "      433983126 15:54:44 Epoch 430/5000, D_loss=0.046396, reward=-7.879867\n",
      "      433983126 15:54:44 Epoch 431/5000, D_loss=0.044810, reward=-7.884829\n",
      "      433983126 15:54:44 Epoch 432/5000, D_loss=0.045970, reward=-7.883982\n",
      "      433983126 15:54:45 Epoch 433/5000, D_loss=0.047128, reward=-7.879432\n",
      "      433983126 15:54:45 Epoch 434/5000, D_loss=0.045521, reward=-7.882227\n",
      "      433983126 15:54:46 Epoch 435/5000, D_loss=0.045802, reward=-7.886444\n",
      "      433983126 15:54:46 Epoch 436/5000, D_loss=0.045239, reward=-7.886242\n",
      "      433983126 15:54:46 Epoch 437/5000, D_loss=0.045565, reward=-7.883090\n",
      "      433983126 15:54:47 Epoch 438/5000, D_loss=0.044800, reward=-7.880041\n",
      "      433983126 15:54:47 Epoch 439/5000, D_loss=0.047069, reward=-7.880536\n",
      "      433983126 15:54:48 Epoch 440/5000, D_loss=0.044700, reward=-7.879692\n",
      "      433983126 15:54:48 Epoch 441/5000, D_loss=0.044367, reward=-7.880582\n",
      "      433983126 15:54:48 Epoch 442/5000, D_loss=0.046028, reward=-7.879836\n",
      "      433983126 15:54:49 Epoch 443/5000, D_loss=0.044435, reward=-7.884822\n",
      "      433983126 15:54:49 Epoch 444/5000, D_loss=0.044586, reward=-7.884881\n",
      "      433983126 15:54:50 Epoch 445/5000, D_loss=0.044356, reward=-7.885053\n",
      "      433983126 15:54:50 Epoch 446/5000, D_loss=0.044577, reward=-7.881612\n",
      "      433983126 15:54:50 Epoch 447/5000, D_loss=0.043745, reward=-7.882236\n",
      "      433983126 15:54:51 Epoch 448/5000, D_loss=0.042795, reward=-7.881277\n",
      "      433983126 15:54:51 Epoch 449/5000, D_loss=0.044395, reward=-7.879145\n",
      "      433983126 15:54:51 Epoch 450/5000, D_loss=0.044347, reward=-7.880352\n",
      "      433983126 15:54:52 Epoch 451/5000, D_loss=0.046011, reward=-7.873877\n",
      "      433983126 15:54:52 Epoch 452/5000, D_loss=0.044610, reward=-7.882464\n",
      "      433983126 15:54:53 Epoch 453/5000, D_loss=0.045196, reward=-7.880960\n",
      "      433983126 15:54:53 Epoch 454/5000, D_loss=0.045314, reward=-7.879885\n",
      "      433983126 15:54:53 Epoch 455/5000, D_loss=0.044426, reward=-7.877104\n",
      "      433983126 15:54:54 Epoch 456/5000, D_loss=0.044913, reward=-7.883213\n",
      "      433983126 15:54:54 Epoch 457/5000, D_loss=0.044067, reward=-7.883534\n",
      "      433983126 15:54:54 Epoch 458/5000, D_loss=0.046626, reward=-7.880988\n",
      "      433983126 15:54:55 Epoch 459/5000, D_loss=0.046503, reward=-7.877551\n",
      "      433983126 15:54:55 Epoch 460/5000, D_loss=0.044381, reward=-7.887291\n",
      "      433983126 15:54:56 Epoch 461/5000, D_loss=0.046162, reward=-7.879798\n",
      "      433983126 15:54:56 Epoch 462/5000, D_loss=0.046612, reward=-7.877653\n",
      "      433983126 15:54:56 Epoch 463/5000, D_loss=0.042486, reward=-7.889600\n",
      "      433983126 15:54:57 Epoch 464/5000, D_loss=0.044684, reward=-7.877737\n",
      "      433983126 15:54:57 Epoch 465/5000, D_loss=0.045470, reward=-7.880030\n",
      "      433983126 15:54:58 Epoch 466/5000, D_loss=0.043877, reward=-7.875169\n",
      "      433983126 15:54:58 Epoch 467/5000, D_loss=0.044686, reward=-7.874938\n",
      "      433983126 15:54:58 Epoch 468/5000, D_loss=0.044951, reward=-7.875492\n",
      "      433983126 15:54:59 Epoch 469/5000, D_loss=0.044130, reward=-7.881280\n",
      "      433983126 15:54:59 Epoch 470/5000, D_loss=0.045855, reward=-7.875193\n",
      "      433983126 15:55:00 Epoch 471/5000, D_loss=0.046507, reward=-7.876709\n",
      "      433983126 15:55:00 Epoch 472/5000, D_loss=0.043585, reward=-7.879319\n",
      "      433983126 15:55:00 Epoch 473/5000, D_loss=0.045451, reward=-7.877232\n",
      "      433983126 15:55:01 Epoch 474/5000, D_loss=0.044795, reward=-7.879230\n",
      "      433983126 15:55:01 Epoch 475/5000, D_loss=0.043072, reward=-7.882937\n",
      "      433983126 15:55:02 Epoch 476/5000, D_loss=0.045189, reward=-7.878754\n",
      "      433983126 15:55:02 Epoch 477/5000, D_loss=0.045052, reward=-7.878675\n",
      "      433983126 15:55:02 Epoch 478/5000, D_loss=0.044214, reward=-7.878190\n",
      "      433983126 15:55:03 Epoch 479/5000, D_loss=0.044283, reward=-7.877712\n",
      "      433983126 15:55:03 Epoch 480/5000, D_loss=0.042795, reward=-7.878697\n",
      "      433983126 15:55:03 Epoch 481/5000, D_loss=0.044865, reward=-7.873265\n",
      "      433983126 15:55:04 Epoch 482/5000, D_loss=0.042461, reward=-7.877413\n",
      "      433983126 15:55:04 Epoch 483/5000, D_loss=0.044401, reward=-7.883299\n",
      "      433983126 15:55:05 Epoch 484/5000, D_loss=0.046573, reward=-7.879528\n",
      "      433983126 15:55:05 Epoch 485/5000, D_loss=0.045222, reward=-7.878473\n",
      "      433983126 15:55:05 Epoch 486/5000, D_loss=0.045604, reward=-7.876225\n",
      "      433983126 15:55:06 Epoch 487/5000, D_loss=0.046289, reward=-7.879805\n",
      "      433983126 15:55:06 Epoch 488/5000, D_loss=0.045665, reward=-7.870400\n",
      "      433983126 15:55:06 Epoch 489/5000, D_loss=0.044619, reward=-7.874117\n",
      "      433983126 15:55:07 Epoch 490/5000, D_loss=0.044228, reward=-7.870082\n",
      "      433983126 15:55:07 Epoch 491/5000, D_loss=0.046052, reward=-7.870195\n",
      "      433983126 15:55:08 Epoch 492/5000, D_loss=0.043904, reward=-7.879485\n",
      "      433983126 15:55:08 Epoch 493/5000, D_loss=0.046068, reward=-7.872241\n",
      "      433983126 15:55:08 Epoch 494/5000, D_loss=0.043932, reward=-7.881728\n",
      "      433983126 15:55:09 Epoch 495/5000, D_loss=0.045808, reward=-7.875087\n",
      "      433983126 15:55:09 Epoch 496/5000, D_loss=0.044345, reward=-7.876626\n",
      "      433983126 15:55:10 Epoch 497/5000, D_loss=0.043513, reward=-7.871279\n",
      "      433983126 15:55:10 Epoch 498/5000, D_loss=0.046037, reward=-7.869654\n",
      "      433983126 15:55:10 Epoch 499/5000, D_loss=0.046232, reward=-7.865768\n",
      "      433983126 15:55:11 Epoch 500/5000, D_loss=0.045642, reward=-7.871980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hhehe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      248942384 15:55:14 MR = 4910.8974609375\n",
      "MRR = 0.17493632435798645\n",
      "Hit@1 = 0.004943968358602505\n",
      "Hit@3 = 0.32679630850362557\n",
      "Hit@10 = 0.4268292682926829\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MR = 4910.8974609375\n",
      "MRR = 0.17493632435798645\n",
      "Hit@1 = 0.004943968358602505\n",
      "Hit@3 = 0.32679630850362557\n",
      "Hit@10 = 0.4268292682926829\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      433983126 15:55:15 Epoch 501/5000, D_loss=0.045607, reward=-7.865664\n",
      "      433983126 15:55:15 Epoch 502/5000, D_loss=0.044791, reward=-7.873797\n",
      "      433983126 15:55:16 Epoch 503/5000, D_loss=0.043893, reward=-7.875847\n",
      "      433983126 15:55:16 Epoch 504/5000, D_loss=0.046280, reward=-7.871843\n",
      "      433983126 15:55:16 Epoch 505/5000, D_loss=0.043968, reward=-7.873231\n",
      "      433983126 15:55:17 Epoch 506/5000, D_loss=0.044250, reward=-7.874781\n",
      "      433983126 15:55:17 Epoch 507/5000, D_loss=0.045207, reward=-7.873032\n",
      "      433983126 15:55:17 Epoch 508/5000, D_loss=0.045119, reward=-7.869075\n",
      "      433983126 15:55:18 Epoch 509/5000, D_loss=0.043706, reward=-7.875257\n",
      "      433983126 15:55:18 Epoch 510/5000, D_loss=0.043775, reward=-7.875618\n",
      "      433983126 15:55:19 Epoch 511/5000, D_loss=0.044062, reward=-7.879329\n",
      "      433983126 15:55:19 Epoch 512/5000, D_loss=0.044053, reward=-7.879839\n",
      "      433983126 15:55:19 Epoch 513/5000, D_loss=0.043846, reward=-7.875437\n",
      "      433983126 15:55:20 Epoch 514/5000, D_loss=0.045512, reward=-7.869930\n",
      "      433983126 15:55:20 Epoch 515/5000, D_loss=0.045368, reward=-7.871021\n",
      "      433983126 15:55:20 Epoch 516/5000, D_loss=0.046780, reward=-7.865164\n",
      "      433983126 15:55:21 Epoch 517/5000, D_loss=0.044799, reward=-7.871430\n",
      "      433983126 15:55:21 Epoch 518/5000, D_loss=0.044345, reward=-7.867913\n",
      "      433983126 15:55:22 Epoch 519/5000, D_loss=0.045856, reward=-7.867259\n",
      "      433983126 15:55:22 Epoch 520/5000, D_loss=0.044428, reward=-7.871831\n",
      "      433983126 15:55:22 Epoch 521/5000, D_loss=0.041676, reward=-7.874398\n",
      "      433983126 15:55:23 Epoch 522/5000, D_loss=0.043562, reward=-7.873159\n",
      "      433983126 15:55:23 Epoch 523/5000, D_loss=0.044556, reward=-7.874514\n",
      "      433983126 15:55:23 Epoch 524/5000, D_loss=0.042753, reward=-7.869189\n",
      "      433983126 15:55:24 Epoch 525/5000, D_loss=0.045136, reward=-7.871107\n",
      "      433983126 15:55:24 Epoch 526/5000, D_loss=0.044536, reward=-7.868460\n",
      "      433983126 15:55:25 Epoch 527/5000, D_loss=0.044134, reward=-7.868262\n",
      "      433983126 15:55:25 Epoch 528/5000, D_loss=0.044389, reward=-7.869076\n",
      "      433983126 15:55:25 Epoch 529/5000, D_loss=0.044432, reward=-7.867793\n",
      "      433983126 15:55:26 Epoch 530/5000, D_loss=0.044151, reward=-7.868917\n",
      "      433983126 15:55:26 Epoch 531/5000, D_loss=0.044326, reward=-7.873590\n",
      "      433983126 15:55:26 Epoch 532/5000, D_loss=0.044200, reward=-7.872086\n",
      "      433983126 15:55:27 Epoch 533/5000, D_loss=0.043885, reward=-7.867420\n",
      "      433983126 15:55:27 Epoch 534/5000, D_loss=0.045447, reward=-7.870089\n",
      "      433983126 15:55:28 Epoch 535/5000, D_loss=0.044261, reward=-7.866826\n",
      "      433983126 15:55:28 Epoch 536/5000, D_loss=0.047286, reward=-7.861652\n",
      "      433983126 15:55:28 Epoch 537/5000, D_loss=0.044301, reward=-7.867818\n",
      "      433983126 15:55:29 Epoch 538/5000, D_loss=0.046293, reward=-7.869815\n",
      "      433983126 15:55:29 Epoch 539/5000, D_loss=0.045095, reward=-7.868474\n",
      "      433983126 15:55:29 Epoch 540/5000, D_loss=0.044445, reward=-7.870945\n",
      "      433983126 15:55:30 Epoch 541/5000, D_loss=0.046617, reward=-7.861783\n",
      "      433983126 15:55:30 Epoch 542/5000, D_loss=0.043034, reward=-7.864084\n",
      "      433983126 15:55:31 Epoch 543/5000, D_loss=0.043633, reward=-7.864927\n",
      "      433983126 15:55:31 Epoch 544/5000, D_loss=0.045731, reward=-7.866290\n",
      "      433983126 15:55:31 Epoch 545/5000, D_loss=0.044879, reward=-7.865846\n",
      "      433983126 15:55:32 Epoch 546/5000, D_loss=0.043836, reward=-7.871678\n",
      "      433983126 15:55:32 Epoch 547/5000, D_loss=0.044951, reward=-7.866211\n",
      "      433983126 15:55:32 Epoch 548/5000, D_loss=0.044638, reward=-7.861232\n",
      "      433983126 15:55:33 Epoch 549/5000, D_loss=0.045177, reward=-7.864456\n",
      "      433983126 15:55:33 Epoch 550/5000, D_loss=0.043846, reward=-7.865057\n",
      "      433983126 15:55:34 Epoch 551/5000, D_loss=0.045121, reward=-7.860710\n",
      "      433983126 15:55:34 Epoch 552/5000, D_loss=0.043405, reward=-7.863389\n",
      "      433983126 15:55:34 Epoch 553/5000, D_loss=0.045316, reward=-7.859532\n",
      "      433983126 15:55:35 Epoch 554/5000, D_loss=0.043106, reward=-7.861655\n",
      "      433983126 15:55:35 Epoch 555/5000, D_loss=0.044321, reward=-7.867742\n",
      "      433983126 15:55:35 Epoch 556/5000, D_loss=0.042397, reward=-7.867684\n",
      "      433983126 15:55:36 Epoch 557/5000, D_loss=0.043033, reward=-7.872309\n",
      "      433983126 15:55:36 Epoch 558/5000, D_loss=0.045086, reward=-7.866899\n",
      "      433983126 15:55:36 Epoch 559/5000, D_loss=0.044735, reward=-7.855555\n",
      "      433983126 15:55:37 Epoch 560/5000, D_loss=0.045025, reward=-7.860169\n",
      "      433983126 15:55:37 Epoch 561/5000, D_loss=0.043766, reward=-7.862409\n",
      "      433983126 15:55:38 Epoch 562/5000, D_loss=0.045784, reward=-7.858385\n",
      "      433983126 15:55:38 Epoch 563/5000, D_loss=0.045989, reward=-7.854733\n",
      "      433983126 15:55:38 Epoch 564/5000, D_loss=0.045835, reward=-7.859518\n",
      "      433983126 15:55:39 Epoch 565/5000, D_loss=0.043808, reward=-7.859855\n",
      "      433983126 15:55:39 Epoch 566/5000, D_loss=0.044777, reward=-7.857827\n",
      "      433983126 15:55:39 Epoch 567/5000, D_loss=0.045092, reward=-7.860565\n",
      "      433983126 15:55:40 Epoch 568/5000, D_loss=0.044964, reward=-7.858517\n",
      "      433983126 15:55:40 Epoch 569/5000, D_loss=0.043409, reward=-7.862540\n",
      "      433983126 15:55:41 Epoch 570/5000, D_loss=0.044879, reward=-7.863491\n",
      "      433983126 15:55:41 Epoch 571/5000, D_loss=0.043772, reward=-7.862076\n",
      "      433983126 15:55:41 Epoch 572/5000, D_loss=0.044009, reward=-7.858175\n",
      "      433983126 15:55:42 Epoch 573/5000, D_loss=0.043904, reward=-7.858576\n",
      "      433983126 15:55:42 Epoch 574/5000, D_loss=0.045580, reward=-7.861526\n",
      "      433983126 15:55:42 Epoch 575/5000, D_loss=0.046913, reward=-7.861917\n",
      "      433983126 15:55:43 Epoch 576/5000, D_loss=0.045115, reward=-7.859063\n",
      "      433983126 15:55:43 Epoch 577/5000, D_loss=0.044561, reward=-7.862848\n",
      "      433983126 15:55:44 Epoch 578/5000, D_loss=0.044055, reward=-7.865687\n",
      "      433983126 15:55:44 Epoch 579/5000, D_loss=0.043562, reward=-7.858827\n",
      "      433983126 15:55:44 Epoch 580/5000, D_loss=0.044222, reward=-7.858047\n",
      "      433983126 15:55:45 Epoch 581/5000, D_loss=0.045899, reward=-7.857758\n",
      "      433983126 15:55:45 Epoch 582/5000, D_loss=0.044751, reward=-7.863383\n",
      "      433983126 15:55:45 Epoch 583/5000, D_loss=0.043708, reward=-7.859052\n",
      "      433983126 15:55:46 Epoch 584/5000, D_loss=0.042611, reward=-7.858056\n",
      "      433983126 15:55:46 Epoch 585/5000, D_loss=0.043963, reward=-7.858970\n",
      "      433983126 15:55:47 Epoch 586/5000, D_loss=0.042519, reward=-7.861725\n",
      "      433983126 15:55:47 Epoch 587/5000, D_loss=0.045044, reward=-7.860996\n",
      "      433983126 15:55:47 Epoch 588/5000, D_loss=0.043864, reward=-7.856384\n",
      "      433983126 15:55:48 Epoch 589/5000, D_loss=0.045040, reward=-7.857893\n",
      "      433983126 15:55:48 Epoch 590/5000, D_loss=0.045519, reward=-7.856897\n",
      "      433983126 15:55:48 Epoch 591/5000, D_loss=0.044894, reward=-7.858084\n",
      "      433983126 15:55:49 Epoch 592/5000, D_loss=0.043059, reward=-7.857148\n",
      "      433983126 15:55:49 Epoch 593/5000, D_loss=0.044728, reward=-7.850861\n",
      "      433983126 15:55:50 Epoch 594/5000, D_loss=0.045703, reward=-7.857523\n",
      "      433983126 15:55:50 Epoch 595/5000, D_loss=0.043724, reward=-7.861204\n",
      "      433983126 15:55:50 Epoch 596/5000, D_loss=0.044575, reward=-7.859194\n",
      "      433983126 15:55:51 Epoch 597/5000, D_loss=0.043877, reward=-7.857326\n",
      "      433983126 15:55:51 Epoch 598/5000, D_loss=0.044016, reward=-7.864720\n",
      "      433983126 15:55:52 Epoch 599/5000, D_loss=0.044577, reward=-7.858918\n",
      "      433983126 15:55:52 Epoch 600/5000, D_loss=0.045765, reward=-7.856164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hhehe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      248942384 15:55:56 MR = 4772.8681640625\n",
      "MRR = 0.17553958296775818\n",
      "Hit@1 = 0.004943968358602505\n",
      "Hit@3 = 0.3276203032300593\n",
      "Hit@10 = 0.4288068556361239\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MR = 4772.8681640625\n",
      "MRR = 0.17553958296775818\n",
      "Hit@1 = 0.004943968358602505\n",
      "Hit@3 = 0.3276203032300593\n",
      "Hit@10 = 0.4288068556361239\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      433983126 15:55:56 Epoch 601/5000, D_loss=0.045569, reward=-7.853202\n",
      "      433983126 15:55:57 Epoch 602/5000, D_loss=0.045192, reward=-7.854895\n",
      "      433983126 15:55:57 Epoch 603/5000, D_loss=0.043935, reward=-7.855207\n",
      "      433983126 15:55:57 Epoch 604/5000, D_loss=0.045824, reward=-7.856401\n",
      "      433983126 15:55:58 Epoch 605/5000, D_loss=0.044454, reward=-7.859341\n",
      "      433983126 15:55:58 Epoch 606/5000, D_loss=0.043726, reward=-7.854714\n",
      "      433983126 15:55:58 Epoch 607/5000, D_loss=0.046050, reward=-7.849056\n",
      "      433983126 15:55:59 Epoch 608/5000, D_loss=0.045044, reward=-7.851746\n",
      "      433983126 15:55:59 Epoch 609/5000, D_loss=0.044976, reward=-7.854652\n",
      "      433983126 15:56:00 Epoch 610/5000, D_loss=0.043867, reward=-7.852609\n",
      "      433983126 15:56:00 Epoch 611/5000, D_loss=0.044232, reward=-7.857585\n",
      "      433983126 15:56:00 Epoch 612/5000, D_loss=0.044906, reward=-7.859560\n",
      "      433983126 15:56:01 Epoch 613/5000, D_loss=0.042780, reward=-7.853894\n",
      "      433983126 15:56:01 Epoch 614/5000, D_loss=0.043289, reward=-7.855488\n",
      "      433983126 15:56:01 Epoch 615/5000, D_loss=0.044435, reward=-7.856548\n",
      "      433983126 15:56:02 Epoch 616/5000, D_loss=0.044421, reward=-7.852887\n",
      "      433983126 15:56:02 Epoch 617/5000, D_loss=0.042961, reward=-7.859138\n",
      "      433983126 15:56:03 Epoch 618/5000, D_loss=0.046457, reward=-7.848826\n",
      "      433983126 15:56:03 Epoch 619/5000, D_loss=0.044222, reward=-7.858716\n",
      "      433983126 15:56:03 Epoch 620/5000, D_loss=0.044848, reward=-7.852234\n",
      "      433983126 15:56:04 Epoch 621/5000, D_loss=0.044160, reward=-7.855613\n",
      "      433983126 15:56:04 Epoch 622/5000, D_loss=0.046431, reward=-7.847309\n",
      "      433983126 15:56:04 Epoch 623/5000, D_loss=0.043785, reward=-7.850608\n",
      "      433983126 15:56:05 Epoch 624/5000, D_loss=0.045451, reward=-7.851877\n",
      "      433983126 15:56:05 Epoch 625/5000, D_loss=0.044210, reward=-7.860109\n",
      "      433983126 15:56:06 Epoch 626/5000, D_loss=0.043229, reward=-7.857820\n",
      "      433983126 15:56:06 Epoch 627/5000, D_loss=0.044778, reward=-7.853123\n",
      "      433983126 15:56:06 Epoch 628/5000, D_loss=0.043972, reward=-7.846304\n",
      "      433983126 15:56:07 Epoch 629/5000, D_loss=0.043715, reward=-7.856968\n",
      "      433983126 15:56:07 Epoch 630/5000, D_loss=0.043490, reward=-7.845990\n",
      "      433983126 15:56:08 Epoch 631/5000, D_loss=0.043523, reward=-7.855641\n",
      "      433983126 15:56:08 Epoch 632/5000, D_loss=0.045498, reward=-7.849839\n",
      "      433983126 15:56:08 Epoch 633/5000, D_loss=0.043915, reward=-7.851304\n",
      "      433983126 15:56:09 Epoch 634/5000, D_loss=0.044873, reward=-7.849657\n",
      "      433983126 15:56:09 Epoch 635/5000, D_loss=0.044676, reward=-7.852727\n",
      "      433983126 15:56:09 Epoch 636/5000, D_loss=0.042037, reward=-7.857044\n",
      "      433983126 15:56:10 Epoch 637/5000, D_loss=0.044683, reward=-7.855386\n",
      "      433983126 15:56:10 Epoch 638/5000, D_loss=0.045195, reward=-7.853580\n",
      "      433983126 15:56:11 Epoch 639/5000, D_loss=0.044760, reward=-7.852718\n",
      "      433983126 15:56:11 Epoch 640/5000, D_loss=0.043939, reward=-7.853851\n",
      "      433983126 15:56:11 Epoch 641/5000, D_loss=0.044515, reward=-7.851954\n",
      "      433983126 15:56:12 Epoch 642/5000, D_loss=0.044591, reward=-7.854049\n",
      "      433983126 15:56:12 Epoch 643/5000, D_loss=0.045341, reward=-7.851960\n",
      "      433983126 15:56:12 Epoch 644/5000, D_loss=0.044996, reward=-7.851818\n",
      "      433983126 15:56:13 Epoch 645/5000, D_loss=0.044569, reward=-7.847695\n",
      "      433983126 15:56:13 Epoch 646/5000, D_loss=0.044327, reward=-7.846088\n",
      "      433983126 15:56:14 Epoch 647/5000, D_loss=0.046027, reward=-7.844419\n",
      "      433983126 15:56:14 Epoch 648/5000, D_loss=0.043984, reward=-7.848751\n",
      "      433983126 15:56:14 Epoch 649/5000, D_loss=0.044867, reward=-7.849904\n",
      "      433983126 15:56:15 Epoch 650/5000, D_loss=0.041617, reward=-7.852872\n",
      "      433983126 15:56:15 Epoch 651/5000, D_loss=0.043954, reward=-7.851686\n",
      "      433983126 15:56:15 Epoch 652/5000, D_loss=0.045728, reward=-7.852624\n",
      "      433983126 15:56:16 Epoch 653/5000, D_loss=0.043819, reward=-7.855350\n",
      "      433983126 15:56:16 Epoch 654/5000, D_loss=0.045525, reward=-7.851670\n",
      "      433983126 15:56:17 Epoch 655/5000, D_loss=0.045210, reward=-7.848408\n",
      "      433983126 15:56:17 Epoch 656/5000, D_loss=0.044861, reward=-7.847294\n",
      "      433983126 15:56:17 Epoch 657/5000, D_loss=0.045129, reward=-7.845452\n",
      "      433983126 15:56:18 Epoch 658/5000, D_loss=0.045363, reward=-7.846659\n",
      "      433983126 15:56:18 Epoch 659/5000, D_loss=0.046158, reward=-7.848499\n",
      "      433983126 15:56:18 Epoch 660/5000, D_loss=0.044761, reward=-7.848787\n",
      "      433983126 15:56:19 Epoch 661/5000, D_loss=0.044106, reward=-7.848995\n",
      "      433983126 15:56:19 Epoch 662/5000, D_loss=0.045479, reward=-7.848626\n",
      "      433983126 15:56:20 Epoch 663/5000, D_loss=0.047378, reward=-7.841441\n",
      "      433983126 15:56:20 Epoch 664/5000, D_loss=0.043136, reward=-7.850702\n",
      "      433983126 15:56:20 Epoch 665/5000, D_loss=0.044506, reward=-7.847376\n",
      "      433983126 15:56:21 Epoch 666/5000, D_loss=0.046205, reward=-7.845627\n",
      "      433983126 15:56:21 Epoch 667/5000, D_loss=0.043730, reward=-7.850198\n",
      "      433983126 15:56:21 Epoch 668/5000, D_loss=0.044063, reward=-7.847023\n",
      "      433983126 15:56:22 Epoch 669/5000, D_loss=0.044801, reward=-7.845478\n",
      "      433983126 15:56:22 Epoch 670/5000, D_loss=0.045692, reward=-7.839772\n",
      "      433983126 15:56:23 Epoch 671/5000, D_loss=0.044311, reward=-7.851197\n",
      "      433983126 15:56:23 Epoch 672/5000, D_loss=0.045356, reward=-7.852355\n",
      "      433983126 15:56:23 Epoch 673/5000, D_loss=0.045167, reward=-7.844898\n",
      "      433983126 15:56:24 Epoch 674/5000, D_loss=0.044639, reward=-7.841618\n",
      "      433983126 15:56:24 Epoch 675/5000, D_loss=0.044275, reward=-7.840883\n",
      "      433983126 15:56:24 Epoch 676/5000, D_loss=0.046319, reward=-7.847038\n",
      "      433983126 15:56:25 Epoch 677/5000, D_loss=0.044881, reward=-7.844306\n",
      "      433983126 15:56:25 Epoch 678/5000, D_loss=0.044396, reward=-7.846596\n",
      "      433983126 15:56:26 Epoch 679/5000, D_loss=0.044336, reward=-7.847461\n",
      "      433983126 15:56:26 Epoch 680/5000, D_loss=0.043893, reward=-7.848057\n",
      "      433983126 15:56:26 Epoch 681/5000, D_loss=0.042699, reward=-7.848584\n",
      "      433983126 15:56:27 Epoch 682/5000, D_loss=0.046538, reward=-7.839398\n",
      "      433983126 15:56:27 Epoch 683/5000, D_loss=0.045120, reward=-7.844731\n",
      "      433983126 15:56:27 Epoch 684/5000, D_loss=0.044490, reward=-7.847283\n",
      "      433983126 15:56:28 Epoch 685/5000, D_loss=0.045067, reward=-7.844285\n",
      "      433983126 15:56:28 Epoch 686/5000, D_loss=0.045353, reward=-7.835088\n",
      "      433983126 15:56:28 Epoch 687/5000, D_loss=0.045991, reward=-7.842027\n",
      "      433983126 15:56:29 Epoch 688/5000, D_loss=0.042393, reward=-7.844912\n",
      "      433983126 15:56:29 Epoch 689/5000, D_loss=0.045504, reward=-7.843430\n",
      "      433983126 15:56:30 Epoch 690/5000, D_loss=0.043599, reward=-7.847957\n",
      "      433983126 15:56:30 Epoch 691/5000, D_loss=0.042885, reward=-7.841826\n",
      "      433983126 15:56:30 Epoch 692/5000, D_loss=0.044564, reward=-7.836708\n",
      "      433983126 15:56:31 Epoch 693/5000, D_loss=0.044146, reward=-7.841750\n",
      "      433983126 15:56:31 Epoch 694/5000, D_loss=0.043757, reward=-7.836307\n",
      "      433983126 15:56:32 Epoch 695/5000, D_loss=0.043518, reward=-7.840006\n",
      "      433983126 15:56:32 Epoch 696/5000, D_loss=0.044928, reward=-7.837984\n",
      "      433983126 15:56:32 Epoch 697/5000, D_loss=0.044382, reward=-7.840573\n",
      "      433983126 15:56:33 Epoch 698/5000, D_loss=0.045188, reward=-7.840850\n",
      "      433983126 15:56:33 Epoch 699/5000, D_loss=0.044037, reward=-7.845584\n",
      "      433983126 15:56:33 Epoch 700/5000, D_loss=0.043810, reward=-7.845284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hhehe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      248942384 15:56:37 MR = 4762.50244140625\n",
      "MRR = 0.1745133250951767\n",
      "Hit@1 = 0.004614370468029005\n",
      "Hit@3 = 0.3266315095583388\n",
      "Hit@10 = 0.4293012524719842\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MR = 4762.50244140625\n",
      "MRR = 0.1745133250951767\n",
      "Hit@1 = 0.004614370468029005\n",
      "Hit@3 = 0.3266315095583388\n",
      "Hit@10 = 0.4293012524719842\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      433983126 15:56:38 Epoch 701/5000, D_loss=0.047420, reward=-7.838552\n",
      "      433983126 15:56:38 Epoch 702/5000, D_loss=0.044551, reward=-7.839141\n",
      "      433983126 15:56:38 Epoch 703/5000, D_loss=0.044110, reward=-7.837630\n",
      "      433983126 15:56:39 Epoch 704/5000, D_loss=0.043370, reward=-7.841883\n",
      "      433983126 15:56:39 Epoch 705/5000, D_loss=0.043514, reward=-7.849239\n",
      "      433983126 15:56:39 Epoch 706/5000, D_loss=0.042605, reward=-7.842292\n",
      "      433983126 15:56:40 Epoch 707/5000, D_loss=0.044818, reward=-7.835853\n",
      "      433983126 15:56:40 Epoch 708/5000, D_loss=0.042166, reward=-7.843591\n",
      "      433983126 15:56:41 Epoch 709/5000, D_loss=0.043358, reward=-7.848644\n",
      "      433983126 15:56:41 Epoch 710/5000, D_loss=0.042756, reward=-7.840231\n",
      "      433983126 15:56:41 Epoch 711/5000, D_loss=0.045131, reward=-7.840500\n",
      "      433983126 15:56:42 Epoch 712/5000, D_loss=0.045610, reward=-7.842658\n",
      "      433983126 15:56:42 Epoch 713/5000, D_loss=0.045723, reward=-7.839142\n",
      "      433983126 15:56:42 Epoch 714/5000, D_loss=0.044987, reward=-7.840237\n",
      "      433983126 15:56:43 Epoch 715/5000, D_loss=0.043792, reward=-7.843784\n",
      "      433983126 15:56:43 Epoch 716/5000, D_loss=0.043020, reward=-7.842196\n",
      "      433983126 15:56:43 Epoch 717/5000, D_loss=0.043175, reward=-7.840838\n",
      "      433983126 15:56:44 Epoch 718/5000, D_loss=0.043374, reward=-7.840758\n",
      "      433983126 15:56:44 Epoch 719/5000, D_loss=0.044005, reward=-7.840616\n",
      "      433983126 15:56:45 Epoch 720/5000, D_loss=0.043163, reward=-7.840237\n",
      "      433983126 15:56:45 Epoch 721/5000, D_loss=0.043813, reward=-7.836516\n",
      "      433983126 15:56:45 Epoch 722/5000, D_loss=0.043431, reward=-7.841537\n",
      "      433983126 15:56:46 Epoch 723/5000, D_loss=0.043456, reward=-7.836679\n",
      "      433983126 15:56:46 Epoch 724/5000, D_loss=0.043298, reward=-7.837534\n",
      "      433983126 15:56:46 Epoch 725/5000, D_loss=0.045037, reward=-7.836926\n",
      "      433983126 15:56:47 Epoch 726/5000, D_loss=0.042290, reward=-7.838997\n",
      "      433983126 15:56:47 Epoch 727/5000, D_loss=0.044673, reward=-7.834142\n",
      "      433983126 15:56:48 Epoch 728/5000, D_loss=0.043503, reward=-7.836533\n",
      "      433983126 15:56:48 Epoch 729/5000, D_loss=0.044425, reward=-7.833235\n",
      "      433983126 15:56:48 Epoch 730/5000, D_loss=0.045208, reward=-7.836171\n",
      "      433983126 15:56:49 Epoch 731/5000, D_loss=0.043168, reward=-7.838079\n",
      "      433983126 15:56:49 Epoch 732/5000, D_loss=0.043175, reward=-7.837640\n",
      "      433983126 15:56:49 Epoch 733/5000, D_loss=0.044127, reward=-7.837599\n",
      "      433983126 15:56:50 Epoch 734/5000, D_loss=0.043510, reward=-7.838547\n",
      "      433983126 15:56:50 Epoch 735/5000, D_loss=0.045853, reward=-7.835356\n",
      "      433983126 15:56:51 Epoch 736/5000, D_loss=0.046386, reward=-7.832731\n",
      "      433983126 15:56:51 Epoch 737/5000, D_loss=0.045492, reward=-7.836417\n",
      "      433983126 15:56:51 Epoch 738/5000, D_loss=0.044349, reward=-7.843228\n",
      "      433983126 15:56:52 Epoch 739/5000, D_loss=0.045722, reward=-7.829063\n",
      "      433983126 15:56:52 Epoch 740/5000, D_loss=0.044454, reward=-7.834344\n",
      "      433983126 15:56:52 Epoch 741/5000, D_loss=0.044017, reward=-7.832359\n",
      "      433983126 15:56:53 Epoch 742/5000, D_loss=0.045318, reward=-7.831590\n",
      "      433983126 15:56:53 Epoch 743/5000, D_loss=0.045442, reward=-7.830716\n",
      "      433983126 15:56:53 Epoch 744/5000, D_loss=0.044765, reward=-7.837155\n",
      "      433983126 15:56:54 Epoch 745/5000, D_loss=0.044077, reward=-7.834653\n",
      "      433983126 15:56:54 Epoch 746/5000, D_loss=0.044591, reward=-7.833533\n",
      "      433983126 15:56:55 Epoch 747/5000, D_loss=0.045135, reward=-7.834120\n",
      "      433983126 15:56:55 Epoch 748/5000, D_loss=0.046240, reward=-7.832982\n",
      "      433983126 15:56:55 Epoch 749/5000, D_loss=0.044270, reward=-7.834681\n",
      "      433983126 15:56:56 Epoch 750/5000, D_loss=0.044965, reward=-7.827780\n",
      "      433983126 15:56:56 Epoch 751/5000, D_loss=0.047460, reward=-7.828737\n",
      "      433983126 15:56:56 Epoch 752/5000, D_loss=0.045480, reward=-7.828983\n",
      "      433983126 15:56:57 Epoch 753/5000, D_loss=0.043133, reward=-7.830002\n",
      "      433983126 15:56:57 Epoch 754/5000, D_loss=0.044927, reward=-7.833148\n",
      "      433983126 15:56:58 Epoch 755/5000, D_loss=0.043334, reward=-7.838023\n",
      "      433983126 15:56:58 Epoch 756/5000, D_loss=0.044955, reward=-7.830132\n",
      "      433983126 15:56:58 Epoch 757/5000, D_loss=0.044473, reward=-7.830601\n",
      "      433983126 15:56:59 Epoch 758/5000, D_loss=0.044837, reward=-7.826395\n",
      "      433983126 15:56:59 Epoch 759/5000, D_loss=0.042324, reward=-7.834418\n",
      "      433983126 15:56:59 Epoch 760/5000, D_loss=0.044076, reward=-7.833985\n",
      "      433983126 15:57:00 Epoch 761/5000, D_loss=0.046540, reward=-7.823366\n",
      "      433983126 15:57:00 Epoch 762/5000, D_loss=0.046034, reward=-7.828875\n",
      "      433983126 15:57:00 Epoch 763/5000, D_loss=0.044291, reward=-7.826155\n",
      "      433983126 15:57:01 Epoch 764/5000, D_loss=0.043994, reward=-7.832893\n",
      "      433983126 15:57:01 Epoch 765/5000, D_loss=0.043127, reward=-7.828762\n",
      "      433983126 15:57:02 Epoch 766/5000, D_loss=0.044050, reward=-7.830956\n",
      "      433983126 15:57:02 Epoch 767/5000, D_loss=0.044280, reward=-7.829920\n",
      "      433983126 15:57:02 Epoch 768/5000, D_loss=0.045171, reward=-7.824168\n",
      "      433983126 15:57:03 Epoch 769/5000, D_loss=0.045008, reward=-7.829105\n",
      "      433983126 15:57:03 Epoch 770/5000, D_loss=0.043781, reward=-7.823831\n",
      "      433983126 15:57:03 Epoch 771/5000, D_loss=0.045109, reward=-7.820470\n",
      "      433983126 15:57:04 Epoch 772/5000, D_loss=0.045852, reward=-7.826753\n",
      "      433983126 15:57:04 Epoch 773/5000, D_loss=0.043376, reward=-7.835329\n",
      "      433983126 15:57:04 Epoch 774/5000, D_loss=0.046154, reward=-7.827251\n",
      "      433983126 15:57:05 Epoch 775/5000, D_loss=0.044738, reward=-7.827048\n",
      "      433983126 15:57:05 Epoch 776/5000, D_loss=0.045478, reward=-7.827765\n",
      "      433983126 15:57:06 Epoch 777/5000, D_loss=0.046179, reward=-7.825885\n",
      "      433983126 15:57:06 Epoch 778/5000, D_loss=0.044540, reward=-7.832171\n",
      "      433983126 15:57:06 Epoch 779/5000, D_loss=0.045664, reward=-7.827621\n",
      "      433983126 15:57:07 Epoch 780/5000, D_loss=0.046143, reward=-7.822140\n",
      "      433983126 15:57:07 Epoch 781/5000, D_loss=0.044186, reward=-7.824261\n",
      "      433983126 15:57:07 Epoch 782/5000, D_loss=0.045602, reward=-7.826975\n",
      "      433983126 15:57:08 Epoch 783/5000, D_loss=0.045080, reward=-7.824732\n",
      "      433983126 15:57:08 Epoch 784/5000, D_loss=0.045891, reward=-7.830035\n",
      "      433983126 15:57:09 Epoch 785/5000, D_loss=0.044859, reward=-7.828683\n",
      "      433983126 15:57:09 Epoch 786/5000, D_loss=0.043518, reward=-7.828653\n",
      "      433983126 15:57:09 Epoch 787/5000, D_loss=0.046179, reward=-7.825396\n",
      "      433983126 15:57:10 Epoch 788/5000, D_loss=0.043271, reward=-7.824407\n",
      "      433983126 15:57:10 Epoch 789/5000, D_loss=0.043901, reward=-7.821508\n",
      "      433983126 15:57:10 Epoch 790/5000, D_loss=0.044439, reward=-7.823123\n",
      "      433983126 15:57:11 Epoch 791/5000, D_loss=0.044338, reward=-7.823666\n",
      "      433983126 15:57:11 Epoch 792/5000, D_loss=0.044405, reward=-7.830364\n",
      "      433983126 15:57:11 Epoch 793/5000, D_loss=0.043859, reward=-7.832745\n",
      "      433983126 15:57:12 Epoch 794/5000, D_loss=0.045446, reward=-7.825046\n",
      "      433983126 15:57:12 Epoch 795/5000, D_loss=0.044338, reward=-7.824984\n",
      "      433983126 15:57:13 Epoch 796/5000, D_loss=0.042183, reward=-7.830749\n",
      "      433983126 15:57:13 Epoch 797/5000, D_loss=0.045985, reward=-7.825590\n",
      "      433983126 15:57:13 Epoch 798/5000, D_loss=0.044174, reward=-7.829421\n",
      "      433983126 15:57:14 Epoch 799/5000, D_loss=0.044520, reward=-7.824741\n",
      "      433983126 15:57:14 Epoch 800/5000, D_loss=0.044428, reward=-7.825171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hhehe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      248942384 15:57:18 MR = 4789.22607421875\n",
      "MRR = 0.17442521452903748\n",
      "Hit@1 = 0.004449571522742254\n",
      "Hit@3 = 0.3246539222148978\n",
      "Hit@10 = 0.4299604482531312\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MR = 4789.22607421875\n",
      "MRR = 0.17442521452903748\n",
      "Hit@1 = 0.004449571522742254\n",
      "Hit@3 = 0.3246539222148978\n",
      "Hit@10 = 0.4299604482531312\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      433983126 15:57:18 Epoch 801/5000, D_loss=0.045540, reward=-7.821571\n",
      "      433983126 15:57:19 Epoch 802/5000, D_loss=0.042724, reward=-7.829964\n",
      "      433983126 15:57:19 Epoch 803/5000, D_loss=0.045321, reward=-7.822020\n",
      "      433983126 15:57:19 Epoch 804/5000, D_loss=0.045212, reward=-7.825594\n",
      "      433983126 15:57:20 Epoch 805/5000, D_loss=0.045258, reward=-7.823433\n",
      "      433983126 15:57:20 Epoch 806/5000, D_loss=0.044512, reward=-7.820435\n",
      "      433983126 15:57:21 Epoch 807/5000, D_loss=0.045307, reward=-7.820511\n",
      "      433983126 15:57:21 Epoch 808/5000, D_loss=0.044146, reward=-7.824034\n",
      "      433983126 15:57:21 Epoch 809/5000, D_loss=0.043557, reward=-7.824746\n",
      "      433983126 15:57:22 Epoch 810/5000, D_loss=0.045349, reward=-7.824862\n",
      "      433983126 15:57:22 Epoch 811/5000, D_loss=0.044292, reward=-7.822688\n",
      "      433983126 15:57:22 Epoch 812/5000, D_loss=0.044360, reward=-7.823657\n",
      "      433983126 15:57:23 Epoch 813/5000, D_loss=0.045932, reward=-7.822784\n",
      "      433983126 15:57:23 Epoch 814/5000, D_loss=0.045024, reward=-7.823565\n",
      "      433983126 15:57:24 Epoch 815/5000, D_loss=0.045379, reward=-7.821760\n",
      "      433983126 15:57:24 Epoch 816/5000, D_loss=0.045659, reward=-7.816782\n",
      "      433983126 15:57:24 Epoch 817/5000, D_loss=0.042919, reward=-7.823594\n",
      "      433983126 15:57:25 Epoch 818/5000, D_loss=0.044351, reward=-7.825337\n",
      "      433983126 15:57:25 Epoch 819/5000, D_loss=0.043876, reward=-7.822659\n",
      "      433983126 15:57:26 Epoch 820/5000, D_loss=0.046341, reward=-7.818954\n",
      "      433983126 15:57:26 Epoch 821/5000, D_loss=0.043750, reward=-7.824713\n",
      "      433983126 15:57:26 Epoch 822/5000, D_loss=0.045075, reward=-7.818636\n",
      "      433983126 15:57:27 Epoch 823/5000, D_loss=0.044905, reward=-7.817828\n",
      "      433983126 15:57:27 Epoch 824/5000, D_loss=0.043904, reward=-7.824470\n",
      "      433983126 15:57:27 Epoch 825/5000, D_loss=0.045108, reward=-7.817784\n",
      "      433983126 15:57:28 Epoch 826/5000, D_loss=0.043080, reward=-7.821432\n",
      "      433983126 15:57:28 Epoch 827/5000, D_loss=0.044353, reward=-7.820296\n",
      "      433983126 15:57:29 Epoch 828/5000, D_loss=0.044619, reward=-7.820815\n",
      "      433983126 15:57:29 Epoch 829/5000, D_loss=0.044754, reward=-7.820050\n",
      "      433983126 15:57:29 Epoch 830/5000, D_loss=0.043990, reward=-7.821155\n",
      "      433983126 15:57:30 Epoch 831/5000, D_loss=0.044624, reward=-7.821929\n",
      "      433983126 15:57:30 Epoch 832/5000, D_loss=0.044683, reward=-7.822945\n",
      "      433983126 15:57:30 Epoch 833/5000, D_loss=0.043919, reward=-7.818177\n",
      "      433983126 15:57:31 Epoch 834/5000, D_loss=0.044320, reward=-7.820277\n",
      "      433983126 15:57:31 Epoch 835/5000, D_loss=0.045645, reward=-7.818769\n",
      "      433983126 15:57:32 Epoch 836/5000, D_loss=0.044032, reward=-7.814795\n",
      "      433983126 15:57:32 Epoch 837/5000, D_loss=0.044078, reward=-7.819392\n",
      "      433983126 15:57:32 Epoch 838/5000, D_loss=0.043775, reward=-7.816781\n",
      "      433983126 15:57:33 Epoch 839/5000, D_loss=0.042348, reward=-7.823366\n",
      "      433983126 15:57:33 Epoch 840/5000, D_loss=0.044283, reward=-7.813639\n",
      "      433983126 15:57:34 Epoch 841/5000, D_loss=0.045367, reward=-7.812638\n",
      "      433983126 15:57:34 Epoch 842/5000, D_loss=0.044277, reward=-7.808167\n",
      "      433983126 15:57:34 Epoch 843/5000, D_loss=0.045758, reward=-7.818050\n",
      "      433983126 15:57:35 Epoch 844/5000, D_loss=0.045363, reward=-7.812213\n",
      "      433983126 15:57:35 Epoch 845/5000, D_loss=0.042949, reward=-7.816217\n",
      "      433983126 15:57:35 Epoch 846/5000, D_loss=0.045899, reward=-7.813891\n",
      "      433983126 15:57:36 Epoch 847/5000, D_loss=0.045121, reward=-7.812663\n",
      "      433983126 15:57:36 Epoch 848/5000, D_loss=0.044519, reward=-7.814673\n",
      "      433983126 15:57:37 Epoch 849/5000, D_loss=0.045024, reward=-7.819935\n",
      "      433983126 15:57:37 Epoch 850/5000, D_loss=0.045310, reward=-7.812694\n",
      "      433983126 15:57:37 Epoch 851/5000, D_loss=0.043705, reward=-7.815802\n",
      "      433983126 15:57:38 Epoch 852/5000, D_loss=0.044561, reward=-7.817730\n",
      "      433983126 15:57:38 Epoch 853/5000, D_loss=0.044102, reward=-7.820872\n",
      "      433983126 15:57:39 Epoch 854/5000, D_loss=0.045023, reward=-7.812040\n",
      "      433983126 15:57:39 Epoch 855/5000, D_loss=0.045597, reward=-7.813800\n",
      "      433983126 15:57:39 Epoch 856/5000, D_loss=0.045132, reward=-7.812363\n",
      "      433983126 15:57:40 Epoch 857/5000, D_loss=0.043258, reward=-7.819785\n",
      "      433983126 15:57:40 Epoch 858/5000, D_loss=0.044109, reward=-7.812487\n",
      "      433983126 15:57:40 Epoch 859/5000, D_loss=0.042724, reward=-7.820190\n",
      "      433983126 15:57:41 Epoch 860/5000, D_loss=0.043981, reward=-7.816427\n",
      "      433983126 15:57:41 Epoch 861/5000, D_loss=0.045666, reward=-7.809119\n",
      "      433983126 15:57:42 Epoch 862/5000, D_loss=0.044041, reward=-7.812522\n",
      "      433983126 15:57:42 Epoch 863/5000, D_loss=0.044050, reward=-7.811836\n",
      "      433983126 15:57:42 Epoch 864/5000, D_loss=0.043823, reward=-7.814064\n",
      "      433983126 15:57:43 Epoch 865/5000, D_loss=0.043518, reward=-7.813070\n",
      "      433983126 15:57:43 Epoch 866/5000, D_loss=0.045624, reward=-7.808174\n",
      "      433983126 15:57:43 Epoch 867/5000, D_loss=0.043404, reward=-7.817603\n",
      "      433983126 15:57:44 Epoch 868/5000, D_loss=0.044767, reward=-7.814715\n",
      "      433983126 15:57:44 Epoch 869/5000, D_loss=0.043994, reward=-7.816955\n",
      "      433983126 15:57:45 Epoch 870/5000, D_loss=0.045349, reward=-7.811748\n",
      "      433983126 15:57:45 Epoch 871/5000, D_loss=0.045483, reward=-7.810970\n",
      "      433983126 15:57:45 Epoch 872/5000, D_loss=0.046132, reward=-7.812570\n",
      "      433983126 15:57:46 Epoch 873/5000, D_loss=0.044395, reward=-7.814982\n",
      "      433983126 15:57:46 Epoch 874/5000, D_loss=0.045224, reward=-7.814001\n",
      "      433983126 15:57:46 Epoch 875/5000, D_loss=0.044157, reward=-7.811168\n",
      "      433983126 15:57:47 Epoch 876/5000, D_loss=0.044802, reward=-7.806033\n",
      "      433983126 15:57:47 Epoch 877/5000, D_loss=0.045005, reward=-7.810881\n",
      "      433983126 15:57:48 Epoch 878/5000, D_loss=0.044105, reward=-7.814365\n",
      "      433983126 15:57:48 Epoch 879/5000, D_loss=0.043965, reward=-7.804865\n",
      "      433983126 15:57:48 Epoch 880/5000, D_loss=0.047270, reward=-7.807948\n",
      "      433983126 15:57:49 Epoch 881/5000, D_loss=0.044217, reward=-7.813026\n",
      "      433983126 15:57:49 Epoch 882/5000, D_loss=0.044876, reward=-7.809819\n",
      "      433983126 15:57:49 Epoch 883/5000, D_loss=0.043171, reward=-7.813767\n",
      "      433983126 15:57:50 Epoch 884/5000, D_loss=0.043104, reward=-7.810613\n",
      "      433983126 15:57:50 Epoch 885/5000, D_loss=0.044765, reward=-7.808617\n",
      "      433983126 15:57:51 Epoch 886/5000, D_loss=0.046169, reward=-7.809957\n",
      "      433983126 15:57:51 Epoch 887/5000, D_loss=0.045423, reward=-7.805399\n",
      "      433983126 15:57:51 Epoch 888/5000, D_loss=0.044194, reward=-7.809475\n",
      "      433983126 15:57:52 Epoch 889/5000, D_loss=0.045268, reward=-7.807428\n",
      "      433983126 15:57:52 Epoch 890/5000, D_loss=0.045511, reward=-7.810856\n",
      "      433983126 15:57:52 Epoch 891/5000, D_loss=0.046737, reward=-7.804017\n",
      "      433983126 15:57:53 Epoch 892/5000, D_loss=0.043779, reward=-7.809813\n",
      "      433983126 15:57:53 Epoch 893/5000, D_loss=0.044568, reward=-7.808752\n",
      "      433983126 15:57:54 Epoch 894/5000, D_loss=0.043349, reward=-7.813763\n",
      "      433983126 15:57:54 Epoch 895/5000, D_loss=0.044639, reward=-7.802452\n",
      "      433983126 15:57:54 Epoch 896/5000, D_loss=0.044674, reward=-7.817773\n",
      "      433983126 15:57:55 Epoch 897/5000, D_loss=0.045257, reward=-7.809475\n",
      "      433983126 15:57:55 Epoch 898/5000, D_loss=0.044654, reward=-7.804140\n",
      "      433983126 15:57:56 Epoch 899/5000, D_loss=0.045836, reward=-7.804719\n",
      "      433983126 15:57:56 Epoch 900/5000, D_loss=0.046426, reward=-7.800056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hhehe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      248942384 15:58:00 MR = 4721.64794921875\n",
      "MRR = 0.17756271362304688\n",
      "Hit@1 = 0.007251153592617007\n",
      "Hit@3 = 0.3266315095583388\n",
      "Hit@10 = 0.4342452208305867\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MR = 4721.64794921875\n",
      "MRR = 0.17756271362304688\n",
      "Hit@1 = 0.007251153592617007\n",
      "Hit@3 = 0.3266315095583388\n",
      "Hit@10 = 0.4342452208305867\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      433983126 15:58:00 Epoch 901/5000, D_loss=0.044935, reward=-7.807064\n",
      "      433983126 15:58:00 Epoch 902/5000, D_loss=0.045312, reward=-7.802032\n",
      "      433983126 15:58:01 Epoch 903/5000, D_loss=0.043868, reward=-7.806353\n",
      "      433983126 15:58:01 Epoch 904/5000, D_loss=0.045326, reward=-7.801484\n",
      "      433983126 15:58:02 Epoch 905/5000, D_loss=0.045224, reward=-7.808917\n",
      "      433983126 15:58:02 Epoch 906/5000, D_loss=0.046778, reward=-7.800796\n",
      "      433983126 15:58:02 Epoch 907/5000, D_loss=0.045651, reward=-7.809041\n",
      "      433983126 15:58:03 Epoch 908/5000, D_loss=0.044285, reward=-7.804794\n",
      "      433983126 15:58:03 Epoch 909/5000, D_loss=0.043641, reward=-7.810477\n",
      "      433983126 15:58:04 Epoch 910/5000, D_loss=0.046691, reward=-7.805077\n",
      "      433983126 15:58:04 Epoch 911/5000, D_loss=0.046752, reward=-7.804611\n",
      "      433983126 15:58:04 Epoch 912/5000, D_loss=0.046979, reward=-7.799766\n",
      "      433983126 15:58:05 Epoch 913/5000, D_loss=0.044085, reward=-7.803256\n",
      "      433983126 15:58:05 Epoch 914/5000, D_loss=0.044410, reward=-7.807158\n",
      "      433983126 15:58:05 Epoch 915/5000, D_loss=0.044846, reward=-7.806154\n",
      "      433983126 15:58:06 Epoch 916/5000, D_loss=0.043677, reward=-7.802395\n",
      "      433983126 15:58:06 Epoch 917/5000, D_loss=0.044119, reward=-7.801126\n",
      "      433983126 15:58:07 Epoch 918/5000, D_loss=0.043919, reward=-7.806281\n",
      "      433983126 15:58:07 Epoch 919/5000, D_loss=0.045881, reward=-7.797965\n",
      "      433983126 15:58:07 Epoch 920/5000, D_loss=0.044073, reward=-7.799639\n",
      "      433983126 15:58:08 Epoch 921/5000, D_loss=0.045695, reward=-7.802588\n",
      "      433983126 15:58:08 Epoch 922/5000, D_loss=0.045452, reward=-7.802168\n",
      "      433983126 15:58:08 Epoch 923/5000, D_loss=0.044241, reward=-7.806873\n",
      "      433983126 15:58:09 Epoch 924/5000, D_loss=0.045967, reward=-7.793052\n",
      "      433983126 15:58:09 Epoch 925/5000, D_loss=0.044124, reward=-7.807124\n",
      "      433983126 15:58:10 Epoch 926/5000, D_loss=0.044231, reward=-7.802475\n",
      "      433983126 15:58:10 Epoch 927/5000, D_loss=0.044524, reward=-7.799410\n",
      "      433983126 15:58:10 Epoch 928/5000, D_loss=0.043920, reward=-7.801019\n",
      "      433983126 15:58:11 Epoch 929/5000, D_loss=0.044934, reward=-7.798760\n",
      "      433983126 15:58:11 Epoch 930/5000, D_loss=0.043030, reward=-7.800448\n",
      "      433983126 15:58:11 Epoch 931/5000, D_loss=0.043806, reward=-7.804842\n",
      "      433983126 15:58:12 Epoch 932/5000, D_loss=0.044320, reward=-7.798582\n",
      "      433983126 15:58:12 Epoch 933/5000, D_loss=0.044723, reward=-7.801891\n",
      "      433983126 15:58:13 Epoch 934/5000, D_loss=0.048264, reward=-7.796279\n",
      "      433983126 15:58:13 Epoch 935/5000, D_loss=0.047991, reward=-7.790052\n",
      "      433983126 15:58:13 Epoch 936/5000, D_loss=0.043012, reward=-7.799545\n",
      "      433983126 15:58:14 Epoch 937/5000, D_loss=0.045402, reward=-7.795009\n",
      "      433983126 15:58:14 Epoch 938/5000, D_loss=0.046213, reward=-7.801692\n",
      "      433983126 15:58:15 Epoch 939/5000, D_loss=0.044492, reward=-7.802412\n",
      "      433983126 15:58:15 Epoch 940/5000, D_loss=0.045925, reward=-7.794951\n",
      "      433983126 15:58:15 Epoch 941/5000, D_loss=0.046021, reward=-7.795577\n",
      "      433983126 15:58:16 Epoch 942/5000, D_loss=0.044085, reward=-7.798910\n",
      "      433983126 15:58:16 Epoch 943/5000, D_loss=0.044083, reward=-7.792628\n",
      "      433983126 15:58:16 Epoch 944/5000, D_loss=0.043714, reward=-7.795660\n",
      "      433983126 15:58:17 Epoch 945/5000, D_loss=0.045858, reward=-7.794312\n",
      "      433983126 15:58:17 Epoch 946/5000, D_loss=0.042764, reward=-7.800050\n",
      "      433983126 15:58:18 Epoch 947/5000, D_loss=0.044464, reward=-7.794218\n",
      "      433983126 15:58:18 Epoch 948/5000, D_loss=0.043951, reward=-7.805965\n",
      "      433983126 15:58:18 Epoch 949/5000, D_loss=0.044641, reward=-7.798676\n",
      "      433983126 15:58:19 Epoch 950/5000, D_loss=0.044589, reward=-7.797248\n",
      "      433983126 15:58:19 Epoch 951/5000, D_loss=0.044823, reward=-7.795438\n",
      "      433983126 15:58:19 Epoch 952/5000, D_loss=0.043044, reward=-7.796029\n",
      "      433983126 15:58:20 Epoch 953/5000, D_loss=0.046345, reward=-7.793560\n",
      "      433983126 15:58:20 Epoch 954/5000, D_loss=0.044078, reward=-7.799333\n",
      "      433983126 15:58:21 Epoch 955/5000, D_loss=0.043804, reward=-7.796832\n",
      "      433983126 15:58:21 Epoch 956/5000, D_loss=0.046159, reward=-7.793285\n",
      "      433983126 15:58:21 Epoch 957/5000, D_loss=0.043615, reward=-7.796984\n",
      "      433983126 15:58:22 Epoch 958/5000, D_loss=0.042313, reward=-7.799610\n",
      "      433983126 15:58:22 Epoch 959/5000, D_loss=0.045188, reward=-7.790108\n",
      "      433983126 15:58:22 Epoch 960/5000, D_loss=0.044503, reward=-7.794284\n",
      "      433983126 15:58:23 Epoch 961/5000, D_loss=0.045042, reward=-7.791703\n",
      "      433983126 15:58:23 Epoch 962/5000, D_loss=0.044137, reward=-7.793915\n",
      "      433983126 15:58:24 Epoch 963/5000, D_loss=0.045425, reward=-7.792469\n",
      "      433983126 15:58:24 Epoch 964/5000, D_loss=0.043877, reward=-7.790838\n",
      "      433983126 15:58:24 Epoch 965/5000, D_loss=0.045837, reward=-7.789425\n",
      "      433983126 15:58:25 Epoch 966/5000, D_loss=0.045972, reward=-7.787737\n",
      "      433983126 15:58:25 Epoch 967/5000, D_loss=0.047166, reward=-7.794752\n",
      "      433983126 15:58:25 Epoch 968/5000, D_loss=0.045357, reward=-7.792056\n",
      "      433983126 15:58:26 Epoch 969/5000, D_loss=0.044403, reward=-7.787968\n",
      "      433983126 15:58:26 Epoch 970/5000, D_loss=0.043979, reward=-7.797626\n",
      "      433983126 15:58:27 Epoch 971/5000, D_loss=0.045448, reward=-7.790300\n",
      "      433983126 15:58:27 Epoch 972/5000, D_loss=0.043644, reward=-7.792273\n",
      "      433983126 15:58:27 Epoch 973/5000, D_loss=0.045733, reward=-7.787494\n",
      "      433983126 15:58:28 Epoch 974/5000, D_loss=0.044304, reward=-7.793264\n",
      "      433983126 15:58:28 Epoch 975/5000, D_loss=0.046348, reward=-7.790227\n",
      "      433983126 15:58:28 Epoch 976/5000, D_loss=0.045779, reward=-7.784444\n",
      "      433983126 15:58:29 Epoch 977/5000, D_loss=0.045801, reward=-7.787922\n",
      "      433983126 15:58:29 Epoch 978/5000, D_loss=0.044659, reward=-7.790297\n",
      "      433983126 15:58:30 Epoch 979/5000, D_loss=0.046423, reward=-7.790026\n",
      "      433983126 15:58:30 Epoch 980/5000, D_loss=0.045260, reward=-7.787836\n",
      "      433983126 15:58:30 Epoch 981/5000, D_loss=0.046216, reward=-7.784690\n",
      "      433983126 15:58:31 Epoch 982/5000, D_loss=0.044278, reward=-7.791804\n",
      "      433983126 15:58:31 Epoch 983/5000, D_loss=0.047081, reward=-7.786926\n",
      "      433983126 15:58:31 Epoch 984/5000, D_loss=0.048107, reward=-7.787215\n",
      "      433983126 15:58:32 Epoch 985/5000, D_loss=0.046161, reward=-7.793322\n",
      "      433983126 15:58:32 Epoch 986/5000, D_loss=0.045295, reward=-7.789488\n",
      "      433983126 15:58:33 Epoch 987/5000, D_loss=0.046776, reward=-7.789925\n",
      "      433983126 15:58:33 Epoch 988/5000, D_loss=0.046599, reward=-7.787907\n",
      "      433983126 15:58:33 Epoch 989/5000, D_loss=0.045348, reward=-7.786238\n",
      "      433983126 15:58:34 Epoch 990/5000, D_loss=0.044436, reward=-7.788974\n",
      "      433983126 15:58:34 Epoch 991/5000, D_loss=0.046591, reward=-7.784411\n",
      "      433983126 15:58:34 Epoch 992/5000, D_loss=0.043851, reward=-7.789557\n",
      "      433983126 15:58:35 Epoch 993/5000, D_loss=0.045619, reward=-7.786524\n",
      "      433983126 15:58:35 Epoch 994/5000, D_loss=0.045638, reward=-7.784745\n",
      "      433983126 15:58:36 Epoch 995/5000, D_loss=0.046105, reward=-7.788168\n",
      "      433983126 15:58:36 Epoch 996/5000, D_loss=0.045122, reward=-7.781297\n",
      "      433983126 15:58:36 Epoch 997/5000, D_loss=0.047305, reward=-7.777665\n",
      "      433983126 15:58:37 Epoch 998/5000, D_loss=0.044560, reward=-7.787353\n",
      "      433983126 15:58:37 Epoch 999/5000, D_loss=0.043273, reward=-7.789756\n",
      "      433983126 15:58:37 Epoch 1000/5000, D_loss=0.043242, reward=-7.786948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hhehe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      248942384 15:58:41 MR = 4640.31396484375\n",
      "MRR = 0.1773034781217575\n",
      "Hit@1 = 0.0065919578114700065\n",
      "Hit@3 = 0.3269611074489123\n",
      "Hit@10 = 0.43342122610415296\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MR = 4640.31396484375\n",
      "MRR = 0.1773034781217575\n",
      "Hit@1 = 0.0065919578114700065\n",
      "Hit@3 = 0.3269611074489123\n",
      "Hit@10 = 0.43342122610415296\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      433983126 15:58:42 Epoch 1001/5000, D_loss=0.045539, reward=-7.781003\n",
      "      433983126 15:58:42 Epoch 1002/5000, D_loss=0.044238, reward=-7.785508\n",
      "      433983126 15:58:42 Epoch 1003/5000, D_loss=0.046808, reward=-7.782728\n",
      "      433983126 15:58:43 Epoch 1004/5000, D_loss=0.046011, reward=-7.776143\n",
      "      433983126 15:58:43 Epoch 1005/5000, D_loss=0.046864, reward=-7.787508\n",
      "      433983126 15:58:43 Epoch 1006/5000, D_loss=0.045537, reward=-7.783315\n",
      "      433983126 15:58:44 Epoch 1007/5000, D_loss=0.045893, reward=-7.781581\n",
      "      433983126 15:58:44 Epoch 1008/5000, D_loss=0.046408, reward=-7.778918\n",
      "      433983126 15:58:45 Epoch 1009/5000, D_loss=0.046089, reward=-7.778867\n",
      "      433983126 15:58:45 Epoch 1010/5000, D_loss=0.045345, reward=-7.779580\n",
      "      433983126 15:58:45 Epoch 1011/5000, D_loss=0.045479, reward=-7.782218\n",
      "      433983126 15:58:46 Epoch 1012/5000, D_loss=0.045468, reward=-7.784847\n",
      "      433983126 15:58:46 Epoch 1013/5000, D_loss=0.047425, reward=-7.779617\n",
      "      433983126 15:58:46 Epoch 1014/5000, D_loss=0.046197, reward=-7.780869\n",
      "      433983126 15:58:47 Epoch 1015/5000, D_loss=0.046133, reward=-7.778652\n",
      "      433983126 15:58:47 Epoch 1016/5000, D_loss=0.046349, reward=-7.777352\n",
      "      433983126 15:58:48 Epoch 1017/5000, D_loss=0.046684, reward=-7.783359\n",
      "      433983126 15:58:48 Epoch 1018/5000, D_loss=0.046188, reward=-7.779891\n",
      "      433983126 15:58:48 Epoch 1019/5000, D_loss=0.045836, reward=-7.778378\n",
      "      433983126 15:58:49 Epoch 1020/5000, D_loss=0.045272, reward=-7.775449\n",
      "      433983126 15:58:49 Epoch 1021/5000, D_loss=0.044768, reward=-7.785913\n",
      "      433983126 15:58:49 Epoch 1022/5000, D_loss=0.044935, reward=-7.779900\n",
      "      433983126 15:58:50 Epoch 1023/5000, D_loss=0.046575, reward=-7.777095\n",
      "      433983126 15:58:50 Epoch 1024/5000, D_loss=0.044251, reward=-7.774845\n",
      "      433983126 15:58:51 Epoch 1025/5000, D_loss=0.044722, reward=-7.781331\n",
      "      433983126 15:58:51 Epoch 1026/5000, D_loss=0.044876, reward=-7.777983\n",
      "      433983126 15:58:51 Epoch 1027/5000, D_loss=0.046905, reward=-7.779402\n",
      "      433983126 15:58:52 Epoch 1028/5000, D_loss=0.047310, reward=-7.775698\n",
      "      433983126 15:58:52 Epoch 1029/5000, D_loss=0.044820, reward=-7.785614\n",
      "      433983126 15:58:52 Epoch 1030/5000, D_loss=0.045050, reward=-7.778993\n",
      "      433983126 15:58:53 Epoch 1031/5000, D_loss=0.045818, reward=-7.779006\n",
      "      433983126 15:58:53 Epoch 1032/5000, D_loss=0.045878, reward=-7.772273\n",
      "      433983126 15:58:54 Epoch 1033/5000, D_loss=0.046301, reward=-7.767912\n",
      "      433983126 15:58:54 Epoch 1034/5000, D_loss=0.046809, reward=-7.771406\n",
      "      433983126 15:58:54 Epoch 1035/5000, D_loss=0.043692, reward=-7.783535\n",
      "      433983126 15:58:55 Epoch 1036/5000, D_loss=0.044494, reward=-7.777982\n",
      "      433983126 15:58:55 Epoch 1037/5000, D_loss=0.044289, reward=-7.778421\n",
      "      433983126 15:58:55 Epoch 1038/5000, D_loss=0.045561, reward=-7.769348\n",
      "      433983126 15:58:56 Epoch 1039/5000, D_loss=0.046338, reward=-7.774240\n",
      "      433983126 15:58:56 Epoch 1040/5000, D_loss=0.044412, reward=-7.778510\n",
      "      433983126 15:58:57 Epoch 1041/5000, D_loss=0.044690, reward=-7.773439\n",
      "      433983126 15:58:57 Epoch 1042/5000, D_loss=0.045786, reward=-7.772249\n",
      "      433983126 15:58:57 Epoch 1043/5000, D_loss=0.045506, reward=-7.770586\n",
      "      433983126 15:58:58 Epoch 1044/5000, D_loss=0.045314, reward=-7.774062\n",
      "      433983126 15:58:58 Epoch 1045/5000, D_loss=0.048103, reward=-7.769681\n",
      "      433983126 15:58:58 Epoch 1046/5000, D_loss=0.043960, reward=-7.774752\n",
      "      433983126 15:58:59 Epoch 1047/5000, D_loss=0.044515, reward=-7.778048\n",
      "      433983126 15:58:59 Epoch 1048/5000, D_loss=0.043487, reward=-7.779816\n",
      "      433983126 15:59:00 Epoch 1049/5000, D_loss=0.044517, reward=-7.773985\n",
      "      433983126 15:59:00 Epoch 1050/5000, D_loss=0.046544, reward=-7.770886\n",
      "      433983126 15:59:00 Epoch 1051/5000, D_loss=0.046208, reward=-7.769182\n",
      "      433983126 15:59:01 Epoch 1052/5000, D_loss=0.045148, reward=-7.769774\n",
      "      433983126 15:59:01 Epoch 1053/5000, D_loss=0.046228, reward=-7.776293\n",
      "      433983126 15:59:02 Epoch 1054/5000, D_loss=0.045646, reward=-7.769571\n",
      "      433983126 15:59:02 Epoch 1055/5000, D_loss=0.045626, reward=-7.778538\n",
      "      433983126 15:59:02 Epoch 1056/5000, D_loss=0.046513, reward=-7.775734\n",
      "      433983126 15:59:03 Epoch 1057/5000, D_loss=0.044298, reward=-7.775536\n",
      "      433983126 15:59:03 Epoch 1058/5000, D_loss=0.045769, reward=-7.772080\n",
      "      433983126 15:59:03 Epoch 1059/5000, D_loss=0.045254, reward=-7.774058\n",
      "      433983126 15:59:04 Epoch 1060/5000, D_loss=0.044619, reward=-7.772823\n",
      "      433983126 15:59:04 Epoch 1061/5000, D_loss=0.046131, reward=-7.773551\n",
      "      433983126 15:59:05 Epoch 1062/5000, D_loss=0.046140, reward=-7.771803\n",
      "      433983126 15:59:05 Epoch 1063/5000, D_loss=0.045341, reward=-7.769805\n",
      "      433983126 15:59:05 Epoch 1064/5000, D_loss=0.048043, reward=-7.760522\n",
      "      433983126 15:59:06 Epoch 1065/5000, D_loss=0.044459, reward=-7.774170\n",
      "      433983126 15:59:06 Epoch 1066/5000, D_loss=0.046639, reward=-7.769816\n",
      "      433983126 15:59:07 Epoch 1067/5000, D_loss=0.046578, reward=-7.765706\n",
      "      433983126 15:59:07 Epoch 1068/5000, D_loss=0.044472, reward=-7.774544\n",
      "      433983126 15:59:07 Epoch 1069/5000, D_loss=0.046400, reward=-7.768788\n",
      "      433983126 15:59:08 Epoch 1070/5000, D_loss=0.045955, reward=-7.772027\n",
      "      433983126 15:59:08 Epoch 1071/5000, D_loss=0.048016, reward=-7.767169\n",
      "      433983126 15:59:08 Epoch 1072/5000, D_loss=0.047240, reward=-7.767975\n",
      "      433983126 15:59:09 Epoch 1073/5000, D_loss=0.044937, reward=-7.761066\n",
      "      433983126 15:59:09 Epoch 1074/5000, D_loss=0.047284, reward=-7.764337\n",
      "      433983126 15:59:10 Epoch 1075/5000, D_loss=0.046142, reward=-7.759792\n",
      "      433983126 15:59:10 Epoch 1076/5000, D_loss=0.047786, reward=-7.756971\n",
      "      433983126 15:59:10 Epoch 1077/5000, D_loss=0.046433, reward=-7.764790\n",
      "      433983126 15:59:11 Epoch 1078/5000, D_loss=0.045455, reward=-7.770343\n",
      "      433983126 15:59:11 Epoch 1079/5000, D_loss=0.047053, reward=-7.760064\n",
      "      433983126 15:59:11 Epoch 1080/5000, D_loss=0.045705, reward=-7.765543\n",
      "      433983126 15:59:12 Epoch 1081/5000, D_loss=0.044658, reward=-7.767867\n",
      "      433983126 15:59:12 Epoch 1082/5000, D_loss=0.048306, reward=-7.759933\n",
      "      433983126 15:59:13 Epoch 1083/5000, D_loss=0.045542, reward=-7.760605\n",
      "      433983126 15:59:13 Epoch 1084/5000, D_loss=0.046227, reward=-7.761352\n",
      "      433983126 15:59:13 Epoch 1085/5000, D_loss=0.046506, reward=-7.763110\n",
      "      433983126 15:59:14 Epoch 1086/5000, D_loss=0.046273, reward=-7.759633\n",
      "      433983126 15:59:14 Epoch 1087/5000, D_loss=0.044246, reward=-7.763261\n",
      "      433983126 15:59:14 Epoch 1088/5000, D_loss=0.045315, reward=-7.766515\n",
      "      433983126 15:59:15 Epoch 1089/5000, D_loss=0.045529, reward=-7.759753\n",
      "      433983126 15:59:15 Epoch 1090/5000, D_loss=0.046963, reward=-7.762912\n",
      "      433983126 15:59:15 Epoch 1091/5000, D_loss=0.045733, reward=-7.759846\n",
      "      433983126 15:59:16 Epoch 1092/5000, D_loss=0.045219, reward=-7.758730\n",
      "      433983126 15:59:16 Epoch 1093/5000, D_loss=0.044112, reward=-7.768052\n",
      "      433983126 15:59:17 Epoch 1094/5000, D_loss=0.046618, reward=-7.766163\n",
      "      433983126 15:59:17 Epoch 1095/5000, D_loss=0.046426, reward=-7.755362\n",
      "      433983126 15:59:17 Epoch 1096/5000, D_loss=0.048328, reward=-7.757305\n",
      "      433983126 15:59:18 Epoch 1097/5000, D_loss=0.047556, reward=-7.757434\n",
      "      433983126 15:59:18 Epoch 1098/5000, D_loss=0.046158, reward=-7.761576\n",
      "      433983126 15:59:18 Epoch 1099/5000, D_loss=0.045655, reward=-7.762319\n",
      "      433983126 15:59:19 Epoch 1100/5000, D_loss=0.045949, reward=-7.759285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hhehe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      248942384 15:59:23 MR = 4651.92626953125\n",
      "MRR = 0.17738938331604004\n",
      "Hit@1 = 0.005932762030323006\n",
      "Hit@3 = 0.3269611074489123\n",
      "Hit@10 = 0.4332564271588662\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MR = 4651.92626953125\n",
      "MRR = 0.17738938331604004\n",
      "Hit@1 = 0.005932762030323006\n",
      "Hit@3 = 0.3269611074489123\n",
      "Hit@10 = 0.4332564271588662\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      433983126 15:59:23 Epoch 1101/5000, D_loss=0.045773, reward=-7.758100\n",
      "      433983126 15:59:23 Epoch 1102/5000, D_loss=0.046090, reward=-7.763723\n",
      "      433983126 15:59:24 Epoch 1103/5000, D_loss=0.044957, reward=-7.757276\n",
      "      433983126 15:59:24 Epoch 1104/5000, D_loss=0.047444, reward=-7.752429\n",
      "      433983126 15:59:25 Epoch 1105/5000, D_loss=0.046432, reward=-7.758199\n",
      "      433983126 15:59:25 Epoch 1106/5000, D_loss=0.045863, reward=-7.759752\n",
      "      433983126 15:59:25 Epoch 1107/5000, D_loss=0.045206, reward=-7.761202\n",
      "      433983126 15:59:26 Epoch 1108/5000, D_loss=0.044578, reward=-7.754630\n",
      "      433983126 15:59:26 Epoch 1109/5000, D_loss=0.046476, reward=-7.752301\n",
      "      433983126 15:59:26 Epoch 1110/5000, D_loss=0.045462, reward=-7.758770\n",
      "      433983126 15:59:27 Epoch 1111/5000, D_loss=0.045785, reward=-7.752745\n",
      "      433983126 15:59:27 Epoch 1112/5000, D_loss=0.045467, reward=-7.753915\n",
      "      433983126 15:59:28 Epoch 1113/5000, D_loss=0.047828, reward=-7.750113\n",
      "      433983126 15:59:28 Epoch 1114/5000, D_loss=0.044686, reward=-7.757078\n",
      "      433983126 15:59:28 Epoch 1115/5000, D_loss=0.044991, reward=-7.755581\n",
      "      433983126 15:59:29 Epoch 1116/5000, D_loss=0.045931, reward=-7.754229\n",
      "      433983126 15:59:29 Epoch 1117/5000, D_loss=0.045650, reward=-7.754986\n",
      "      433983126 15:59:29 Epoch 1118/5000, D_loss=0.046397, reward=-7.755733\n",
      "      433983126 15:59:30 Epoch 1119/5000, D_loss=0.045949, reward=-7.755403\n",
      "      433983126 15:59:30 Epoch 1120/5000, D_loss=0.044685, reward=-7.751595\n",
      "      433983126 15:59:30 Epoch 1121/5000, D_loss=0.048181, reward=-7.751542\n",
      "      433983126 15:59:31 Epoch 1122/5000, D_loss=0.046018, reward=-7.748579\n",
      "      433983126 15:59:31 Epoch 1123/5000, D_loss=0.047446, reward=-7.747692\n",
      "      433983126 15:59:32 Epoch 1124/5000, D_loss=0.045372, reward=-7.750144\n",
      "      433983126 15:59:32 Epoch 1125/5000, D_loss=0.047737, reward=-7.750155\n",
      "      433983126 15:59:32 Epoch 1126/5000, D_loss=0.047029, reward=-7.749876\n",
      "      433983126 15:59:33 Epoch 1127/5000, D_loss=0.047222, reward=-7.749151\n",
      "      433983126 15:59:33 Epoch 1128/5000, D_loss=0.046188, reward=-7.746691\n",
      "      433983126 15:59:33 Epoch 1129/5000, D_loss=0.045887, reward=-7.749559\n",
      "      433983126 15:59:34 Epoch 1130/5000, D_loss=0.047518, reward=-7.750801\n",
      "      433983126 15:59:34 Epoch 1131/5000, D_loss=0.046345, reward=-7.750320\n",
      "      433983126 15:59:35 Epoch 1132/5000, D_loss=0.044758, reward=-7.758508\n",
      "      433983126 15:59:35 Epoch 1133/5000, D_loss=0.045176, reward=-7.750415\n",
      "      433983126 15:59:35 Epoch 1134/5000, D_loss=0.047195, reward=-7.747970\n",
      "      433983126 15:59:36 Epoch 1135/5000, D_loss=0.046640, reward=-7.747222\n",
      "      433983126 15:59:36 Epoch 1136/5000, D_loss=0.046260, reward=-7.742597\n",
      "      433983126 15:59:36 Epoch 1137/5000, D_loss=0.045079, reward=-7.748800\n",
      "      433983126 15:59:37 Epoch 1138/5000, D_loss=0.045616, reward=-7.751259\n",
      "      433983126 15:59:37 Epoch 1139/5000, D_loss=0.045683, reward=-7.747957\n",
      "      433983126 15:59:37 Epoch 1140/5000, D_loss=0.045775, reward=-7.744490\n",
      "      433983126 15:59:38 Epoch 1141/5000, D_loss=0.046029, reward=-7.750400\n",
      "      433983126 15:59:38 Epoch 1142/5000, D_loss=0.046191, reward=-7.744137\n",
      "      433983126 15:59:39 Epoch 1143/5000, D_loss=0.044866, reward=-7.744999\n",
      "      433983126 15:59:39 Epoch 1144/5000, D_loss=0.047819, reward=-7.742240\n",
      "      433983126 15:59:39 Epoch 1145/5000, D_loss=0.045934, reward=-7.748614\n",
      "      433983126 15:59:40 Epoch 1146/5000, D_loss=0.046696, reward=-7.741792\n",
      "      433983126 15:59:40 Epoch 1147/5000, D_loss=0.046876, reward=-7.745349\n",
      "      433983126 15:59:40 Epoch 1148/5000, D_loss=0.047762, reward=-7.740663\n",
      "      433983126 15:59:41 Epoch 1149/5000, D_loss=0.047230, reward=-7.743774\n",
      "      433983126 15:59:41 Epoch 1150/5000, D_loss=0.047342, reward=-7.739641\n",
      "      433983126 15:59:41 Epoch 1151/5000, D_loss=0.045230, reward=-7.740840\n",
      "      433983126 15:59:42 Epoch 1152/5000, D_loss=0.045454, reward=-7.742376\n",
      "      433983126 15:59:42 Epoch 1153/5000, D_loss=0.047487, reward=-7.748223\n",
      "      433983126 15:59:43 Epoch 1154/5000, D_loss=0.045989, reward=-7.739958\n",
      "      433983126 15:59:43 Epoch 1155/5000, D_loss=0.047603, reward=-7.739741\n",
      "      433983126 15:59:43 Epoch 1156/5000, D_loss=0.046180, reward=-7.745116\n",
      "      433983126 15:59:44 Epoch 1157/5000, D_loss=0.047310, reward=-7.743122\n",
      "      433983126 15:59:44 Epoch 1158/5000, D_loss=0.047094, reward=-7.740578\n",
      "      433983126 15:59:44 Epoch 1159/5000, D_loss=0.045790, reward=-7.742723\n",
      "      433983126 15:59:45 Epoch 1160/5000, D_loss=0.046266, reward=-7.737873\n",
      "      433983126 15:59:45 Epoch 1161/5000, D_loss=0.046327, reward=-7.739357\n",
      "      433983126 15:59:46 Epoch 1162/5000, D_loss=0.046471, reward=-7.739411\n",
      "      433983126 15:59:46 Epoch 1163/5000, D_loss=0.047757, reward=-7.735262\n",
      "      433983126 15:59:46 Epoch 1164/5000, D_loss=0.047740, reward=-7.734119\n",
      "      433983126 15:59:47 Epoch 1165/5000, D_loss=0.046607, reward=-7.741139\n",
      "      433983126 15:59:47 Epoch 1166/5000, D_loss=0.047415, reward=-7.739687\n",
      "      433983126 15:59:47 Epoch 1167/5000, D_loss=0.045048, reward=-7.740861\n",
      "      433983126 15:59:48 Epoch 1168/5000, D_loss=0.046418, reward=-7.741660\n",
      "      433983126 15:59:48 Epoch 1169/5000, D_loss=0.046110, reward=-7.729515\n",
      "      433983126 15:59:48 Epoch 1170/5000, D_loss=0.047503, reward=-7.738799\n",
      "      433983126 15:59:49 Epoch 1171/5000, D_loss=0.046027, reward=-7.740401\n",
      "      433983126 15:59:49 Epoch 1172/5000, D_loss=0.045103, reward=-7.738437\n",
      "      433983126 15:59:50 Epoch 1173/5000, D_loss=0.046238, reward=-7.739034\n",
      "      433983126 15:59:50 Epoch 1174/5000, D_loss=0.046219, reward=-7.743556\n",
      "      433983126 15:59:50 Epoch 1175/5000, D_loss=0.047354, reward=-7.735580\n",
      "      433983126 15:59:51 Epoch 1176/5000, D_loss=0.046283, reward=-7.741865\n",
      "      433983126 15:59:51 Epoch 1177/5000, D_loss=0.046341, reward=-7.737283\n",
      "      433983126 15:59:51 Epoch 1178/5000, D_loss=0.047331, reward=-7.735455\n",
      "      433983126 15:59:52 Epoch 1179/5000, D_loss=0.048261, reward=-7.733764\n",
      "      433983126 15:59:52 Epoch 1180/5000, D_loss=0.046387, reward=-7.734591\n",
      "      433983126 15:59:53 Epoch 1181/5000, D_loss=0.045406, reward=-7.742856\n",
      "      433983126 15:59:53 Epoch 1182/5000, D_loss=0.046483, reward=-7.730439\n",
      "      433983126 15:59:53 Epoch 1183/5000, D_loss=0.048353, reward=-7.731657\n",
      "      433983126 15:59:54 Epoch 1184/5000, D_loss=0.048023, reward=-7.733883\n",
      "      433983126 15:59:54 Epoch 1185/5000, D_loss=0.046644, reward=-7.737153\n",
      "      433983126 15:59:54 Epoch 1186/5000, D_loss=0.047806, reward=-7.731153\n",
      "      433983126 15:59:55 Epoch 1187/5000, D_loss=0.047157, reward=-7.731959\n",
      "      433983126 15:59:55 Epoch 1188/5000, D_loss=0.047757, reward=-7.729118\n",
      "      433983126 15:59:55 Epoch 1189/5000, D_loss=0.046913, reward=-7.730292\n",
      "      433983126 15:59:56 Epoch 1190/5000, D_loss=0.046003, reward=-7.740909\n",
      "      433983126 15:59:56 Epoch 1191/5000, D_loss=0.049981, reward=-7.734035\n",
      "      433983126 15:59:57 Epoch 1192/5000, D_loss=0.047884, reward=-7.728438\n",
      "      433983126 15:59:57 Epoch 1193/5000, D_loss=0.048193, reward=-7.729250\n",
      "      433983126 15:59:57 Epoch 1194/5000, D_loss=0.047055, reward=-7.735712\n",
      "      433983126 15:59:58 Epoch 1195/5000, D_loss=0.045728, reward=-7.736450\n",
      "      433983126 15:59:58 Epoch 1196/5000, D_loss=0.046802, reward=-7.731284\n",
      "      433983126 15:59:58 Epoch 1197/5000, D_loss=0.047686, reward=-7.732419\n",
      "      433983126 15:59:59 Epoch 1198/5000, D_loss=0.044428, reward=-7.734327\n",
      "      433983126 15:59:59 Epoch 1199/5000, D_loss=0.047895, reward=-7.733184\n",
      "      433983126 15:59:59 Epoch 1200/5000, D_loss=0.047619, reward=-7.729455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hhehe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      248942384 16:00:03 MR = 4628.58837890625\n",
      "MRR = 0.1757526844739914\n",
      "Hit@1 = 0.005108767303889255\n",
      "Hit@3 = 0.3231707317073171\n",
      "Hit@10 = 0.4312788398154252\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MR = 4628.58837890625\n",
      "MRR = 0.1757526844739914\n",
      "Hit@1 = 0.005108767303889255\n",
      "Hit@3 = 0.3231707317073171\n",
      "Hit@10 = 0.4312788398154252\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      433983126 16:00:04 Epoch 1201/5000, D_loss=0.045141, reward=-7.737478\n",
      "      433983126 16:00:04 Epoch 1202/5000, D_loss=0.046819, reward=-7.731623\n",
      "      433983126 16:00:04 Epoch 1203/5000, D_loss=0.047127, reward=-7.730025\n",
      "      433983126 16:00:05 Epoch 1204/5000, D_loss=0.046370, reward=-7.730460\n",
      "      433983126 16:00:05 Epoch 1205/5000, D_loss=0.047656, reward=-7.726569\n",
      "      433983126 16:00:06 Epoch 1206/5000, D_loss=0.048688, reward=-7.726820\n",
      "      433983126 16:00:06 Epoch 1207/5000, D_loss=0.046540, reward=-7.728196\n",
      "      433983126 16:00:06 Epoch 1208/5000, D_loss=0.047504, reward=-7.726711\n",
      "      433983126 16:00:07 Epoch 1209/5000, D_loss=0.046738, reward=-7.727208\n",
      "      433983126 16:00:07 Epoch 1210/5000, D_loss=0.047708, reward=-7.721283\n",
      "      433983126 16:00:07 Epoch 1211/5000, D_loss=0.048069, reward=-7.720345\n",
      "      433983126 16:00:08 Epoch 1212/5000, D_loss=0.047706, reward=-7.723946\n",
      "      433983126 16:00:08 Epoch 1213/5000, D_loss=0.047445, reward=-7.728153\n",
      "      433983126 16:00:08 Epoch 1214/5000, D_loss=0.048745, reward=-7.729404\n",
      "      433983126 16:00:09 Epoch 1215/5000, D_loss=0.047761, reward=-7.720230\n",
      "      433983126 16:00:09 Epoch 1216/5000, D_loss=0.046812, reward=-7.726068\n",
      "      433983126 16:00:10 Epoch 1217/5000, D_loss=0.048709, reward=-7.728086\n",
      "      433983126 16:00:10 Epoch 1218/5000, D_loss=0.048721, reward=-7.723547\n",
      "      433983126 16:00:10 Epoch 1219/5000, D_loss=0.047216, reward=-7.724312\n",
      "      433983126 16:00:11 Epoch 1220/5000, D_loss=0.047326, reward=-7.719022\n",
      "      433983126 16:00:11 Epoch 1221/5000, D_loss=0.048401, reward=-7.722504\n",
      "      433983126 16:00:11 Epoch 1222/5000, D_loss=0.046033, reward=-7.726616\n",
      "      433983126 16:00:12 Epoch 1223/5000, D_loss=0.047801, reward=-7.718360\n",
      "      433983126 16:00:12 Epoch 1224/5000, D_loss=0.048650, reward=-7.721504\n",
      "      433983126 16:00:13 Epoch 1225/5000, D_loss=0.044794, reward=-7.730955\n",
      "      433983126 16:00:13 Epoch 1226/5000, D_loss=0.043790, reward=-7.725683\n",
      "      433983126 16:00:13 Epoch 1227/5000, D_loss=0.048409, reward=-7.719452\n",
      "      433983126 16:00:14 Epoch 1228/5000, D_loss=0.046356, reward=-7.719738\n",
      "      433983126 16:00:14 Epoch 1229/5000, D_loss=0.044812, reward=-7.723054\n",
      "      433983126 16:00:14 Epoch 1230/5000, D_loss=0.047687, reward=-7.720669\n",
      "      433983126 16:00:15 Epoch 1231/5000, D_loss=0.048896, reward=-7.715863\n",
      "      433983126 16:00:15 Epoch 1232/5000, D_loss=0.046714, reward=-7.717819\n",
      "      433983126 16:00:15 Epoch 1233/5000, D_loss=0.047324, reward=-7.719026\n",
      "      433983126 16:00:16 Epoch 1234/5000, D_loss=0.049066, reward=-7.717207\n",
      "      433983126 16:00:16 Epoch 1235/5000, D_loss=0.045772, reward=-7.720661\n",
      "      433983126 16:00:17 Epoch 1236/5000, D_loss=0.047850, reward=-7.716839\n",
      "      433983126 16:00:17 Epoch 1237/5000, D_loss=0.048072, reward=-7.719213\n",
      "      433983126 16:00:17 Epoch 1238/5000, D_loss=0.049334, reward=-7.715221\n",
      "      433983126 16:00:18 Epoch 1239/5000, D_loss=0.046052, reward=-7.716818\n",
      "      433983126 16:00:18 Epoch 1240/5000, D_loss=0.048655, reward=-7.712736\n",
      "      433983126 16:00:18 Epoch 1241/5000, D_loss=0.048849, reward=-7.718349\n",
      "      433983126 16:00:19 Epoch 1242/5000, D_loss=0.047604, reward=-7.717105\n",
      "      433983126 16:00:19 Epoch 1243/5000, D_loss=0.047137, reward=-7.714224\n",
      "      433983126 16:00:20 Epoch 1244/5000, D_loss=0.047348, reward=-7.715888\n",
      "      433983126 16:00:20 Epoch 1245/5000, D_loss=0.047086, reward=-7.709527\n",
      "      433983126 16:00:20 Epoch 1246/5000, D_loss=0.046906, reward=-7.714024\n",
      "      433983126 16:00:21 Epoch 1247/5000, D_loss=0.048374, reward=-7.711402\n",
      "      433983126 16:00:21 Epoch 1248/5000, D_loss=0.047633, reward=-7.716024\n",
      "      433983126 16:00:21 Epoch 1249/5000, D_loss=0.047825, reward=-7.711434\n",
      "      433983126 16:00:22 Epoch 1250/5000, D_loss=0.048856, reward=-7.715736\n",
      "      433983126 16:00:22 Epoch 1251/5000, D_loss=0.047253, reward=-7.718331\n",
      "      433983126 16:00:23 Epoch 1252/5000, D_loss=0.048697, reward=-7.709302\n",
      "      433983126 16:00:23 Epoch 1253/5000, D_loss=0.047331, reward=-7.710552\n",
      "      433983126 16:00:23 Epoch 1254/5000, D_loss=0.048233, reward=-7.710172\n",
      "      433983126 16:00:24 Epoch 1255/5000, D_loss=0.046580, reward=-7.713039\n",
      "      433983126 16:00:24 Epoch 1256/5000, D_loss=0.046485, reward=-7.717078\n",
      "      433983126 16:00:24 Epoch 1257/5000, D_loss=0.046691, reward=-7.715511\n",
      "      433983126 16:00:25 Epoch 1258/5000, D_loss=0.046241, reward=-7.717660\n",
      "      433983126 16:00:25 Epoch 1259/5000, D_loss=0.048337, reward=-7.716104\n",
      "      433983126 16:00:26 Epoch 1260/5000, D_loss=0.047862, reward=-7.715374\n",
      "      433983126 16:00:26 Epoch 1261/5000, D_loss=0.046820, reward=-7.717667\n",
      "      433983126 16:00:26 Epoch 1262/5000, D_loss=0.046902, reward=-7.710422\n",
      "      433983126 16:00:27 Epoch 1263/5000, D_loss=0.047357, reward=-7.713780\n",
      "      433983126 16:00:27 Epoch 1264/5000, D_loss=0.047089, reward=-7.706006\n",
      "      433983126 16:00:27 Epoch 1265/5000, D_loss=0.046192, reward=-7.709431\n",
      "      433983126 16:00:28 Epoch 1266/5000, D_loss=0.048427, reward=-7.711035\n",
      "      433983126 16:00:28 Epoch 1267/5000, D_loss=0.046338, reward=-7.712955\n",
      "      433983126 16:00:29 Epoch 1268/5000, D_loss=0.048330, reward=-7.707708\n",
      "      433983126 16:00:29 Epoch 1269/5000, D_loss=0.049863, reward=-7.704468\n",
      "      433983126 16:00:29 Epoch 1270/5000, D_loss=0.045898, reward=-7.711846\n",
      "      433983126 16:00:30 Epoch 1271/5000, D_loss=0.047333, reward=-7.709651\n",
      "      433983126 16:00:30 Epoch 1272/5000, D_loss=0.046835, reward=-7.712802\n",
      "      433983126 16:00:30 Epoch 1273/5000, D_loss=0.047942, reward=-7.710285\n",
      "      433983126 16:00:31 Epoch 1274/5000, D_loss=0.047640, reward=-7.712408\n",
      "      433983126 16:00:31 Epoch 1275/5000, D_loss=0.047287, reward=-7.706186\n",
      "      433983126 16:00:32 Epoch 1276/5000, D_loss=0.047528, reward=-7.703159\n",
      "      433983126 16:00:32 Epoch 1277/5000, D_loss=0.046486, reward=-7.709892\n",
      "      433983126 16:00:32 Epoch 1278/5000, D_loss=0.047913, reward=-7.706836\n",
      "      433983126 16:00:33 Epoch 1279/5000, D_loss=0.046263, reward=-7.706365\n",
      "      433983126 16:00:33 Epoch 1280/5000, D_loss=0.047124, reward=-7.707236\n",
      "      433983126 16:00:33 Epoch 1281/5000, D_loss=0.046232, reward=-7.711307\n",
      "      433983126 16:00:34 Epoch 1282/5000, D_loss=0.047580, reward=-7.714430\n",
      "      433983126 16:00:34 Epoch 1283/5000, D_loss=0.048083, reward=-7.701380\n",
      "      433983126 16:00:35 Epoch 1284/5000, D_loss=0.048488, reward=-7.703324\n",
      "      433983126 16:00:35 Epoch 1285/5000, D_loss=0.046577, reward=-7.707449\n",
      "      433983126 16:00:35 Epoch 1286/5000, D_loss=0.048877, reward=-7.704701\n",
      "      433983126 16:00:36 Epoch 1287/5000, D_loss=0.047803, reward=-7.705529\n",
      "      433983126 16:00:36 Epoch 1288/5000, D_loss=0.047977, reward=-7.703879\n",
      "      433983126 16:00:36 Epoch 1289/5000, D_loss=0.048240, reward=-7.706035\n",
      "      433983126 16:00:37 Epoch 1290/5000, D_loss=0.047584, reward=-7.701849\n",
      "      433983126 16:00:37 Epoch 1291/5000, D_loss=0.046836, reward=-7.705693\n",
      "      433983126 16:00:38 Epoch 1292/5000, D_loss=0.045481, reward=-7.709178\n",
      "      433983126 16:00:38 Epoch 1293/5000, D_loss=0.046644, reward=-7.705667\n",
      "      433983126 16:00:38 Epoch 1294/5000, D_loss=0.048694, reward=-7.702572\n",
      "      433983126 16:00:39 Epoch 1295/5000, D_loss=0.047383, reward=-7.706531\n",
      "      433983126 16:00:39 Epoch 1296/5000, D_loss=0.048294, reward=-7.705344\n",
      "      433983126 16:00:39 Epoch 1297/5000, D_loss=0.048601, reward=-7.706253\n",
      "      433983126 16:00:40 Epoch 1298/5000, D_loss=0.047256, reward=-7.699636\n",
      "      433983126 16:00:40 Epoch 1299/5000, D_loss=0.047054, reward=-7.700101\n",
      "      433983126 16:00:41 Epoch 1300/5000, D_loss=0.049897, reward=-7.695323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hhehe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      248942384 16:00:45 MR = 4626.69873046875\n",
      "MRR = 0.17784765362739563\n",
      "Hit@1 = 0.005767963085036256\n",
      "Hit@3 = 0.32679630850362557\n",
      "Hit@10 = 0.43243243243243246\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MR = 4626.69873046875\n",
      "MRR = 0.17784765362739563\n",
      "Hit@1 = 0.005767963085036256\n",
      "Hit@3 = 0.32679630850362557\n",
      "Hit@10 = 0.43243243243243246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      433983126 16:00:45 Epoch 1301/5000, D_loss=0.047460, reward=-7.703501\n",
      "      433983126 16:00:45 Epoch 1302/5000, D_loss=0.047297, reward=-7.698467\n",
      "      433983126 16:00:46 Epoch 1303/5000, D_loss=0.047584, reward=-7.697904\n",
      "      433983126 16:00:46 Epoch 1304/5000, D_loss=0.047846, reward=-7.699614\n",
      "      433983126 16:00:46 Epoch 1305/5000, D_loss=0.048169, reward=-7.695851\n",
      "      433983126 16:00:47 Epoch 1306/5000, D_loss=0.047025, reward=-7.700907\n",
      "      433983126 16:00:47 Epoch 1307/5000, D_loss=0.047089, reward=-7.707117\n",
      "      433983126 16:00:48 Epoch 1308/5000, D_loss=0.048719, reward=-7.703499\n",
      "      433983126 16:00:48 Epoch 1309/5000, D_loss=0.048462, reward=-7.699844\n",
      "      433983126 16:00:48 Epoch 1310/5000, D_loss=0.046847, reward=-7.699240\n",
      "      433983126 16:00:49 Epoch 1311/5000, D_loss=0.047271, reward=-7.699429\n",
      "      433983126 16:00:49 Epoch 1312/5000, D_loss=0.048194, reward=-7.697919\n",
      "      433983126 16:00:49 Epoch 1313/5000, D_loss=0.049502, reward=-7.695900\n",
      "      433983126 16:00:50 Epoch 1314/5000, D_loss=0.046579, reward=-7.704482\n",
      "      433983126 16:00:50 Epoch 1315/5000, D_loss=0.048380, reward=-7.695477\n",
      "      433983126 16:00:51 Epoch 1316/5000, D_loss=0.048122, reward=-7.698008\n",
      "      433983126 16:00:51 Epoch 1317/5000, D_loss=0.047223, reward=-7.698873\n",
      "      433983126 16:00:51 Epoch 1318/5000, D_loss=0.048500, reward=-7.696572\n",
      "      433983126 16:00:52 Epoch 1319/5000, D_loss=0.047250, reward=-7.690525\n",
      "      433983126 16:00:52 Epoch 1320/5000, D_loss=0.048440, reward=-7.694992\n",
      "      433983126 16:00:52 Epoch 1321/5000, D_loss=0.047686, reward=-7.705143\n",
      "      433983126 16:00:53 Epoch 1322/5000, D_loss=0.047482, reward=-7.695055\n",
      "      433983126 16:00:53 Epoch 1323/5000, D_loss=0.046284, reward=-7.696915\n",
      "      433983126 16:00:54 Epoch 1324/5000, D_loss=0.047727, reward=-7.691160\n",
      "      433983126 16:00:54 Epoch 1325/5000, D_loss=0.049442, reward=-7.692829\n",
      "      433983126 16:00:54 Epoch 1326/5000, D_loss=0.046015, reward=-7.699281\n",
      "      433983126 16:00:55 Epoch 1327/5000, D_loss=0.047570, reward=-7.694304\n",
      "      433983126 16:00:55 Epoch 1328/5000, D_loss=0.046774, reward=-7.689150\n",
      "      433983126 16:00:55 Epoch 1329/5000, D_loss=0.047939, reward=-7.693390\n",
      "      433983126 16:00:56 Epoch 1330/5000, D_loss=0.048837, reward=-7.692959\n",
      "      433983126 16:00:56 Epoch 1331/5000, D_loss=0.048499, reward=-7.688139\n",
      "      433983126 16:00:57 Epoch 1332/5000, D_loss=0.048222, reward=-7.690853\n",
      "      433983126 16:00:57 Epoch 1333/5000, D_loss=0.048133, reward=-7.692503\n",
      "      433983126 16:00:57 Epoch 1334/5000, D_loss=0.049141, reward=-7.690811\n",
      "      433983126 16:00:58 Epoch 1335/5000, D_loss=0.049607, reward=-7.683284\n",
      "      433983126 16:00:58 Epoch 1336/5000, D_loss=0.048812, reward=-7.684609\n",
      "      433983126 16:00:59 Epoch 1337/5000, D_loss=0.047544, reward=-7.690572\n",
      "      433983126 16:00:59 Epoch 1338/5000, D_loss=0.047566, reward=-7.693531\n",
      "      433983126 16:00:59 Epoch 1339/5000, D_loss=0.048631, reward=-7.689530\n",
      "      433983126 16:01:00 Epoch 1340/5000, D_loss=0.048107, reward=-7.687873\n",
      "      433983126 16:01:00 Epoch 1341/5000, D_loss=0.049399, reward=-7.689139\n",
      "      433983126 16:01:00 Epoch 1342/5000, D_loss=0.049681, reward=-7.683178\n",
      "      433983126 16:01:01 Epoch 1343/5000, D_loss=0.047567, reward=-7.690142\n",
      "      433983126 16:01:01 Epoch 1344/5000, D_loss=0.047677, reward=-7.692209\n",
      "      433983126 16:01:02 Epoch 1345/5000, D_loss=0.048225, reward=-7.687470\n",
      "      433983126 16:01:02 Epoch 1346/5000, D_loss=0.049198, reward=-7.684240\n",
      "      433983126 16:01:02 Epoch 1347/5000, D_loss=0.048726, reward=-7.681636\n",
      "      433983126 16:01:03 Epoch 1348/5000, D_loss=0.048166, reward=-7.687414\n",
      "      433983126 16:01:03 Epoch 1349/5000, D_loss=0.048715, reward=-7.685758\n",
      "      433983126 16:01:03 Epoch 1350/5000, D_loss=0.049306, reward=-7.687216\n",
      "      433983126 16:01:04 Epoch 1351/5000, D_loss=0.050344, reward=-7.678876\n",
      "      433983126 16:01:04 Epoch 1352/5000, D_loss=0.048162, reward=-7.685514\n",
      "      433983126 16:01:05 Epoch 1353/5000, D_loss=0.048503, reward=-7.682920\n",
      "      433983126 16:01:05 Epoch 1354/5000, D_loss=0.049115, reward=-7.682914\n",
      "      433983126 16:01:05 Epoch 1355/5000, D_loss=0.048542, reward=-7.685028\n",
      "      433983126 16:01:06 Epoch 1356/5000, D_loss=0.046619, reward=-7.688629\n",
      "      433983126 16:01:06 Epoch 1357/5000, D_loss=0.048103, reward=-7.686798\n",
      "      433983126 16:01:06 Epoch 1358/5000, D_loss=0.047743, reward=-7.685256\n",
      "      433983126 16:01:07 Epoch 1359/5000, D_loss=0.048362, reward=-7.685579\n",
      "      433983126 16:01:07 Epoch 1360/5000, D_loss=0.048263, reward=-7.679663\n",
      "      433983126 16:01:08 Epoch 1361/5000, D_loss=0.049490, reward=-7.684748\n",
      "      433983126 16:01:08 Epoch 1362/5000, D_loss=0.049529, reward=-7.679058\n",
      "      433983126 16:01:08 Epoch 1363/5000, D_loss=0.048004, reward=-7.683526\n",
      "      433983126 16:01:09 Epoch 1364/5000, D_loss=0.048788, reward=-7.679772\n",
      "      433983126 16:01:09 Epoch 1365/5000, D_loss=0.049075, reward=-7.685522\n",
      "      433983126 16:01:09 Epoch 1366/5000, D_loss=0.048226, reward=-7.681360\n",
      "      433983126 16:01:10 Epoch 1367/5000, D_loss=0.048382, reward=-7.686521\n",
      "      433983126 16:01:10 Epoch 1368/5000, D_loss=0.048793, reward=-7.680660\n",
      "      433983126 16:01:11 Epoch 1369/5000, D_loss=0.048484, reward=-7.680837\n",
      "      433983126 16:01:11 Epoch 1370/5000, D_loss=0.048597, reward=-7.681422\n",
      "      433983126 16:01:11 Epoch 1371/5000, D_loss=0.049027, reward=-7.684079\n",
      "      433983126 16:01:12 Epoch 1372/5000, D_loss=0.048334, reward=-7.682544\n",
      "      433983126 16:01:12 Epoch 1373/5000, D_loss=0.050345, reward=-7.682504\n",
      "      433983126 16:01:12 Epoch 1374/5000, D_loss=0.048367, reward=-7.678619\n",
      "      433983126 16:01:13 Epoch 1375/5000, D_loss=0.046770, reward=-7.680495\n",
      "      433983126 16:01:13 Epoch 1376/5000, D_loss=0.048518, reward=-7.682165\n",
      "      433983126 16:01:14 Epoch 1377/5000, D_loss=0.047706, reward=-7.679306\n",
      "      433983126 16:01:14 Epoch 1378/5000, D_loss=0.048646, reward=-7.681090\n",
      "      433983126 16:01:14 Epoch 1379/5000, D_loss=0.048939, reward=-7.680546\n",
      "      433983126 16:01:15 Epoch 1380/5000, D_loss=0.049103, reward=-7.676847\n",
      "      433983126 16:01:15 Epoch 1381/5000, D_loss=0.049291, reward=-7.674685\n",
      "      433983126 16:01:15 Epoch 1382/5000, D_loss=0.046768, reward=-7.678247\n",
      "      433983126 16:01:16 Epoch 1383/5000, D_loss=0.047754, reward=-7.681628\n",
      "      433983126 16:01:16 Epoch 1384/5000, D_loss=0.048157, reward=-7.673065\n",
      "      433983126 16:01:17 Epoch 1385/5000, D_loss=0.047358, reward=-7.675653\n",
      "      433983126 16:01:17 Epoch 1386/5000, D_loss=0.048941, reward=-7.674570\n",
      "      433983126 16:01:17 Epoch 1387/5000, D_loss=0.048541, reward=-7.671435\n",
      "      433983126 16:01:18 Epoch 1388/5000, D_loss=0.049294, reward=-7.666037\n",
      "      433983126 16:01:18 Epoch 1389/5000, D_loss=0.047756, reward=-7.678648\n",
      "      433983126 16:01:19 Epoch 1390/5000, D_loss=0.049079, reward=-7.679958\n",
      "      433983126 16:01:19 Epoch 1391/5000, D_loss=0.046994, reward=-7.681897\n",
      "      433983126 16:01:19 Epoch 1392/5000, D_loss=0.048819, reward=-7.679559\n",
      "      433983126 16:01:20 Epoch 1393/5000, D_loss=0.049169, reward=-7.677654\n",
      "      433983126 16:01:20 Epoch 1394/5000, D_loss=0.048459, reward=-7.669868\n",
      "      433983126 16:01:21 Epoch 1395/5000, D_loss=0.050764, reward=-7.670062\n",
      "      433983126 16:01:21 Epoch 1396/5000, D_loss=0.047965, reward=-7.674119\n",
      "      433983126 16:01:21 Epoch 1397/5000, D_loss=0.049260, reward=-7.675896\n",
      "      433983126 16:01:22 Epoch 1398/5000, D_loss=0.048635, reward=-7.670718\n",
      "      433983126 16:01:22 Epoch 1399/5000, D_loss=0.049497, reward=-7.669870\n",
      "      433983126 16:01:22 Epoch 1400/5000, D_loss=0.049138, reward=-7.670702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hhehe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      248942384 16:01:26 MR = 4626.4912109375\n",
      "MRR = 0.17746135592460632\n",
      "Hit@1 = 0.005108767303889255\n",
      "Hit@3 = 0.32778510217534607\n",
      "Hit@10 = 0.43441001977587346\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MR = 4626.4912109375\n",
      "MRR = 0.17746135592460632\n",
      "Hit@1 = 0.005108767303889255\n",
      "Hit@3 = 0.32778510217534607\n",
      "Hit@10 = 0.43441001977587346\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      433983126 16:01:27 Epoch 1401/5000, D_loss=0.049672, reward=-7.666975\n",
      "      433983126 16:01:27 Epoch 1402/5000, D_loss=0.048225, reward=-7.671791\n",
      "      433983126 16:01:27 Epoch 1403/5000, D_loss=0.047758, reward=-7.676845\n",
      "      433983126 16:01:28 Epoch 1404/5000, D_loss=0.047675, reward=-7.671540\n",
      "      433983126 16:01:28 Epoch 1405/5000, D_loss=0.047337, reward=-7.673842\n",
      "      433983126 16:01:29 Epoch 1406/5000, D_loss=0.048340, reward=-7.669434\n",
      "      433983126 16:01:29 Epoch 1407/5000, D_loss=0.049533, reward=-7.671974\n",
      "      433983126 16:01:29 Epoch 1408/5000, D_loss=0.049535, reward=-7.673113\n",
      "      433983126 16:01:30 Epoch 1409/5000, D_loss=0.047255, reward=-7.669598\n",
      "      433983126 16:01:30 Epoch 1410/5000, D_loss=0.049078, reward=-7.667931\n",
      "      433983126 16:01:30 Epoch 1411/5000, D_loss=0.049930, reward=-7.673573\n",
      "      433983126 16:01:31 Epoch 1412/5000, D_loss=0.048409, reward=-7.673744\n",
      "      433983126 16:01:31 Epoch 1413/5000, D_loss=0.050835, reward=-7.671271\n",
      "      433983126 16:01:32 Epoch 1414/5000, D_loss=0.050886, reward=-7.671203\n",
      "      433983126 16:01:32 Epoch 1415/5000, D_loss=0.048978, reward=-7.663085\n",
      "      433983126 16:01:32 Epoch 1416/5000, D_loss=0.048007, reward=-7.672802\n",
      "      433983126 16:01:33 Epoch 1417/5000, D_loss=0.049292, reward=-7.670400\n",
      "      433983126 16:01:33 Epoch 1418/5000, D_loss=0.048401, reward=-7.672110\n",
      "      433983126 16:01:33 Epoch 1419/5000, D_loss=0.049916, reward=-7.666477\n",
      "      433983126 16:01:34 Epoch 1420/5000, D_loss=0.048289, reward=-7.667309\n",
      "      433983126 16:01:34 Epoch 1421/5000, D_loss=0.048808, reward=-7.673954\n",
      "      433983126 16:01:35 Epoch 1422/5000, D_loss=0.049350, reward=-7.664563\n",
      "      433983126 16:01:35 Epoch 1423/5000, D_loss=0.048629, reward=-7.663231\n",
      "      433983126 16:01:35 Epoch 1424/5000, D_loss=0.047749, reward=-7.669150\n",
      "      433983126 16:01:36 Epoch 1425/5000, D_loss=0.048295, reward=-7.668835\n",
      "      433983126 16:01:36 Epoch 1426/5000, D_loss=0.048367, reward=-7.675063\n",
      "      433983126 16:01:36 Epoch 1427/5000, D_loss=0.045891, reward=-7.671647\n",
      "      433983126 16:01:37 Epoch 1428/5000, D_loss=0.047210, reward=-7.672180\n",
      "      433983126 16:01:37 Epoch 1429/5000, D_loss=0.048447, reward=-7.670390\n",
      "      433983126 16:01:38 Epoch 1430/5000, D_loss=0.050990, reward=-7.664348\n",
      "      433983126 16:01:38 Epoch 1431/5000, D_loss=0.048081, reward=-7.663671\n",
      "      433983126 16:01:38 Epoch 1432/5000, D_loss=0.049766, reward=-7.666375\n",
      "      433983126 16:01:39 Epoch 1433/5000, D_loss=0.048708, reward=-7.668580\n",
      "      433983126 16:01:39 Epoch 1434/5000, D_loss=0.048508, reward=-7.664891\n",
      "      433983126 16:01:39 Epoch 1435/5000, D_loss=0.048663, reward=-7.666499\n",
      "      433983126 16:01:40 Epoch 1436/5000, D_loss=0.048104, reward=-7.662480\n",
      "      433983126 16:01:40 Epoch 1437/5000, D_loss=0.048533, reward=-7.662560\n",
      "      433983126 16:01:41 Epoch 1438/5000, D_loss=0.047192, reward=-7.666228\n",
      "      433983126 16:01:41 Epoch 1439/5000, D_loss=0.048699, reward=-7.659893\n",
      "      433983126 16:01:41 Epoch 1440/5000, D_loss=0.049522, reward=-7.662437\n",
      "      433983126 16:01:42 Epoch 1441/5000, D_loss=0.047315, reward=-7.661038\n",
      "      433983126 16:01:42 Epoch 1442/5000, D_loss=0.049112, reward=-7.660564\n",
      "      433983126 16:01:42 Epoch 1443/5000, D_loss=0.048253, reward=-7.663547\n",
      "      433983126 16:01:43 Epoch 1444/5000, D_loss=0.049339, reward=-7.657578\n",
      "      433983126 16:01:43 Epoch 1445/5000, D_loss=0.049427, reward=-7.665946\n",
      "      433983126 16:01:44 Epoch 1446/5000, D_loss=0.049475, reward=-7.656674\n",
      "      433983126 16:01:44 Epoch 1447/5000, D_loss=0.048717, reward=-7.658051\n",
      "      433983126 16:01:44 Epoch 1448/5000, D_loss=0.049214, reward=-7.656499\n",
      "      433983126 16:01:45 Epoch 1449/5000, D_loss=0.048586, reward=-7.662350\n",
      "      433983126 16:01:45 Epoch 1450/5000, D_loss=0.048660, reward=-7.662177\n",
      "      433983126 16:01:45 Epoch 1451/5000, D_loss=0.048905, reward=-7.653880\n",
      "      433983126 16:01:46 Epoch 1452/5000, D_loss=0.048772, reward=-7.660289\n",
      "      433983126 16:01:46 Epoch 1453/5000, D_loss=0.048404, reward=-7.658702\n",
      "      433983126 16:01:47 Epoch 1454/5000, D_loss=0.046732, reward=-7.664762\n",
      "      433983126 16:01:47 Epoch 1455/5000, D_loss=0.049287, reward=-7.658010\n",
      "      433983126 16:01:47 Epoch 1456/5000, D_loss=0.049839, reward=-7.659317\n",
      "      433983126 16:01:48 Epoch 1457/5000, D_loss=0.050660, reward=-7.662586\n",
      "      433983126 16:01:48 Epoch 1458/5000, D_loss=0.047937, reward=-7.661386\n",
      "      433983126 16:01:48 Epoch 1459/5000, D_loss=0.047602, reward=-7.659555\n",
      "      433983126 16:01:49 Epoch 1460/5000, D_loss=0.048483, reward=-7.656459\n",
      "      433983126 16:01:49 Epoch 1461/5000, D_loss=0.047654, reward=-7.660844\n",
      "      433983126 16:01:50 Epoch 1462/5000, D_loss=0.048528, reward=-7.659040\n",
      "      433983126 16:01:50 Epoch 1463/5000, D_loss=0.049223, reward=-7.657093\n",
      "      433983126 16:01:50 Epoch 1464/5000, D_loss=0.048403, reward=-7.656139\n",
      "      433983126 16:01:51 Epoch 1465/5000, D_loss=0.048800, reward=-7.660316\n",
      "      433983126 16:01:51 Epoch 1466/5000, D_loss=0.049731, reward=-7.654809\n",
      "      433983126 16:01:51 Epoch 1467/5000, D_loss=0.050120, reward=-7.657806\n",
      "      433983126 16:01:52 Epoch 1468/5000, D_loss=0.049684, reward=-7.653281\n",
      "      433983126 16:01:52 Epoch 1469/5000, D_loss=0.050892, reward=-7.657326\n",
      "      433983126 16:01:53 Epoch 1470/5000, D_loss=0.050091, reward=-7.653852\n",
      "      433983126 16:01:53 Epoch 1471/5000, D_loss=0.050514, reward=-7.656761\n",
      "      433983126 16:01:53 Epoch 1472/5000, D_loss=0.048096, reward=-7.656285\n",
      "      433983126 16:01:54 Epoch 1473/5000, D_loss=0.050646, reward=-7.651019\n",
      "      433983126 16:01:54 Epoch 1474/5000, D_loss=0.048817, reward=-7.653792\n",
      "      433983126 16:01:54 Epoch 1475/5000, D_loss=0.047865, reward=-7.656694\n",
      "      433983126 16:01:55 Epoch 1476/5000, D_loss=0.049889, reward=-7.656445\n",
      "      433983126 16:01:55 Epoch 1477/5000, D_loss=0.047306, reward=-7.650309\n",
      "      433983126 16:01:56 Epoch 1478/5000, D_loss=0.050797, reward=-7.652484\n",
      "      433983126 16:01:56 Epoch 1479/5000, D_loss=0.048045, reward=-7.653347\n",
      "      433983126 16:01:56 Epoch 1480/5000, D_loss=0.049637, reward=-7.653109\n",
      "      433983126 16:01:57 Epoch 1481/5000, D_loss=0.049306, reward=-7.652744\n",
      "      433983126 16:01:57 Epoch 1482/5000, D_loss=0.048542, reward=-7.650318\n",
      "      433983126 16:01:57 Epoch 1483/5000, D_loss=0.048214, reward=-7.654934\n",
      "      433983126 16:01:58 Epoch 1484/5000, D_loss=0.049105, reward=-7.655397\n",
      "      433983126 16:01:58 Epoch 1485/5000, D_loss=0.047268, reward=-7.655434\n",
      "      433983126 16:01:59 Epoch 1486/5000, D_loss=0.047369, reward=-7.657926\n",
      "      433983126 16:01:59 Epoch 1487/5000, D_loss=0.049254, reward=-7.649382\n",
      "      433983126 16:01:59 Epoch 1488/5000, D_loss=0.049366, reward=-7.651674\n",
      "      433983126 16:02:00 Epoch 1489/5000, D_loss=0.048529, reward=-7.650584\n",
      "      433983126 16:02:00 Epoch 1490/5000, D_loss=0.049311, reward=-7.652855\n",
      "      433983126 16:02:00 Epoch 1491/5000, D_loss=0.048755, reward=-7.653442\n",
      "      433983126 16:02:01 Epoch 1492/5000, D_loss=0.050722, reward=-7.650105\n",
      "      433983126 16:02:01 Epoch 1493/5000, D_loss=0.049624, reward=-7.643472\n",
      "      433983126 16:02:02 Epoch 1494/5000, D_loss=0.049588, reward=-7.649266\n",
      "      433983126 16:02:02 Epoch 1495/5000, D_loss=0.048722, reward=-7.654663\n",
      "      433983126 16:02:02 Epoch 1496/5000, D_loss=0.048264, reward=-7.649518\n",
      "      433983126 16:02:03 Epoch 1497/5000, D_loss=0.049358, reward=-7.648598\n",
      "      433983126 16:02:03 Epoch 1498/5000, D_loss=0.051161, reward=-7.646523\n",
      "      433983126 16:02:03 Epoch 1499/5000, D_loss=0.049572, reward=-7.646884\n",
      "      433983126 16:02:04 Epoch 1500/5000, D_loss=0.049377, reward=-7.652916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hhehe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      248942384 16:02:08 MR = 4599.9267578125\n",
      "MRR = 0.18133024871349335\n",
      "Hit@1 = 0.008404746209624258\n",
      "Hit@3 = 0.3314106789716546\n",
      "Hit@10 = 0.4372116018457482\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MR = 4599.9267578125\n",
      "MRR = 0.18133024871349335\n",
      "Hit@1 = 0.008404746209624258\n",
      "Hit@3 = 0.3314106789716546\n",
      "Hit@10 = 0.4372116018457482\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      433983126 16:02:08 Epoch 1501/5000, D_loss=0.050165, reward=-7.647502\n",
      "      433983126 16:02:08 Epoch 1502/5000, D_loss=0.050491, reward=-7.646110\n",
      "      433983126 16:02:09 Epoch 1503/5000, D_loss=0.048644, reward=-7.653928\n",
      "      433983126 16:02:09 Epoch 1504/5000, D_loss=0.047805, reward=-7.647807\n",
      "      433983126 16:02:10 Epoch 1505/5000, D_loss=0.048258, reward=-7.652993\n",
      "      433983126 16:02:10 Epoch 1506/5000, D_loss=0.049238, reward=-7.643524\n",
      "      433983126 16:02:10 Epoch 1507/5000, D_loss=0.050686, reward=-7.646611\n",
      "      433983126 16:02:11 Epoch 1508/5000, D_loss=0.049254, reward=-7.649440\n",
      "      433983126 16:02:11 Epoch 1509/5000, D_loss=0.051681, reward=-7.648094\n",
      "      433983126 16:02:11 Epoch 1510/5000, D_loss=0.049092, reward=-7.654383\n",
      "      433983126 16:02:12 Epoch 1511/5000, D_loss=0.050252, reward=-7.640727\n",
      "      433983126 16:02:12 Epoch 1512/5000, D_loss=0.049116, reward=-7.641704\n",
      "      433983126 16:02:13 Epoch 1513/5000, D_loss=0.048796, reward=-7.650681\n",
      "      433983126 16:02:13 Epoch 1514/5000, D_loss=0.047703, reward=-7.648036\n",
      "      433983126 16:02:13 Epoch 1515/5000, D_loss=0.047400, reward=-7.647340\n",
      "      433983126 16:02:14 Epoch 1516/5000, D_loss=0.048953, reward=-7.645405\n",
      "      433983126 16:02:14 Epoch 1517/5000, D_loss=0.048903, reward=-7.649883\n",
      "      433983126 16:02:14 Epoch 1518/5000, D_loss=0.050242, reward=-7.644237\n",
      "      433983126 16:02:15 Epoch 1519/5000, D_loss=0.049590, reward=-7.647760\n",
      "      433983126 16:02:15 Epoch 1520/5000, D_loss=0.049638, reward=-7.645499\n",
      "      433983126 16:02:16 Epoch 1521/5000, D_loss=0.048999, reward=-7.646259\n",
      "      433983126 16:02:16 Epoch 1522/5000, D_loss=0.049249, reward=-7.641475\n",
      "      433983126 16:02:16 Epoch 1523/5000, D_loss=0.050262, reward=-7.640745\n",
      "      433983126 16:02:17 Epoch 1524/5000, D_loss=0.049780, reward=-7.644380\n",
      "      433983126 16:02:17 Epoch 1525/5000, D_loss=0.048762, reward=-7.647786\n",
      "      433983126 16:02:17 Epoch 1526/5000, D_loss=0.050627, reward=-7.641970\n",
      "      433983126 16:02:18 Epoch 1527/5000, D_loss=0.047774, reward=-7.642646\n",
      "      433983126 16:02:18 Epoch 1528/5000, D_loss=0.047360, reward=-7.642721\n",
      "      433983126 16:02:18 Epoch 1529/5000, D_loss=0.049023, reward=-7.639922\n",
      "      433983126 16:02:19 Epoch 1530/5000, D_loss=0.049370, reward=-7.642488\n",
      "      433983126 16:02:19 Epoch 1531/5000, D_loss=0.050288, reward=-7.640390\n",
      "      433983126 16:02:20 Epoch 1532/5000, D_loss=0.049584, reward=-7.636593\n",
      "      433983126 16:02:20 Epoch 1533/5000, D_loss=0.048613, reward=-7.641911\n",
      "      433983126 16:02:20 Epoch 1534/5000, D_loss=0.048872, reward=-7.637717\n",
      "      433983126 16:02:21 Epoch 1535/5000, D_loss=0.048079, reward=-7.641939\n",
      "      433983126 16:02:21 Epoch 1536/5000, D_loss=0.050785, reward=-7.638556\n",
      "      433983126 16:02:21 Epoch 1537/5000, D_loss=0.049379, reward=-7.633004\n",
      "      433983126 16:02:22 Epoch 1538/5000, D_loss=0.050747, reward=-7.639959\n",
      "      433983126 16:02:22 Epoch 1539/5000, D_loss=0.048614, reward=-7.640890\n",
      "      433983126 16:02:23 Epoch 1540/5000, D_loss=0.048959, reward=-7.640016\n",
      "      433983126 16:02:23 Epoch 1541/5000, D_loss=0.049175, reward=-7.640090\n",
      "      433983126 16:02:23 Epoch 1542/5000, D_loss=0.047436, reward=-7.641036\n",
      "      433983126 16:02:24 Epoch 1543/5000, D_loss=0.047819, reward=-7.639223\n",
      "      433983126 16:02:24 Epoch 1544/5000, D_loss=0.048165, reward=-7.640415\n",
      "      433983126 16:02:24 Epoch 1545/5000, D_loss=0.049252, reward=-7.642052\n",
      "      433983126 16:02:25 Epoch 1546/5000, D_loss=0.047971, reward=-7.641220\n",
      "      433983126 16:02:25 Epoch 1547/5000, D_loss=0.048093, reward=-7.638348\n",
      "      433983126 16:02:26 Epoch 1548/5000, D_loss=0.048649, reward=-7.640063\n",
      "      433983126 16:02:26 Epoch 1549/5000, D_loss=0.052118, reward=-7.633577\n",
      "      433983126 16:02:26 Epoch 1550/5000, D_loss=0.049631, reward=-7.632762\n",
      "      433983126 16:02:27 Epoch 1551/5000, D_loss=0.048149, reward=-7.635408\n",
      "      433983126 16:02:27 Epoch 1552/5000, D_loss=0.048509, reward=-7.644814\n",
      "      433983126 16:02:27 Epoch 1553/5000, D_loss=0.050287, reward=-7.637049\n",
      "      433983126 16:02:28 Epoch 1554/5000, D_loss=0.049636, reward=-7.636697\n",
      "      433983126 16:02:28 Epoch 1555/5000, D_loss=0.048325, reward=-7.635555\n",
      "      433983126 16:02:29 Epoch 1556/5000, D_loss=0.050682, reward=-7.631696\n",
      "      433983126 16:02:29 Epoch 1557/5000, D_loss=0.050616, reward=-7.632080\n",
      "      433983126 16:02:29 Epoch 1558/5000, D_loss=0.049688, reward=-7.635725\n",
      "      433983126 16:02:30 Epoch 1559/5000, D_loss=0.049939, reward=-7.636102\n",
      "      433983126 16:02:30 Epoch 1560/5000, D_loss=0.048563, reward=-7.632988\n",
      "      433983126 16:02:30 Epoch 1561/5000, D_loss=0.050827, reward=-7.633868\n",
      "      433983126 16:02:31 Epoch 1562/5000, D_loss=0.050111, reward=-7.637757\n",
      "      433983126 16:02:31 Epoch 1563/5000, D_loss=0.049431, reward=-7.638281\n",
      "      433983126 16:02:32 Epoch 1564/5000, D_loss=0.047506, reward=-7.639219\n",
      "      433983126 16:02:32 Epoch 1565/5000, D_loss=0.048515, reward=-7.639332\n",
      "      433983126 16:02:32 Epoch 1566/5000, D_loss=0.050540, reward=-7.635437\n",
      "      433983126 16:02:33 Epoch 1567/5000, D_loss=0.048242, reward=-7.638532\n",
      "      433983126 16:02:33 Epoch 1568/5000, D_loss=0.049900, reward=-7.633473\n",
      "      433983126 16:02:33 Epoch 1569/5000, D_loss=0.050221, reward=-7.635000\n",
      "      433983126 16:02:34 Epoch 1570/5000, D_loss=0.050291, reward=-7.626252\n",
      "      433983126 16:02:34 Epoch 1571/5000, D_loss=0.049275, reward=-7.628888\n",
      "      433983126 16:02:35 Epoch 1572/5000, D_loss=0.050951, reward=-7.627646\n",
      "      433983126 16:02:35 Epoch 1573/5000, D_loss=0.049266, reward=-7.630114\n",
      "      433983126 16:02:35 Epoch 1574/5000, D_loss=0.049118, reward=-7.633059\n",
      "      433983126 16:02:36 Epoch 1575/5000, D_loss=0.050416, reward=-7.629552\n",
      "      433983126 16:02:36 Epoch 1576/5000, D_loss=0.049541, reward=-7.628510\n",
      "      433983126 16:02:36 Epoch 1577/5000, D_loss=0.049608, reward=-7.630606\n",
      "      433983126 16:02:37 Epoch 1578/5000, D_loss=0.049308, reward=-7.632163\n",
      "      433983126 16:02:37 Epoch 1579/5000, D_loss=0.050542, reward=-7.629655\n",
      "      433983126 16:02:38 Epoch 1580/5000, D_loss=0.048168, reward=-7.634240\n",
      "      433983126 16:02:38 Epoch 1581/5000, D_loss=0.049880, reward=-7.633778\n",
      "      433983126 16:02:38 Epoch 1582/5000, D_loss=0.050589, reward=-7.629656\n",
      "      433983126 16:02:39 Epoch 1583/5000, D_loss=0.049707, reward=-7.625441\n",
      "      433983126 16:02:39 Epoch 1584/5000, D_loss=0.048412, reward=-7.630892\n",
      "      433983126 16:02:39 Epoch 1585/5000, D_loss=0.050585, reward=-7.627916\n",
      "      433983126 16:02:40 Epoch 1586/5000, D_loss=0.049967, reward=-7.626996\n",
      "      433983126 16:02:40 Epoch 1587/5000, D_loss=0.048651, reward=-7.634274\n",
      "      433983126 16:02:41 Epoch 1588/5000, D_loss=0.050590, reward=-7.629900\n",
      "      433983126 16:02:41 Epoch 1589/5000, D_loss=0.050054, reward=-7.630345\n",
      "      433983126 16:02:41 Epoch 1590/5000, D_loss=0.048557, reward=-7.635429\n",
      "      433983126 16:02:42 Epoch 1591/5000, D_loss=0.051925, reward=-7.625065\n",
      "      433983126 16:02:42 Epoch 1592/5000, D_loss=0.050479, reward=-7.629359\n",
      "      433983126 16:02:42 Epoch 1593/5000, D_loss=0.051503, reward=-7.626514\n",
      "      433983126 16:02:43 Epoch 1594/5000, D_loss=0.048335, reward=-7.625715\n",
      "      433983126 16:02:43 Epoch 1595/5000, D_loss=0.049747, reward=-7.624781\n",
      "      433983126 16:02:44 Epoch 1596/5000, D_loss=0.049411, reward=-7.622639\n",
      "      433983126 16:02:44 Epoch 1597/5000, D_loss=0.050383, reward=-7.624368\n",
      "      433983126 16:02:44 Epoch 1598/5000, D_loss=0.049834, reward=-7.631119\n",
      "      433983126 16:02:45 Epoch 1599/5000, D_loss=0.049626, reward=-7.623706\n",
      "      433983126 16:02:45 Epoch 1600/5000, D_loss=0.050853, reward=-7.621402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hhehe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      248942384 16:02:49 MR = 4501.2490234375\n",
      "MRR = 0.18070189654827118\n",
      "Hit@1 = 0.006262359920896506\n",
      "Hit@3 = 0.3360250494396836\n",
      "Hit@10 = 0.4362228081740277\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MR = 4501.2490234375\n",
      "MRR = 0.18070189654827118\n",
      "Hit@1 = 0.006262359920896506\n",
      "Hit@3 = 0.3360250494396836\n",
      "Hit@10 = 0.4362228081740277\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      433983126 16:02:49 Epoch 1601/5000, D_loss=0.048574, reward=-7.624144\n",
      "      433983126 16:02:50 Epoch 1602/5000, D_loss=0.049220, reward=-7.626698\n",
      "      433983126 16:02:50 Epoch 1603/5000, D_loss=0.049794, reward=-7.630246\n",
      "      433983126 16:02:50 Epoch 1604/5000, D_loss=0.049144, reward=-7.617232\n",
      "      433983126 16:02:51 Epoch 1605/5000, D_loss=0.048330, reward=-7.627537\n",
      "      433983126 16:02:51 Epoch 1606/5000, D_loss=0.050618, reward=-7.624564\n",
      "      433983126 16:02:52 Epoch 1607/5000, D_loss=0.051424, reward=-7.622464\n",
      "      433983126 16:02:52 Epoch 1608/5000, D_loss=0.049650, reward=-7.623703\n",
      "      433983126 16:02:52 Epoch 1609/5000, D_loss=0.048742, reward=-7.625051\n",
      "      433983126 16:02:53 Epoch 1610/5000, D_loss=0.049100, reward=-7.625636\n",
      "      433983126 16:02:53 Epoch 1611/5000, D_loss=0.049694, reward=-7.625449\n",
      "      433983126 16:02:53 Epoch 1612/5000, D_loss=0.047545, reward=-7.627980\n",
      "      433983126 16:02:54 Epoch 1613/5000, D_loss=0.049966, reward=-7.617781\n",
      "      433983126 16:02:54 Epoch 1614/5000, D_loss=0.047833, reward=-7.624076\n",
      "      433983126 16:02:55 Epoch 1615/5000, D_loss=0.049786, reward=-7.622398\n",
      "      433983126 16:02:55 Epoch 1616/5000, D_loss=0.050433, reward=-7.621797\n",
      "      433983126 16:02:55 Epoch 1617/5000, D_loss=0.049712, reward=-7.626029\n",
      "      433983126 16:02:56 Epoch 1618/5000, D_loss=0.048565, reward=-7.625793\n",
      "      433983126 16:02:56 Epoch 1619/5000, D_loss=0.051316, reward=-7.620787\n",
      "      433983126 16:02:56 Epoch 1620/5000, D_loss=0.048015, reward=-7.627630\n",
      "      433983126 16:02:57 Epoch 1621/5000, D_loss=0.049950, reward=-7.622663\n",
      "      433983126 16:02:57 Epoch 1622/5000, D_loss=0.049250, reward=-7.618446\n",
      "      433983126 16:02:58 Epoch 1623/5000, D_loss=0.049729, reward=-7.616736\n",
      "      433983126 16:02:58 Epoch 1624/5000, D_loss=0.049896, reward=-7.620187\n",
      "      433983126 16:02:58 Epoch 1625/5000, D_loss=0.049310, reward=-7.619610\n",
      "      433983126 16:02:59 Epoch 1626/5000, D_loss=0.047850, reward=-7.621245\n",
      "      433983126 16:02:59 Epoch 1627/5000, D_loss=0.048774, reward=-7.617279\n",
      "      433983126 16:02:59 Epoch 1628/5000, D_loss=0.048755, reward=-7.617396\n",
      "      433983126 16:03:00 Epoch 1629/5000, D_loss=0.050816, reward=-7.613621\n",
      "      433983126 16:03:00 Epoch 1630/5000, D_loss=0.049979, reward=-7.619515\n",
      "      433983126 16:03:01 Epoch 1631/5000, D_loss=0.050424, reward=-7.614108\n",
      "      433983126 16:03:01 Epoch 1632/5000, D_loss=0.049251, reward=-7.614196\n",
      "      433983126 16:03:01 Epoch 1633/5000, D_loss=0.049864, reward=-7.616284\n",
      "      433983126 16:03:02 Epoch 1634/5000, D_loss=0.050217, reward=-7.617967\n",
      "      433983126 16:03:02 Epoch 1635/5000, D_loss=0.048806, reward=-7.618299\n",
      "      433983126 16:03:03 Epoch 1636/5000, D_loss=0.049123, reward=-7.617594\n",
      "      433983126 16:03:03 Epoch 1637/5000, D_loss=0.049501, reward=-7.612008\n",
      "      433983126 16:03:03 Epoch 1638/5000, D_loss=0.048325, reward=-7.617933\n",
      "      433983126 16:03:04 Epoch 1639/5000, D_loss=0.049700, reward=-7.618792\n",
      "      433983126 16:03:04 Epoch 1640/5000, D_loss=0.051525, reward=-7.615707\n",
      "      433983126 16:03:04 Epoch 1641/5000, D_loss=0.050929, reward=-7.614324\n",
      "      433983126 16:03:05 Epoch 1642/5000, D_loss=0.051063, reward=-7.613419\n",
      "      433983126 16:03:05 Epoch 1643/5000, D_loss=0.049793, reward=-7.618235\n",
      "      433983126 16:03:06 Epoch 1644/5000, D_loss=0.049820, reward=-7.613328\n",
      "      433983126 16:03:06 Epoch 1645/5000, D_loss=0.050831, reward=-7.614716\n",
      "      433983126 16:03:06 Epoch 1646/5000, D_loss=0.049085, reward=-7.618402\n",
      "      433983126 16:03:07 Epoch 1647/5000, D_loss=0.049870, reward=-7.612709\n",
      "      433983126 16:03:07 Epoch 1648/5000, D_loss=0.051149, reward=-7.614251\n",
      "      433983126 16:03:07 Epoch 1649/5000, D_loss=0.049845, reward=-7.614623\n",
      "      433983126 16:03:08 Epoch 1650/5000, D_loss=0.050064, reward=-7.617271\n",
      "      433983126 16:03:08 Epoch 1651/5000, D_loss=0.049046, reward=-7.612450\n",
      "      433983126 16:03:09 Epoch 1652/5000, D_loss=0.048934, reward=-7.615935\n",
      "      433983126 16:03:09 Epoch 1653/5000, D_loss=0.050238, reward=-7.610267\n",
      "      433983126 16:03:09 Epoch 1654/5000, D_loss=0.051264, reward=-7.608325\n",
      "      433983126 16:03:10 Epoch 1655/5000, D_loss=0.048234, reward=-7.617688\n",
      "      433983126 16:03:10 Epoch 1656/5000, D_loss=0.051023, reward=-7.605055\n",
      "      433983126 16:03:10 Epoch 1657/5000, D_loss=0.050611, reward=-7.612139\n",
      "      433983126 16:03:11 Epoch 1658/5000, D_loss=0.049723, reward=-7.612334\n",
      "      433983126 16:03:11 Epoch 1659/5000, D_loss=0.050621, reward=-7.607982\n",
      "      433983126 16:03:11 Epoch 1660/5000, D_loss=0.049064, reward=-7.614264\n",
      "      433983126 16:03:12 Epoch 1661/5000, D_loss=0.049576, reward=-7.614114\n",
      "      433983126 16:03:12 Epoch 1662/5000, D_loss=0.049255, reward=-7.618544\n",
      "      433983126 16:03:13 Epoch 1663/5000, D_loss=0.049878, reward=-7.609437\n",
      "      433983126 16:03:13 Epoch 1664/5000, D_loss=0.047518, reward=-7.611844\n",
      "      433983126 16:03:13 Epoch 1665/5000, D_loss=0.049791, reward=-7.616424\n",
      "      433983126 16:03:14 Epoch 1666/5000, D_loss=0.048899, reward=-7.612503\n",
      "      433983126 16:03:14 Epoch 1667/5000, D_loss=0.049244, reward=-7.611657\n",
      "      433983126 16:03:14 Epoch 1668/5000, D_loss=0.049985, reward=-7.612221\n",
      "      433983126 16:03:15 Epoch 1669/5000, D_loss=0.049077, reward=-7.611410\n",
      "      433983126 16:03:15 Epoch 1670/5000, D_loss=0.050813, reward=-7.606524\n",
      "      433983126 16:03:15 Epoch 1671/5000, D_loss=0.049333, reward=-7.612817\n",
      "      433983126 16:03:16 Epoch 1672/5000, D_loss=0.048650, reward=-7.615781\n",
      "      433983126 16:03:16 Epoch 1673/5000, D_loss=0.050311, reward=-7.609747\n",
      "      433983126 16:03:16 Epoch 1674/5000, D_loss=0.051301, reward=-7.606442\n",
      "      433983126 16:03:17 Epoch 1675/5000, D_loss=0.047645, reward=-7.611362\n",
      "      433983126 16:03:17 Epoch 1676/5000, D_loss=0.050783, reward=-7.604915\n",
      "      433983126 16:03:18 Epoch 1677/5000, D_loss=0.049774, reward=-7.610999\n",
      "      433983126 16:03:18 Epoch 1678/5000, D_loss=0.050143, reward=-7.611149\n",
      "      433983126 16:03:18 Epoch 1679/5000, D_loss=0.049528, reward=-7.612098\n",
      "      433983126 16:03:19 Epoch 1680/5000, D_loss=0.049498, reward=-7.609864\n",
      "      433983126 16:03:19 Epoch 1681/5000, D_loss=0.050004, reward=-7.611256\n",
      "      433983126 16:03:19 Epoch 1682/5000, D_loss=0.050455, reward=-7.606555\n",
      "      433983126 16:03:20 Epoch 1683/5000, D_loss=0.051321, reward=-7.607870\n",
      "      433983126 16:03:20 Epoch 1684/5000, D_loss=0.051912, reward=-7.597709\n",
      "      433983126 16:03:20 Epoch 1685/5000, D_loss=0.051343, reward=-7.601247\n",
      "      433983126 16:03:21 Epoch 1686/5000, D_loss=0.051191, reward=-7.598642\n",
      "      433983126 16:03:21 Epoch 1687/5000, D_loss=0.048003, reward=-7.610753\n",
      "      433983126 16:03:22 Epoch 1688/5000, D_loss=0.047757, reward=-7.608320\n",
      "      433983126 16:03:22 Epoch 1689/5000, D_loss=0.050181, reward=-7.603734\n",
      "      433983126 16:03:22 Epoch 1690/5000, D_loss=0.050257, reward=-7.605118\n",
      "      433983126 16:03:23 Epoch 1691/5000, D_loss=0.049280, reward=-7.608394\n",
      "      433983126 16:03:23 Epoch 1692/5000, D_loss=0.049283, reward=-7.605119\n",
      "      433983126 16:03:23 Epoch 1693/5000, D_loss=0.050023, reward=-7.608699\n",
      "      433983126 16:03:24 Epoch 1694/5000, D_loss=0.049698, reward=-7.605208\n",
      "      433983126 16:03:24 Epoch 1695/5000, D_loss=0.051430, reward=-7.599894\n",
      "      433983126 16:03:24 Epoch 1696/5000, D_loss=0.049531, reward=-7.605853\n",
      "      433983126 16:03:25 Epoch 1697/5000, D_loss=0.052057, reward=-7.603004\n",
      "      433983126 16:03:25 Epoch 1698/5000, D_loss=0.049980, reward=-7.598245\n",
      "      433983126 16:03:26 Epoch 1699/5000, D_loss=0.048892, reward=-7.601089\n",
      "      433983126 16:03:26 Epoch 1700/5000, D_loss=0.048576, reward=-7.605547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hhehe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      248942384 16:03:30 MR = 4478.18408203125\n",
      "MRR = 0.18287399411201477\n",
      "Hit@1 = 0.008075148319050759\n",
      "Hit@3 = 0.33569545154911007\n",
      "Hit@10 = 0.4401779828609097\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MR = 4478.18408203125\n",
      "MRR = 0.18287399411201477\n",
      "Hit@1 = 0.008075148319050759\n",
      "Hit@3 = 0.33569545154911007\n",
      "Hit@10 = 0.4401779828609097\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      433983126 16:03:30 Epoch 1701/5000, D_loss=0.048348, reward=-7.609323\n",
      "      433983126 16:03:31 Epoch 1702/5000, D_loss=0.050975, reward=-7.601363\n",
      "      433983126 16:03:31 Epoch 1703/5000, D_loss=0.050582, reward=-7.604121\n",
      "      433983126 16:03:31 Epoch 1704/5000, D_loss=0.048839, reward=-7.606769\n",
      "      433983126 16:03:32 Epoch 1705/5000, D_loss=0.049708, reward=-7.598791\n",
      "      433983126 16:03:32 Epoch 1706/5000, D_loss=0.050567, reward=-7.608658\n",
      "      433983126 16:03:32 Epoch 1707/5000, D_loss=0.050544, reward=-7.601024\n",
      "      433983126 16:03:33 Epoch 1708/5000, D_loss=0.050806, reward=-7.601025\n",
      "      433983126 16:03:33 Epoch 1709/5000, D_loss=0.048570, reward=-7.600719\n",
      "      433983126 16:03:34 Epoch 1710/5000, D_loss=0.050365, reward=-7.599561\n",
      "      433983126 16:03:34 Epoch 1711/5000, D_loss=0.050717, reward=-7.596979\n",
      "      433983126 16:03:34 Epoch 1712/5000, D_loss=0.049803, reward=-7.598984\n",
      "      433983126 16:03:35 Epoch 1713/5000, D_loss=0.049088, reward=-7.598957\n",
      "      433983126 16:03:35 Epoch 1714/5000, D_loss=0.048273, reward=-7.596966\n",
      "      433983126 16:03:35 Epoch 1715/5000, D_loss=0.050613, reward=-7.597263\n",
      "      433983126 16:03:36 Epoch 1716/5000, D_loss=0.049563, reward=-7.598659\n",
      "      433983126 16:03:36 Epoch 1717/5000, D_loss=0.049957, reward=-7.596622\n",
      "      433983126 16:03:37 Epoch 1718/5000, D_loss=0.050624, reward=-7.601326\n",
      "      433983126 16:03:37 Epoch 1719/5000, D_loss=0.049941, reward=-7.597529\n",
      "      433983126 16:03:37 Epoch 1720/5000, D_loss=0.049254, reward=-7.596256\n",
      "      433983126 16:03:38 Epoch 1721/5000, D_loss=0.049567, reward=-7.597062\n",
      "      433983126 16:03:38 Epoch 1722/5000, D_loss=0.048372, reward=-7.602352\n",
      "      433983126 16:03:38 Epoch 1723/5000, D_loss=0.048437, reward=-7.598386\n",
      "      433983126 16:03:39 Epoch 1724/5000, D_loss=0.051792, reward=-7.595699\n",
      "      433983126 16:03:39 Epoch 1725/5000, D_loss=0.052076, reward=-7.594611\n",
      "      433983126 16:03:39 Epoch 1726/5000, D_loss=0.049723, reward=-7.597277\n",
      "      433983126 16:03:40 Epoch 1727/5000, D_loss=0.051897, reward=-7.595394\n",
      "      433983126 16:03:40 Epoch 1728/5000, D_loss=0.049891, reward=-7.597595\n",
      "      433983126 16:03:41 Epoch 1729/5000, D_loss=0.050071, reward=-7.591701\n",
      "      433983126 16:03:41 Epoch 1730/5000, D_loss=0.047688, reward=-7.599873\n",
      "      433983126 16:03:41 Epoch 1731/5000, D_loss=0.049748, reward=-7.594965\n",
      "      433983126 16:03:42 Epoch 1732/5000, D_loss=0.049850, reward=-7.595188\n",
      "      433983126 16:03:42 Epoch 1733/5000, D_loss=0.050455, reward=-7.597719\n",
      "      433983126 16:03:43 Epoch 1734/5000, D_loss=0.047555, reward=-7.595165\n",
      "      433983126 16:03:43 Epoch 1735/5000, D_loss=0.050000, reward=-7.597779\n",
      "      433983126 16:03:43 Epoch 1736/5000, D_loss=0.049124, reward=-7.596972\n",
      "      433983126 16:03:44 Epoch 1737/5000, D_loss=0.051567, reward=-7.589080\n",
      "      433983126 16:03:44 Epoch 1738/5000, D_loss=0.049252, reward=-7.596558\n",
      "      433983126 16:03:44 Epoch 1739/5000, D_loss=0.049070, reward=-7.599720\n",
      "      433983126 16:03:45 Epoch 1740/5000, D_loss=0.049198, reward=-7.591062\n",
      "      433983126 16:03:45 Epoch 1741/5000, D_loss=0.049965, reward=-7.594625\n",
      "      433983126 16:03:46 Epoch 1742/5000, D_loss=0.049395, reward=-7.595683\n",
      "      433983126 16:03:46 Epoch 1743/5000, D_loss=0.050925, reward=-7.593016\n",
      "      433983126 16:03:46 Epoch 1744/5000, D_loss=0.048527, reward=-7.601311\n",
      "      433983126 16:03:47 Epoch 1745/5000, D_loss=0.050994, reward=-7.590500\n",
      "      433983126 16:03:47 Epoch 1746/5000, D_loss=0.050408, reward=-7.594501\n",
      "      433983126 16:03:47 Epoch 1747/5000, D_loss=0.049824, reward=-7.596850\n",
      "      433983126 16:03:48 Epoch 1748/5000, D_loss=0.050197, reward=-7.588826\n",
      "      433983126 16:03:48 Epoch 1749/5000, D_loss=0.051477, reward=-7.591790\n",
      "      433983126 16:03:49 Epoch 1750/5000, D_loss=0.051344, reward=-7.592223\n",
      "      433983126 16:03:49 Epoch 1751/5000, D_loss=0.048629, reward=-7.593832\n",
      "      433983126 16:03:49 Epoch 1752/5000, D_loss=0.048687, reward=-7.594894\n",
      "      433983126 16:03:50 Epoch 1753/5000, D_loss=0.051267, reward=-7.585405\n",
      "      433983126 16:03:50 Epoch 1754/5000, D_loss=0.051028, reward=-7.593080\n",
      "      433983126 16:03:50 Epoch 1755/5000, D_loss=0.049615, reward=-7.592286\n",
      "      433983126 16:03:51 Epoch 1756/5000, D_loss=0.050217, reward=-7.589792\n",
      "      433983126 16:03:51 Epoch 1757/5000, D_loss=0.050666, reward=-7.593129\n",
      "      433983126 16:03:52 Epoch 1758/5000, D_loss=0.050572, reward=-7.589104\n",
      "      433983126 16:03:52 Epoch 1759/5000, D_loss=0.052300, reward=-7.588144\n",
      "      433983126 16:03:52 Epoch 1760/5000, D_loss=0.050805, reward=-7.587918\n",
      "      433983126 16:03:53 Epoch 1761/5000, D_loss=0.052304, reward=-7.583642\n",
      "      433983126 16:03:53 Epoch 1762/5000, D_loss=0.050233, reward=-7.584240\n",
      "      433983126 16:03:53 Epoch 1763/5000, D_loss=0.051218, reward=-7.585252\n",
      "      433983126 16:03:54 Epoch 1764/5000, D_loss=0.049858, reward=-7.585204\n",
      "      433983126 16:03:54 Epoch 1765/5000, D_loss=0.049107, reward=-7.592214\n",
      "      433983126 16:03:55 Epoch 1766/5000, D_loss=0.050168, reward=-7.590370\n",
      "      433983126 16:03:55 Epoch 1767/5000, D_loss=0.050056, reward=-7.586096\n",
      "      433983126 16:03:55 Epoch 1768/5000, D_loss=0.048671, reward=-7.582182\n",
      "      433983126 16:03:56 Epoch 1769/5000, D_loss=0.049493, reward=-7.590677\n",
      "      433983126 16:03:56 Epoch 1770/5000, D_loss=0.050415, reward=-7.585810\n",
      "      433983126 16:03:56 Epoch 1771/5000, D_loss=0.049741, reward=-7.584826\n",
      "      433983126 16:03:57 Epoch 1772/5000, D_loss=0.049879, reward=-7.583272\n",
      "      433983126 16:03:57 Epoch 1773/5000, D_loss=0.050510, reward=-7.583780\n",
      "      433983126 16:03:58 Epoch 1774/5000, D_loss=0.050005, reward=-7.588459\n",
      "      433983126 16:03:58 Epoch 1775/5000, D_loss=0.051264, reward=-7.583664\n",
      "      433983126 16:03:58 Epoch 1776/5000, D_loss=0.051063, reward=-7.585024\n",
      "      433983126 16:03:59 Epoch 1777/5000, D_loss=0.050516, reward=-7.588315\n",
      "      433983126 16:03:59 Epoch 1778/5000, D_loss=0.051868, reward=-7.580490\n",
      "      433983126 16:03:59 Epoch 1779/5000, D_loss=0.050230, reward=-7.585433\n",
      "      433983126 16:04:00 Epoch 1780/5000, D_loss=0.049375, reward=-7.585026\n",
      "      433983126 16:04:00 Epoch 1781/5000, D_loss=0.051051, reward=-7.581401\n",
      "      433983126 16:04:01 Epoch 1782/5000, D_loss=0.049815, reward=-7.591042\n",
      "      433983126 16:04:01 Epoch 1783/5000, D_loss=0.048499, reward=-7.589727\n",
      "      433983126 16:04:01 Epoch 1784/5000, D_loss=0.048522, reward=-7.586508\n",
      "      433983126 16:04:02 Epoch 1785/5000, D_loss=0.049301, reward=-7.588342\n",
      "      433983126 16:04:02 Epoch 1786/5000, D_loss=0.050744, reward=-7.584483\n",
      "      433983126 16:04:02 Epoch 1787/5000, D_loss=0.051522, reward=-7.579513\n",
      "      433983126 16:04:03 Epoch 1788/5000, D_loss=0.050709, reward=-7.574485\n",
      "      433983126 16:04:03 Epoch 1789/5000, D_loss=0.050594, reward=-7.577930\n",
      "      433983126 16:04:04 Epoch 1790/5000, D_loss=0.048821, reward=-7.585756\n",
      "      433983126 16:04:04 Epoch 1791/5000, D_loss=0.050740, reward=-7.586212\n",
      "      433983126 16:04:04 Epoch 1792/5000, D_loss=0.052125, reward=-7.579877\n",
      "      433983126 16:04:05 Epoch 1793/5000, D_loss=0.050851, reward=-7.578414\n",
      "      433983126 16:04:05 Epoch 1794/5000, D_loss=0.051559, reward=-7.576861\n",
      "      433983126 16:04:05 Epoch 1795/5000, D_loss=0.050227, reward=-7.577440\n",
      "      433983126 16:04:06 Epoch 1796/5000, D_loss=0.050299, reward=-7.578540\n",
      "      433983126 16:04:06 Epoch 1797/5000, D_loss=0.051093, reward=-7.580292\n",
      "      433983126 16:04:07 Epoch 1798/5000, D_loss=0.050067, reward=-7.581209\n",
      "      433983126 16:04:07 Epoch 1799/5000, D_loss=0.050782, reward=-7.582852\n",
      "      433983126 16:04:07 Epoch 1800/5000, D_loss=0.049719, reward=-7.580753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hhehe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      248942384 16:04:11 MR = 4543.48974609375\n",
      "MRR = 0.18234692513942719\n",
      "Hit@1 = 0.007910349373764008\n",
      "Hit@3 = 0.3380026367831246\n",
      "Hit@10 = 0.43852999340804216\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MR = 4543.48974609375\n",
      "MRR = 0.18234692513942719\n",
      "Hit@1 = 0.007910349373764008\n",
      "Hit@3 = 0.3380026367831246\n",
      "Hit@10 = 0.43852999340804216\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      433983126 16:04:12 Epoch 1801/5000, D_loss=0.051725, reward=-7.577101\n",
      "      433983126 16:04:12 Epoch 1802/5000, D_loss=0.049891, reward=-7.578153\n",
      "      433983126 16:04:12 Epoch 1803/5000, D_loss=0.048826, reward=-7.578783\n",
      "      433983126 16:04:13 Epoch 1804/5000, D_loss=0.048490, reward=-7.577110\n",
      "      433983126 16:04:13 Epoch 1805/5000, D_loss=0.049611, reward=-7.578300\n",
      "      433983126 16:04:14 Epoch 1806/5000, D_loss=0.049796, reward=-7.581692\n",
      "      433983126 16:04:14 Epoch 1807/5000, D_loss=0.050209, reward=-7.577842\n",
      "      433983126 16:04:14 Epoch 1808/5000, D_loss=0.049460, reward=-7.579483\n",
      "      433983126 16:04:15 Epoch 1809/5000, D_loss=0.049332, reward=-7.579086\n",
      "      433983126 16:04:15 Epoch 1810/5000, D_loss=0.052811, reward=-7.569573\n",
      "      433983126 16:04:15 Epoch 1811/5000, D_loss=0.050028, reward=-7.581668\n",
      "      433983126 16:04:16 Epoch 1812/5000, D_loss=0.050191, reward=-7.577880\n",
      "      433983126 16:04:16 Epoch 1813/5000, D_loss=0.050578, reward=-7.575333\n",
      "      433983126 16:04:16 Epoch 1814/5000, D_loss=0.050566, reward=-7.576911\n",
      "      433983126 16:04:17 Epoch 1815/5000, D_loss=0.049114, reward=-7.578536\n",
      "      433983126 16:04:17 Epoch 1816/5000, D_loss=0.050662, reward=-7.578373\n",
      "      433983126 16:04:18 Epoch 1817/5000, D_loss=0.050249, reward=-7.579971\n",
      "      433983126 16:04:18 Epoch 1818/5000, D_loss=0.051375, reward=-7.573529\n",
      "      433983126 16:04:18 Epoch 1819/5000, D_loss=0.049874, reward=-7.579807\n",
      "      433983126 16:04:19 Epoch 1820/5000, D_loss=0.049040, reward=-7.580387\n",
      "      433983126 16:04:19 Epoch 1821/5000, D_loss=0.050347, reward=-7.580960\n",
      "      433983126 16:04:20 Epoch 1822/5000, D_loss=0.050585, reward=-7.574485\n",
      "      433983126 16:04:20 Epoch 1823/5000, D_loss=0.049965, reward=-7.575829\n",
      "      433983126 16:04:20 Epoch 1824/5000, D_loss=0.049339, reward=-7.575144\n",
      "      433983126 16:04:21 Epoch 1825/5000, D_loss=0.049559, reward=-7.577255\n",
      "      433983126 16:04:21 Epoch 1826/5000, D_loss=0.049801, reward=-7.573615\n",
      "      433983126 16:04:21 Epoch 1827/5000, D_loss=0.050141, reward=-7.573738\n",
      "      433983126 16:04:22 Epoch 1828/5000, D_loss=0.050928, reward=-7.570770\n",
      "      433983126 16:04:22 Epoch 1829/5000, D_loss=0.049696, reward=-7.576295\n",
      "      433983126 16:04:23 Epoch 1830/5000, D_loss=0.050856, reward=-7.570311\n",
      "      433983126 16:04:23 Epoch 1831/5000, D_loss=0.049130, reward=-7.576384\n",
      "      433983126 16:04:23 Epoch 1832/5000, D_loss=0.050020, reward=-7.571035\n",
      "      433983126 16:04:24 Epoch 1833/5000, D_loss=0.050800, reward=-7.573741\n",
      "      433983126 16:04:24 Epoch 1834/5000, D_loss=0.050058, reward=-7.571892\n",
      "      433983126 16:04:24 Epoch 1835/5000, D_loss=0.050621, reward=-7.575087\n",
      "      433983126 16:04:25 Epoch 1836/5000, D_loss=0.050572, reward=-7.573284\n",
      "      433983126 16:04:25 Epoch 1837/5000, D_loss=0.049944, reward=-7.571354\n",
      "      433983126 16:04:26 Epoch 1838/5000, D_loss=0.050135, reward=-7.573547\n",
      "      433983126 16:04:26 Epoch 1839/5000, D_loss=0.051720, reward=-7.572123\n",
      "      433983126 16:04:26 Epoch 1840/5000, D_loss=0.050509, reward=-7.572402\n",
      "      433983126 16:04:27 Epoch 1841/5000, D_loss=0.048912, reward=-7.567862\n",
      "      433983126 16:04:27 Epoch 1842/5000, D_loss=0.049420, reward=-7.569396\n",
      "      433983126 16:04:28 Epoch 1843/5000, D_loss=0.051143, reward=-7.570392\n",
      "      433983126 16:04:28 Epoch 1844/5000, D_loss=0.051511, reward=-7.567074\n",
      "      433983126 16:04:28 Epoch 1845/5000, D_loss=0.049388, reward=-7.574729\n",
      "      433983126 16:04:29 Epoch 1846/5000, D_loss=0.049851, reward=-7.572955\n",
      "      433983126 16:04:29 Epoch 1847/5000, D_loss=0.051214, reward=-7.568089\n",
      "      433983126 16:04:29 Epoch 1848/5000, D_loss=0.050508, reward=-7.569941\n",
      "      433983126 16:04:30 Epoch 1849/5000, D_loss=0.051365, reward=-7.571049\n",
      "      433983126 16:04:30 Epoch 1850/5000, D_loss=0.050242, reward=-7.567476\n",
      "      433983126 16:04:31 Epoch 1851/5000, D_loss=0.050315, reward=-7.569027\n",
      "      433983126 16:04:31 Epoch 1852/5000, D_loss=0.050782, reward=-7.565501\n",
      "      433983126 16:04:31 Epoch 1853/5000, D_loss=0.049997, reward=-7.566327\n",
      "      433983126 16:04:32 Epoch 1854/5000, D_loss=0.050393, reward=-7.568442\n",
      "      433983126 16:04:32 Epoch 1855/5000, D_loss=0.052110, reward=-7.562082\n",
      "      433983126 16:04:32 Epoch 1856/5000, D_loss=0.049929, reward=-7.569171\n",
      "      433983126 16:04:33 Epoch 1857/5000, D_loss=0.050198, reward=-7.564231\n",
      "      433983126 16:04:33 Epoch 1858/5000, D_loss=0.051687, reward=-7.567293\n",
      "      433983126 16:04:34 Epoch 1859/5000, D_loss=0.050736, reward=-7.567749\n",
      "      433983126 16:04:34 Epoch 1860/5000, D_loss=0.049838, reward=-7.561989\n",
      "      433983126 16:04:34 Epoch 1861/5000, D_loss=0.050811, reward=-7.563422\n",
      "      433983126 16:04:35 Epoch 1862/5000, D_loss=0.049630, reward=-7.563379\n",
      "      433983126 16:04:35 Epoch 1863/5000, D_loss=0.049032, reward=-7.567169\n",
      "      433983126 16:04:35 Epoch 1864/5000, D_loss=0.049262, reward=-7.570251\n",
      "      433983126 16:04:36 Epoch 1865/5000, D_loss=0.051553, reward=-7.565909\n",
      "      433983126 16:04:36 Epoch 1866/5000, D_loss=0.049374, reward=-7.560078\n",
      "      433983126 16:04:37 Epoch 1867/5000, D_loss=0.049811, reward=-7.564895\n",
      "      433983126 16:04:37 Epoch 1868/5000, D_loss=0.048726, reward=-7.572124\n",
      "      433983126 16:04:37 Epoch 1869/5000, D_loss=0.052107, reward=-7.565609\n",
      "      433983126 16:04:38 Epoch 1870/5000, D_loss=0.051387, reward=-7.566662\n",
      "      433983126 16:04:38 Epoch 1871/5000, D_loss=0.050957, reward=-7.562988\n",
      "      433983126 16:04:38 Epoch 1872/5000, D_loss=0.050358, reward=-7.564332\n",
      "      433983126 16:04:39 Epoch 1873/5000, D_loss=0.049547, reward=-7.570374\n",
      "      433983126 16:04:39 Epoch 1874/5000, D_loss=0.050745, reward=-7.557450\n",
      "      433983126 16:04:40 Epoch 1875/5000, D_loss=0.050022, reward=-7.561507\n",
      "      433983126 16:04:40 Epoch 1876/5000, D_loss=0.050033, reward=-7.557606\n",
      "      433983126 16:04:40 Epoch 1877/5000, D_loss=0.050746, reward=-7.560787\n",
      "      433983126 16:04:41 Epoch 1878/5000, D_loss=0.051530, reward=-7.559504\n",
      "      433983126 16:04:41 Epoch 1879/5000, D_loss=0.050539, reward=-7.561548\n",
      "      433983126 16:04:41 Epoch 1880/5000, D_loss=0.052350, reward=-7.560714\n",
      "      433983126 16:04:42 Epoch 1881/5000, D_loss=0.050581, reward=-7.561128\n",
      "      433983126 16:04:42 Epoch 1882/5000, D_loss=0.052548, reward=-7.556963\n",
      "      433983126 16:04:43 Epoch 1883/5000, D_loss=0.050448, reward=-7.559480\n",
      "      433983126 16:04:43 Epoch 1884/5000, D_loss=0.051230, reward=-7.558661\n",
      "      433983126 16:04:43 Epoch 1885/5000, D_loss=0.049145, reward=-7.564289\n",
      "      433983126 16:04:44 Epoch 1886/5000, D_loss=0.050485, reward=-7.561735\n",
      "      433983126 16:04:44 Epoch 1887/5000, D_loss=0.050302, reward=-7.562859\n",
      "      433983126 16:04:45 Epoch 1888/5000, D_loss=0.050019, reward=-7.562030\n",
      "      433983126 16:04:45 Epoch 1889/5000, D_loss=0.050604, reward=-7.560527\n",
      "      433983126 16:04:45 Epoch 1890/5000, D_loss=0.048944, reward=-7.569970\n",
      "      433983126 16:04:46 Epoch 1891/5000, D_loss=0.051764, reward=-7.561211\n",
      "      433983126 16:04:46 Epoch 1892/5000, D_loss=0.049040, reward=-7.565718\n",
      "      433983126 16:04:46 Epoch 1893/5000, D_loss=0.049114, reward=-7.558519\n",
      "      433983126 16:04:47 Epoch 1894/5000, D_loss=0.049955, reward=-7.556705\n",
      "      433983126 16:04:47 Epoch 1895/5000, D_loss=0.050358, reward=-7.563717\n",
      "      433983126 16:04:48 Epoch 1896/5000, D_loss=0.050730, reward=-7.558271\n",
      "      433983126 16:04:48 Epoch 1897/5000, D_loss=0.050512, reward=-7.558955\n",
      "      433983126 16:04:48 Epoch 1898/5000, D_loss=0.051470, reward=-7.555892\n",
      "      433983126 16:04:49 Epoch 1899/5000, D_loss=0.052620, reward=-7.560771\n",
      "      433983126 16:04:49 Epoch 1900/5000, D_loss=0.051156, reward=-7.557167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hhehe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      248942384 16:04:53 MR = 4395.1435546875\n",
      "MRR = 0.18395082652568817\n",
      "Hit@1 = 0.008075148319050759\n",
      "Hit@3 = 0.34278180619644033\n",
      "Hit@10 = 0.4398483849703362\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MR = 4395.1435546875\n",
      "MRR = 0.18395082652568817\n",
      "Hit@1 = 0.008075148319050759\n",
      "Hit@3 = 0.34278180619644033\n",
      "Hit@10 = 0.4398483849703362\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      433983126 16:04:53 Epoch 1901/5000, D_loss=0.049108, reward=-7.553177\n",
      "      433983126 16:04:54 Epoch 1902/5000, D_loss=0.050863, reward=-7.561576\n",
      "      433983126 16:04:54 Epoch 1903/5000, D_loss=0.050645, reward=-7.561898\n",
      "      433983126 16:04:54 Epoch 1904/5000, D_loss=0.049056, reward=-7.554152\n",
      "      433983126 16:04:55 Epoch 1905/5000, D_loss=0.050119, reward=-7.558410\n",
      "      433983126 16:04:55 Epoch 1906/5000, D_loss=0.051800, reward=-7.556155\n",
      "      433983126 16:04:56 Epoch 1907/5000, D_loss=0.049556, reward=-7.562667\n",
      "      433983126 16:04:56 Epoch 1908/5000, D_loss=0.049890, reward=-7.556755\n",
      "      433983126 16:04:56 Epoch 1909/5000, D_loss=0.050838, reward=-7.551769\n",
      "      433983126 16:04:57 Epoch 1910/5000, D_loss=0.050325, reward=-7.554568\n",
      "      433983126 16:04:57 Epoch 1911/5000, D_loss=0.051205, reward=-7.557166\n",
      "      433983126 16:04:57 Epoch 1912/5000, D_loss=0.049957, reward=-7.553424\n",
      "      433983126 16:04:58 Epoch 1913/5000, D_loss=0.050222, reward=-7.554236\n",
      "      433983126 16:04:58 Epoch 1914/5000, D_loss=0.050239, reward=-7.553066\n",
      "      433983126 16:04:59 Epoch 1915/5000, D_loss=0.049932, reward=-7.553194\n",
      "      433983126 16:04:59 Epoch 1916/5000, D_loss=0.050406, reward=-7.556605\n",
      "      433983126 16:04:59 Epoch 1917/5000, D_loss=0.051959, reward=-7.555668\n",
      "      433983126 16:05:00 Epoch 1918/5000, D_loss=0.049673, reward=-7.555821\n",
      "      433983126 16:05:00 Epoch 1919/5000, D_loss=0.050290, reward=-7.552867\n",
      "      433983126 16:05:00 Epoch 1920/5000, D_loss=0.051731, reward=-7.554150\n",
      "      433983126 16:05:01 Epoch 1921/5000, D_loss=0.052111, reward=-7.547324\n",
      "      433983126 16:05:01 Epoch 1922/5000, D_loss=0.050323, reward=-7.555220\n",
      "      433983126 16:05:02 Epoch 1923/5000, D_loss=0.050351, reward=-7.554800\n",
      "      433983126 16:05:02 Epoch 1924/5000, D_loss=0.051458, reward=-7.552721\n",
      "      433983126 16:05:02 Epoch 1925/5000, D_loss=0.050445, reward=-7.552323\n",
      "      433983126 16:05:03 Epoch 1926/5000, D_loss=0.052318, reward=-7.545306\n",
      "      433983126 16:05:03 Epoch 1927/5000, D_loss=0.049378, reward=-7.553705\n",
      "      433983126 16:05:03 Epoch 1928/5000, D_loss=0.051967, reward=-7.550772\n",
      "      433983126 16:05:04 Epoch 1929/5000, D_loss=0.050794, reward=-7.549631\n",
      "      433983126 16:05:04 Epoch 1930/5000, D_loss=0.049319, reward=-7.555806\n",
      "      433983126 16:05:05 Epoch 1931/5000, D_loss=0.049552, reward=-7.554085\n",
      "      433983126 16:05:05 Epoch 1932/5000, D_loss=0.051253, reward=-7.549380\n",
      "      433983126 16:05:05 Epoch 1933/5000, D_loss=0.051067, reward=-7.552242\n",
      "      433983126 16:05:06 Epoch 1934/5000, D_loss=0.050496, reward=-7.544395\n",
      "      433983126 16:05:06 Epoch 1935/5000, D_loss=0.051899, reward=-7.548294\n",
      "      433983126 16:05:06 Epoch 1936/5000, D_loss=0.052291, reward=-7.543697\n",
      "      433983126 16:05:07 Epoch 1937/5000, D_loss=0.049402, reward=-7.552110\n",
      "      433983126 16:05:07 Epoch 1938/5000, D_loss=0.052733, reward=-7.546220\n",
      "      433983126 16:05:08 Epoch 1939/5000, D_loss=0.051029, reward=-7.546175\n",
      "      433983126 16:05:08 Epoch 1940/5000, D_loss=0.051084, reward=-7.543927\n",
      "      433983126 16:05:08 Epoch 1941/5000, D_loss=0.050779, reward=-7.546717\n",
      "      433983126 16:05:09 Epoch 1942/5000, D_loss=0.050667, reward=-7.550448\n",
      "      433983126 16:05:09 Epoch 1943/5000, D_loss=0.049281, reward=-7.551970\n",
      "      433983126 16:05:09 Epoch 1944/5000, D_loss=0.050642, reward=-7.545341\n",
      "      433983126 16:05:10 Epoch 1945/5000, D_loss=0.051036, reward=-7.542195\n",
      "      433983126 16:05:10 Epoch 1946/5000, D_loss=0.051426, reward=-7.545756\n",
      "      433983126 16:05:11 Epoch 1947/5000, D_loss=0.051831, reward=-7.548654\n",
      "      433983126 16:05:11 Epoch 1948/5000, D_loss=0.051735, reward=-7.546885\n",
      "      433983126 16:05:11 Epoch 1949/5000, D_loss=0.050064, reward=-7.545170\n",
      "      433983126 16:05:12 Epoch 1950/5000, D_loss=0.049464, reward=-7.546802\n",
      "      433983126 16:05:12 Epoch 1951/5000, D_loss=0.050303, reward=-7.545091\n",
      "      433983126 16:05:12 Epoch 1952/5000, D_loss=0.050018, reward=-7.548987\n",
      "      433983126 16:05:13 Epoch 1953/5000, D_loss=0.050452, reward=-7.545492\n",
      "      433983126 16:05:13 Epoch 1954/5000, D_loss=0.051451, reward=-7.549716\n",
      "      433983126 16:05:14 Epoch 1955/5000, D_loss=0.052327, reward=-7.545759\n",
      "      433983126 16:05:14 Epoch 1956/5000, D_loss=0.050069, reward=-7.548844\n",
      "      433983126 16:05:14 Epoch 1957/5000, D_loss=0.050896, reward=-7.549721\n",
      "      433983126 16:05:15 Epoch 1958/5000, D_loss=0.050251, reward=-7.551186\n",
      "      433983126 16:05:15 Epoch 1959/5000, D_loss=0.049451, reward=-7.548271\n",
      "      433983126 16:05:16 Epoch 1960/5000, D_loss=0.050907, reward=-7.542739\n",
      "      433983126 16:05:16 Epoch 1961/5000, D_loss=0.050884, reward=-7.547661\n",
      "      433983126 16:05:16 Epoch 1962/5000, D_loss=0.054170, reward=-7.534931\n",
      "      433983126 16:05:17 Epoch 1963/5000, D_loss=0.048728, reward=-7.547562\n",
      "      433983126 16:05:17 Epoch 1964/5000, D_loss=0.049790, reward=-7.542127\n",
      "      433983126 16:05:17 Epoch 1965/5000, D_loss=0.051271, reward=-7.545130\n",
      "      433983126 16:05:18 Epoch 1966/5000, D_loss=0.050964, reward=-7.541465\n",
      "      433983126 16:05:18 Epoch 1967/5000, D_loss=0.049944, reward=-7.541780\n",
      "      433983126 16:05:19 Epoch 1968/5000, D_loss=0.050336, reward=-7.542244\n",
      "      433983126 16:05:19 Epoch 1969/5000, D_loss=0.050874, reward=-7.535948\n",
      "      433983126 16:05:19 Epoch 1970/5000, D_loss=0.050063, reward=-7.540840\n",
      "      433983126 16:05:20 Epoch 1971/5000, D_loss=0.051996, reward=-7.537848\n",
      "      433983126 16:05:20 Epoch 1972/5000, D_loss=0.049952, reward=-7.541991\n",
      "      433983126 16:05:20 Epoch 1973/5000, D_loss=0.049648, reward=-7.541910\n",
      "      433983126 16:05:21 Epoch 1974/5000, D_loss=0.050361, reward=-7.541812\n",
      "      433983126 16:05:21 Epoch 1975/5000, D_loss=0.051106, reward=-7.540553\n",
      "      433983126 16:05:22 Epoch 1976/5000, D_loss=0.050197, reward=-7.541844\n",
      "      433983126 16:05:22 Epoch 1977/5000, D_loss=0.052746, reward=-7.536592\n",
      "      433983126 16:05:22 Epoch 1978/5000, D_loss=0.052383, reward=-7.543320\n",
      "      433983126 16:05:23 Epoch 1979/5000, D_loss=0.052017, reward=-7.541435\n",
      "      433983126 16:05:23 Epoch 1980/5000, D_loss=0.052140, reward=-7.539889\n",
      "      433983126 16:05:23 Epoch 1981/5000, D_loss=0.050097, reward=-7.537541\n",
      "      433983126 16:05:24 Epoch 1982/5000, D_loss=0.051160, reward=-7.542264\n",
      "      433983126 16:05:24 Epoch 1983/5000, D_loss=0.049293, reward=-7.542536\n",
      "      433983126 16:05:25 Epoch 1984/5000, D_loss=0.052534, reward=-7.533644\n",
      "      433983126 16:05:25 Epoch 1985/5000, D_loss=0.052251, reward=-7.541602\n",
      "      433983126 16:05:25 Epoch 1986/5000, D_loss=0.050239, reward=-7.542094\n",
      "      433983126 16:05:26 Epoch 1987/5000, D_loss=0.051539, reward=-7.533638\n",
      "      433983126 16:05:26 Epoch 1988/5000, D_loss=0.051462, reward=-7.538070\n",
      "      433983126 16:05:26 Epoch 1989/5000, D_loss=0.051178, reward=-7.543045\n",
      "      433983126 16:05:27 Epoch 1990/5000, D_loss=0.052333, reward=-7.534845\n",
      "      433983126 16:05:27 Epoch 1991/5000, D_loss=0.051519, reward=-7.530927\n",
      "      433983126 16:05:28 Epoch 1992/5000, D_loss=0.051051, reward=-7.535013\n",
      "      433983126 16:05:28 Epoch 1993/5000, D_loss=0.052939, reward=-7.536064\n",
      "      433983126 16:05:28 Epoch 1994/5000, D_loss=0.051461, reward=-7.534281\n",
      "      433983126 16:05:29 Epoch 1995/5000, D_loss=0.050888, reward=-7.533647\n",
      "      433983126 16:05:29 Epoch 1996/5000, D_loss=0.050515, reward=-7.538195\n",
      "      433983126 16:05:29 Epoch 1997/5000, D_loss=0.053153, reward=-7.540473\n",
      "      433983126 16:05:30 Epoch 1998/5000, D_loss=0.050897, reward=-7.535718\n",
      "      433983126 16:05:30 Epoch 1999/5000, D_loss=0.051389, reward=-7.534135\n",
      "      433983126 16:05:31 Epoch 2000/5000, D_loss=0.051607, reward=-7.534610\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hhehe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      248942384 16:05:34 MR = 4312.9677734375\n",
      "MRR = 0.1838178038597107\n",
      "Hit@1 = 0.00922874093605801\n",
      "Hit@3 = 0.33569545154911007\n",
      "Hit@10 = 0.4396835860250494\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MR = 4312.9677734375\n",
      "MRR = 0.1838178038597107\n",
      "Hit@1 = 0.00922874093605801\n",
      "Hit@3 = 0.33569545154911007\n",
      "Hit@10 = 0.4396835860250494\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      433983126 16:05:35 Epoch 2001/5000, D_loss=0.049689, reward=-7.536351\n",
      "      433983126 16:05:35 Epoch 2002/5000, D_loss=0.048703, reward=-7.540701\n",
      "      433983126 16:05:36 Epoch 2003/5000, D_loss=0.051491, reward=-7.536658\n",
      "      433983126 16:05:36 Epoch 2004/5000, D_loss=0.051623, reward=-7.534793\n",
      "      433983126 16:05:36 Epoch 2005/5000, D_loss=0.051514, reward=-7.529397\n",
      "      433983126 16:05:37 Epoch 2006/5000, D_loss=0.048654, reward=-7.535520\n",
      "      433983126 16:05:37 Epoch 2007/5000, D_loss=0.050887, reward=-7.529681\n",
      "      433983126 16:05:37 Epoch 2008/5000, D_loss=0.049645, reward=-7.532733\n",
      "      433983126 16:05:38 Epoch 2009/5000, D_loss=0.049717, reward=-7.533763\n",
      "      433983126 16:05:38 Epoch 2010/5000, D_loss=0.053165, reward=-7.524553\n",
      "      433983126 16:05:39 Epoch 2011/5000, D_loss=0.049726, reward=-7.532300\n",
      "      433983126 16:05:39 Epoch 2012/5000, D_loss=0.052258, reward=-7.531756\n",
      "      433983126 16:05:39 Epoch 2013/5000, D_loss=0.049567, reward=-7.534202\n",
      "      433983126 16:05:40 Epoch 2014/5000, D_loss=0.050346, reward=-7.532816\n",
      "      433983126 16:05:40 Epoch 2015/5000, D_loss=0.049720, reward=-7.532183\n",
      "      433983126 16:05:40 Epoch 2016/5000, D_loss=0.050309, reward=-7.529088\n",
      "      433983126 16:05:41 Epoch 2017/5000, D_loss=0.048688, reward=-7.532821\n",
      "      433983126 16:05:41 Epoch 2018/5000, D_loss=0.049377, reward=-7.529813\n",
      "      433983126 16:05:42 Epoch 2019/5000, D_loss=0.050366, reward=-7.530570\n",
      "      433983126 16:05:42 Epoch 2020/5000, D_loss=0.050514, reward=-7.535211\n",
      "      433983126 16:05:42 Epoch 2021/5000, D_loss=0.051387, reward=-7.531573\n",
      "      433983126 16:05:43 Epoch 2022/5000, D_loss=0.050231, reward=-7.533116\n",
      "      433983126 16:05:43 Epoch 2023/5000, D_loss=0.050155, reward=-7.533905\n",
      "      433983126 16:05:43 Epoch 2024/5000, D_loss=0.052446, reward=-7.527699\n",
      "      433983126 16:05:44 Epoch 2025/5000, D_loss=0.051470, reward=-7.528862\n",
      "      433983126 16:05:44 Epoch 2026/5000, D_loss=0.050474, reward=-7.530464\n",
      "      433983126 16:05:45 Epoch 2027/5000, D_loss=0.050960, reward=-7.527781\n",
      "      433983126 16:05:45 Epoch 2028/5000, D_loss=0.051372, reward=-7.527781\n",
      "      433983126 16:05:45 Epoch 2029/5000, D_loss=0.051036, reward=-7.524179\n",
      "      433983126 16:05:46 Epoch 2030/5000, D_loss=0.049987, reward=-7.526493\n",
      "      433983126 16:05:46 Epoch 2031/5000, D_loss=0.049337, reward=-7.527644\n",
      "      433983126 16:05:46 Epoch 2032/5000, D_loss=0.049659, reward=-7.532194\n",
      "      433983126 16:05:47 Epoch 2033/5000, D_loss=0.049493, reward=-7.529114\n",
      "      433983126 16:05:47 Epoch 2034/5000, D_loss=0.052620, reward=-7.524797\n",
      "      433983126 16:05:48 Epoch 2035/5000, D_loss=0.051426, reward=-7.523285\n",
      "      433983126 16:05:48 Epoch 2036/5000, D_loss=0.049459, reward=-7.530146\n",
      "      433983126 16:05:48 Epoch 2037/5000, D_loss=0.051834, reward=-7.526935\n",
      "      433983126 16:05:49 Epoch 2038/5000, D_loss=0.050022, reward=-7.523877\n",
      "      433983126 16:05:49 Epoch 2039/5000, D_loss=0.050898, reward=-7.525173\n",
      "      433983126 16:05:49 Epoch 2040/5000, D_loss=0.050185, reward=-7.524926\n",
      "      433983126 16:05:50 Epoch 2041/5000, D_loss=0.052824, reward=-7.524191\n",
      "      433983126 16:05:50 Epoch 2042/5000, D_loss=0.049334, reward=-7.528008\n",
      "      433983126 16:05:51 Epoch 2043/5000, D_loss=0.050716, reward=-7.525140\n",
      "      433983126 16:05:51 Epoch 2044/5000, D_loss=0.049829, reward=-7.526916\n",
      "      433983126 16:05:51 Epoch 2045/5000, D_loss=0.052515, reward=-7.520417\n",
      "      433983126 16:05:52 Epoch 2046/5000, D_loss=0.051333, reward=-7.525632\n",
      "      433983126 16:05:52 Epoch 2047/5000, D_loss=0.050735, reward=-7.523014\n",
      "      433983126 16:05:52 Epoch 2048/5000, D_loss=0.048340, reward=-7.522565\n",
      "      433983126 16:05:53 Epoch 2049/5000, D_loss=0.050284, reward=-7.526246\n",
      "      433983126 16:05:53 Epoch 2050/5000, D_loss=0.052225, reward=-7.516150\n",
      "      433983126 16:05:54 Epoch 2051/5000, D_loss=0.051249, reward=-7.527747\n",
      "      433983126 16:05:54 Epoch 2052/5000, D_loss=0.049820, reward=-7.524801\n",
      "      433983126 16:05:54 Epoch 2053/5000, D_loss=0.051294, reward=-7.524123\n",
      "      433983126 16:05:55 Epoch 2054/5000, D_loss=0.050326, reward=-7.529411\n",
      "      433983126 16:05:55 Epoch 2055/5000, D_loss=0.052443, reward=-7.517705\n",
      "      433983126 16:05:55 Epoch 2056/5000, D_loss=0.052950, reward=-7.517533\n",
      "      433983126 16:05:56 Epoch 2057/5000, D_loss=0.049427, reward=-7.519271\n",
      "      433983126 16:05:56 Epoch 2058/5000, D_loss=0.049846, reward=-7.524369\n",
      "      433983126 16:05:57 Epoch 2059/5000, D_loss=0.049906, reward=-7.524143\n",
      "      433983126 16:05:57 Epoch 2060/5000, D_loss=0.050155, reward=-7.517488\n",
      "      433983126 16:05:57 Epoch 2061/5000, D_loss=0.049704, reward=-7.522135\n",
      "      433983126 16:05:58 Epoch 2062/5000, D_loss=0.050501, reward=-7.523433\n",
      "      433983126 16:05:58 Epoch 2063/5000, D_loss=0.051599, reward=-7.513128\n",
      "      433983126 16:05:59 Epoch 2064/5000, D_loss=0.052070, reward=-7.518542\n",
      "      433983126 16:05:59 Epoch 2065/5000, D_loss=0.048817, reward=-7.522792\n",
      "      433983126 16:05:59 Epoch 2066/5000, D_loss=0.048983, reward=-7.524818\n",
      "      433983126 16:06:00 Epoch 2067/5000, D_loss=0.049672, reward=-7.524819\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m main_model\u001b[38;5;241m.\u001b[39mtrain_n_test(n_entity, n_relation, heads, tails, train_data, valid_data, test_data)\n",
      "Cell \u001b[0;32mIn[13], line 220\u001b[0m, in \u001b[0;36mKBGAN.train_n_test\u001b[0;34m(self, n_entity, n_relation, heads, tails, train_data, valid_data, test_data)\u001b[0m\n\u001b[1;32m    217\u001b[0m gen_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerator\u001b[38;5;241m.\u001b[39mstep(hs, rs, ts, temperature\u001b[38;5;241m=\u001b[39mconfig()\u001b[38;5;241m.\u001b[39mKBGAN\u001b[38;5;241m.\u001b[39mtemperature)\n\u001b[1;32m    218\u001b[0m head_smpl, tail_smpl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(gen_step)\n\u001b[0;32m--> 220\u001b[0m losses, rewards \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscriminator\u001b[38;5;241m.\u001b[39mstep(h, r, t, head_fake\u001b[38;5;241m=\u001b[39mhead_smpl\u001b[38;5;241m.\u001b[39msqueeze(), tail_fake\u001b[38;5;241m=\u001b[39mtail_smpl\u001b[38;5;241m.\u001b[39msqueeze())\n\u001b[1;32m    221\u001b[0m epoch_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(rewards)\n\u001b[1;32m    223\u001b[0m rewards \u001b[38;5;241m=\u001b[39m rewards \u001b[38;5;241m-\u001b[39m avg_reward\n",
      "Cell \u001b[0;32mIn[13], line 67\u001b[0m, in \u001b[0;36mComponent.step\u001b[0;34m(self, head, relation, tail, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generator_step(head, relation, tail, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrole \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiscriminator\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_discriminator_step(head, relation, tail, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRole must be either \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerator\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiscriminator\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[13], line 120\u001b[0m, in \u001b[0;36mComponent._discriminator_step\u001b[0;34m(self, head, relation, tail, head_fake, tail_fake, train)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39m_ensure_optimizer()\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmdl\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 120\u001b[0m torch\u001b[38;5;241m.\u001b[39msum(losses)\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmdl\u001b[38;5;241m.\u001b[39mconstraint()\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    582\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    583\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[1;32m    348\u001b[0m     tensors,\n\u001b[1;32m    349\u001b[0m     grad_tensors_,\n\u001b[1;32m    350\u001b[0m     retain_graph,\n\u001b[1;32m    351\u001b[0m     create_graph,\n\u001b[1;32m    352\u001b[0m     inputs,\n\u001b[1;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    355\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main_model.train_n_test(n_entity, n_relation, heads, tails, train_data, valid_data, test_data)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8291845,
     "sourceId": 13090861,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
